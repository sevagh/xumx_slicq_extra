\documentclass[report.tex]{subfiles}
\begin{document}

\section{Methodology}
\label{sec:methodology}

The proposed adaptation of Open-Unmix (\cite{umx}) to use the sliCQ Transform (sliCQT) (\cite{invertiblecqt, slicq}) is named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ}} and is the subject and main result of this thesis.

Open-Unmix (\cite{umx}) has a reference implementation released as an open-source Python codebase for general music demixing,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} which uses PyTorch (\cite{pytorch}) for GPU-accelerated numerical computation and deep learning, and includes a MUSDB18-HQ data loader (\cite{musdb18hq}) and BSS metrics evaluator (\cite{bss}), both necessary components for this thesis.

xumx-sliCQ will be based on the Open-Unmix PyTorch template, and as a result, all of the code developed will be in Python and with PyTorch code targeting the GPU. The rest of this chapter will be focused on the Python and PyTorch implementation details of xumx-sliCQ, including improvements or changes made to the reference NSGT/sliCQT Python library\footnote{\url{https://github.com/grrrr/nsgt}} which will be forked to my GitHub account\footnote{\url{https://github.com/sevagh/nsgt}} to contain all of the modifications required for this thesis.

\subsection{Improving the NSGT/sliCQT reference library}
\label{sec:improvelib}

\subsubsection{New frequency scales}
\label{sec:freqscales}

The time and frequency resolution of the NSGT and sliCQT in the reference library are defined from the frequency scale. In essence the NSGT/sliCQT can be thought of as a filterbank (\cite{variableq1}). The essential parameters to a frequency scale in the library are $f_{\text{min}}$ and $f_{\text{max}}$, which are the minimum and maximum frequency in Hz, and the total frequency bins. The type of scale determines how the frequency bins are distributed between $f_{min}$--$f_{\text{max}}$. A custom frequency scale may use additional parameters as needed.

The provided scales\footnote{\url{https://github.com/grrrr/nsgt/blob/master/nsgt/fscale.py}} are octave, log, linear (i.e. the regular STFT), and mel. The octave scale is similar to the log scale, except that the octave scale takes a bins-per-octave (bpo) argument, and the log scale takes a bins argument. Recall from section \ref{sec:cqt} that to compute the total bins from the bins-per-octave setting of the CQ-NSGT, we can use $K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}}) + 1]$, where $K$ is the total bins, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies.

In the Python implementation, using the CQ-NSGT is the same as the base NSGT (or sliCQT) with a manually-defined octave scale. One can use the logarithmic scale for more direct control over the total bins. The difference is shown in listing \ref{code:octvlog}. The resulting frequency scale is plotted in figure \ref{fig:octvlog}.

\begin{figure}[h]
  \centering
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{inputminted}[linenos,breaklines,frame=single,firstline=4,lastline=16]{text}{./scripts/fscale.py}
\end{inputminted}
 \subfloat{(a) Code snippet defining octave and log scales}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||]{text}
|\$ python scripts/fscale.py 3 # invoke script for bpo=3|
|bpo: 3, bins: 21, len(oct): 21, len(log): 21|
\end{minted}
 \subfloat{(b) Output printed by above}
 \end{minipage}
  \captionof{listing}{Octave and log scales for the NSGT}
  \label{code:octvlog}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/log_vs_oct.png}
	\caption{Frequency bins for the octave and log scales, annotated with their nearest musical note}
	\label{fig:octvlog}
\end{figure}

To supplement the octave and log scale, the variable-Q scale (\cite{variableq1, variableq2}) was implemented.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L105}}

The variable-Q scale is the same as the log or octave scales, except with a small fixed frequency offset, denoted by gamma or $\gamma \text{ (Hz)}$, added to each frequency bin. \textcite{variableq1} provide the motivation for the variable-Q scale:
\begin{quote}
	... [The] CQT has several advantages over STFT when analysing music signals. However, one considerable practical drawback is the fact that the analysis/synthesis atoms get very long towards lower frequencies. This is unreasonable both from a perceptual viewpoint and from a musical viewpoint. Auditory filters in the human auditory system are approximately constant-Q only for frequencies above 500 Hz and smoothly approach a constant bandwidth towards lower frequencies. Accordingly, music signals generally do not contain closely spaced pitches at low frequencies, thus the Q-factors (relative frequency resolution) can safely be reduced towards lower frequencies, which in turn improves the time resolution.
\end{quote}

Both \textcite{variableq1} and \textcite{variableq2} cite the ERBlet transform (\cite{erblet}) as a psychoacoustic transform which in turn motivates the Variable-Q Transform. \textcite{variableq1} provides the following equations in \ref{equation:variablebw} for computing the bandwidth $B_{k}$ of the frequency bin (or filter channel) $k$ where $b$ is the bins-per-octave or bpo:
\begin{align}\tag{5}\label{equation:variablebw}
	\nonumber & B_{k} = \alpha f_{k} + \gamma\\
	\nonumber & \alpha = 2^{\frac{1}{b}} - 2^{\frac{1}{b}}
\end{align}

\textcite{variableq2} use the same equations. The effect this has is to widen the low frequency bands significantly, since the offset is comparable in its order of magnitude to the lower frequency bandwidths. \textcite{variableq1} show some examples with $\gamma = [0, 3, 6.6, 10, 30] \text{ Hz}$. As the center frequency increases, the effect of the small offset becomes negligible. This results in a widening of the Q-factors in the low frequency bins, but Q-factors that are still constant in the high frequency bins. Figure \ref{fig:vq} shows the constant-Q/log, and variable-Q scales for different values of the frequency offset $\gamma$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/vqlog.png}
	\caption{Frequency bins and Q factors for the constant-Q/log and variable-Q scales}
	\label{fig:vq}
\end{figure}

The next frequency scale added was the Bark psychoacoustic scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L207}} to complement the included mel scale. We used one of the possible formulas used to convert between Bark and Hz frequencies (\cite{barktan}) and vice versa, sometimes colloquially called the ``Barktan formula''\footnote{\url{https://ccrma.stanford.edu/~jos/bbt/Optimal_Frequency_Warpings.html}}:
\begin{align}
	\nonumber & f_{\text{Bark}} = 6 \cdot arcsinh(\frac{f_{\text{Hz}}}{600}), f_{\text{Hz}} = 600 \cdot sinh(\frac{f_{\text{Bark}}}{6})
\end{align}

To plot the mel and Bark filterbanks, we prefer to use librosa which has some additional utilities for plotting mel filterbanks (\cite{librosa}). Bark support was added with the same Barktan equations above in my own fork of librosa.\footnote{\url{https://github.com/sevagh/librosa/commit/695ecc61d0622c4abf41afda77c571dd58e9b9fd}} The resulting frequency scales (constructed from librosa or the forked NSGT library) are equivalent, since the mel to Hz and bark to Hz are identical. Figure \ref{fig:melbarkbands} shows the filterbanks visualized by librosa. Figure \ref{fig:melbarkfsandqs} shows the mel and Bark frequency scales and Q-factors from the NSGT library for 12 total mel or Bark bands.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/melbarks.png}
	\caption{librosa visualization of triangular filterbanks, nfft=512}
	\label{fig:melbarkbands}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/melbarkpitchesqs.png}
	\caption{Mel and Bark frequency curves and Q-factors from the NSGT library}
	\label{fig:melbarkfsandqs}
\end{figure}

\subsubsection{Automatic slice length picker}

An addition made to the sliced version of the NSGT, or the sliCQT, was to automatically suggest an appropriate slice length and transition area for a given frequency scale. The slice length and transition area, as mentioned in section \ref{sec:theoryslicqt}, define how the input signal can be processed in smaller-sized chunks (or slices).

Choosing an incorrect slice length which can't support the windows needed to support the desired frequency resolution will result in a warning from the NSGT library, creating confusion for users on how to pick a slice length.\footnote{\url{https://github.com/grrrr/nsgt/issues/18}, \url{https://github.com/grrrr/nsgt/issues/24}} The warning code was adapted to suggest the minimum slice length and transition area for a given frequency scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L36}} and the code is shown in listing \ref{lst:suggestedsllen}. The resulting improvement in the NSGT library is shown in listing \ref{code:slicenoslice}.

\begin{figure}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||]{text}
|f,q = self() # get the frequency bins and q-factors for scale|
|Ls = int(np.ceil(max((q*8.*sr)/f))) # sr = sample rate|
|Ls = Ls + -Ls \% 4|
|sllen = Ls|
|trlen = sllen//4|
|trlen = trlen + -trlen \% 2 # make trlen divisible by 2|
\end{minted}
  \captionof{listing}{Suggested slice and transition lengths for the sliCQT}
  \label{lst:suggestedsllen}
\end{figure}

\begin{figure}[h]
  \centering
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{inputminted}[linenos,breaklines,frame=single]{python}{./scripts/slicenoslice.py}
\end{inputminted}
 \subfloat{(a) Code snippet for NSGT and sliCQT with slice and transition lengths}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||]{text}
|\$ python scripts/slicenoslice.py|
|/home/sevagh/repos/nsgt/nsgt/nsgfwin\_sl.py:66: UserWarning: Q-factor too high for frequencies 82.41,90.81,100.07,110.27,121.52,133.91,147.56,162.61,179.18,...|
|  warn("Q-factor too high for frequencies \%s"\%",".join("\%.2f"\%fi for fi in f[q >= qneeded]))|
|suggested sllen, trlen: 22040 5510|
\end{minted}
 \subfloat{(b) Output printed by above}
 \end{minipage}
  \captionof{listing}{Suggested slice and transition lengths based on the desired frequency scale}
  \label{code:slicenoslice}
\end{figure}

\newpagefill

\subsubsection{GPU-accelerated NSGT/sliCQT with PyTorch}
\label{sec:torchslicq}

The reference NSGT/sliCQT library uses NumPy (\cite{numpy}), a CPU-based Python numerical computation library. To train deep neural networks efficiently, using PyTorch with GPU support is required (\cite{pytorch}). PyTorch, in addition to being a deep learning framework, is also a numerical computation library which implements a large amount of NumPy's functions (\cite{pytorch}).

The line-profiler\footnote{\url{https://pypi.org/project/line-profiler/}} package for Python prints a line-by-line output of the execution times of Python functions that you annotate with \Verb#@profile#, and it was used to find performance hotspots and bottlenecks in the NSGT library. An example output is shown in listing \ref{lst:profilerout}, showing what the most costly functions of the reference forward transform are.

\chaptertodo{
	actually put this excerpt in
}

The starting point of porting the library to the GPU is to simply replace all NumPy functions with their PyTorch equivalents. When refering to code changes, NumPy and PyTorch will be referred to as numpy and torch, their respective Python module names. The torch port to the NSGT/sliCQT can be viewed on GitHub through the commit browser, starting from the following commit.\footnote{\url{https://github.com/sevagh/nsgt/commits/main?after=9aed369865989d90664445c3a6f74843be62f778+34&branch=main}} Figure \ref{fig:ghcommitbrowser} shows how to use the GitHub UI to examine the code changes made in each commit.

\afterpage{
\begin{figure}[!ht]
	\centering
	\subfloat[GitHub commit browser]{\includegraphics[width=0.75\textwidth]{./images-misc/ghcommitbrowser.png}}\\
	\subfloat[View file and line changes in each commit]{\includegraphics[width=0.75\textwidth]{./images-misc/ghcommitsingle.png}}
	\caption{How to browse the individual steps of the torch port\protect\footnotemark}
	\label{fig:ghcommitbrowser}
\end{figure}
\footnotetext{\url{https://github.com/sevagh/nsgt/commits/main?after=9aed369865989d90664445c3a6f74843be62f778+34&branch=main}}
}

An excerpt of the change is shown in listing \ref{code:firstdiff}. 

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single]{text}{./diffs/diff1.txt}
\end{inputminted}
  \caption{Example changes of porting from numpy to torch}
  \label{code:firstdiff}
\end{listing}

Next, the list and generator for the forward operation of the transform, which is one of the performance hotspots, are converted to a preallocated tensor, shown in listings \ref{code:seconddiff1} and \ref{code:seconddiff2}. The significant commit which introduced this change is here.\footnote{\url{https://github.com/sevagh/nsgt/commit/49a70d112ba4091d271a9438a8c38cf380ce62b4}}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single]{text}{./diffs/diff2_1.txt}
\end{inputminted}
  \caption{For-loop and generator-based design of the forward NSGT}
  \label{code:seconddiff1}
\end{listing}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single]{text}{./diffs/diff2_2.txt}
\end{inputminted}
  \caption{Torch tensor conversion of the forward NSGT}
  \label{code:seconddiff2}
\end{listing}

\newpagefill

\subsubsection{Matrix and ragged forms}

\textcite{klapuricqt}
\begin{quote}
	CQT produces a data structure that is more difficult to work with than the time-frequency matrix (spectrogram) obtained by using Short-Time Fourier transform in successive time frames. The last problem is due to the fact that in CQT, the time resolution varies for different frequency bins, in effect meaning that the ``sampling'' of different frequency bins is not synchronized.
\end{quote}

\chaptertodo{
	Intent to group frequency bins by time resolution\\
	call it the ragged transform
}

\subsubsection{Slice overlap-add utility}

The next addition to the slice utilities was to include a utility function for the 50\% overlapping of adjacent slices,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/slicq.py\#L38}} where the overlap length is automatically inferred from the shape of the transform. The function is named \Verb#overlap_add_slicq# and is imported from the \Verb#nsgt.slicq# file.

In fact, this function was included in the original project, but in the example spectrogram code.\footnote{\url{https://github.com/grrrr/nsgt/blob/master/examples/spectrogram.py\#L19}} We believe that due to necessity of the 50\% overlap to create a meaningful spectrogram, the overlap-add utility should be included in the core of the library. Finally, in the spectrogram example of the forked library, the new \Verb#overlap_add_slicq# function is imported from the core library and used to generate a good spectrogram.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/examples/spectrogram.py\#L19}}

Figure \ref{fig:spectrograms} shows

\subsection{Choosing sliCQT parameters}
\label{sec:slicqparamsrch}

The initial exploration is on whether the sliCQT is suitable as an STFT replacement for music demixing with magnitude spectrogram masking, since this is how Open-Unmix works. A simple approach is to experiment with the sliCQT replacing the STFT inside oracle mask estimators, which were described in \ref{sec:masksandoracles}. If the oracle experiments show promising results, such as the sliCQT oracle performance surpassing STFT oracle performance, it's a good first step towards replacing the STFT with the sliCQT in a neural network.

To recap, the oracle mask is a perfect time-frequency mask which is computed from ground-truth training data. The oracle is used to give an idea of the upper limit of audio quality from an algorithm or model for music demixing. The mask is typically applied to the magnitude transform (\cite{umx}), and the phase is discarded.

In this parameter exploration, we require the following components to build the oracle testbench:

\begin{tight_enumerate}
	\item
		Python MUSDB18-HQ (\cite{musdb18hq}) loader library\footnote{\url{https://github.com/sigsep/sigsep-mus-db}}
	\item
		Python BSS metrics (\cite{bss}) evaluator library\footnote{\url{https://github.com/sigsep/sigsep-mus-eval/}} by the SigSep community
\end{tight_enumerate}

\subsubsection{Mix-phase inversion (MPI) oracle}

\textbf{N.B.} The words TFTransform (and iTFTransform for the inverse or backward transform) will be used to refer to either the STFT or sliCQT in the equations shown in this section, since both are complex and invertible time-frequency transforms with Fourier coefficients

As shown in section \ref{sec:masksandoracles}, the definition of the IRM1 oracle based on the STFT, or ideal ratio/soft mask with the magnitude spectrogram raised to the first power, for a mixed song waveform $x_{\text{mix}}[n]$ and one of its isolated sources $x_{\text{source}}[n]$, and the subsequent oracle-estimated source waveform $\hat{x}_{\text{source}}[n]$ are computed with the following equations in \ref{equation:irm1}:
\begin{align}\tag{1}\label{equation:irm1}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n]), X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), |X_{\text{source}}| = \text{abs}(X_{\text{source}})\\
	\nonumber & \text{IRM1}_{\text{source}} = \frac{|X_{\text{source}}|^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, IRM1}} = X_{\text{mix}} \cdot \text{IRM1}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, IRM1}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, IRM1}})
\end{align}

Similar to the IRM1 calculation, let's examine the real case where we only have the mix available, and the magnitude spectrogram of the source was estimated by a neural network. Let's call this $|X_{\text{source}}|_{\text{est}}$, and see how the source waveform is computed with ratio masks in equations \ref{equation:softneural}:
\begin{align}\tag{2}\label{equation:softneural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & \text{EstRatioMask}_{\text{source}} = \frac{{|X_{\text{source}}|_{\text{est}}}^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, est}} = X_{\text{mix}} \cdot \text{EstRatioMask}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

This is the soft mask output of Open-Unmix, which is supported by one of the configuration options. However, by default Open-Unmix prefers to use the estimated magnitude and the phase of the mixture, over soft masks. The source waveform is estimated using the source magnitude spectrogram and mix phase spectrogram, and show in equations \ref{equation:mpineural}:
\begin{align}\tag{3}\label{equation:mpineural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

In other words, the magnitude of the isolated source is combined with the phase, or the angle, of the mixed waveform, to produce a complex time-frequency transform, which is then inverted with the backward transform to obtain the estimated isolated source waveform. I discussed the idea of the mix-phase inversion oracle here\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/issues/83}} and here\footnote{\url{https://discourse.aicrowd.com/t/oracle-baselines/6211/4?u=sevagh}} with Stefan Uhlich and Fabian-Robert St{\"o}ter, authors of Open-Unmix.

There are some mentions in demixing or source separation literature for the ``noisy phase'' oracle (\cite{noisyphase1, noisyphase2}), which is a more speech-based definition -- recall that in speech separation, a common task is to separate speech from noise signals. In music demixing, referring to interfering musical instruments as ``noise'' is inappropriate, so we should call it the mix-phase inversion (or MPI) oracle.

Let's write out the equations for the MPI oracle derived from the mixed-phase neural network estimate equations from \ref{equation:mpineural}. The MPI oracle can be computed from ground-truth data, using the equations \ref{equation:mpioracle}:
\begin{align}\tag{4}\label{equation:mpioracle}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{source}}| = \text{abs}(X_{\text{source}}), \measuredangle{X_{\text{source}}} = \text{angle}(X_{\text{source}})\\
	\nonumber & \hat{X}_{\text{source, MPI}} = |X_{\text{source}}| \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, MPI}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, MPI}})
\end{align}

This is the first estimate of the mix-phase inversion strategy, and doesn't assume further post-processing of the estimates. It gives us the idea of the performance of a time-frequency transform of an isolated source when the phase is discarded and the magnitude is combined with the phase of the mixture.

The mix-phase inversion oracle (MPI) score is the target we will choose to maximize when searching for sliCQT parameters to beat the STFT.

\subsubsection{MPI oracle parameter search testbench}

The splitting of a dataset into training, test, and validation sets was covered in section \ref{sec:ml}. To recap, the training set is used by the model to make predictions from the input and comparing its predicted output to the ground truth training output. The test set must be unseen during training, and the model's performance is tested on this set at the end of each training iteration (also called epoch) to show the generalization performance of the network. The validation set is also a set of data unseen in training, which can be used for the hyperparameter search.

In Open-Unmix, the STFT window size is set to 4096 by default (with a 1024 overlap), and it is a hyperparameter of the network. Therefore, the sliCQT parameters should equivalently be considered a hyperparameter in xumx-sliCQ. The MPI oracle testbench should use the predefined validation set of MUSDB18-HQ, which consists of 14 tracks. sliCQT parameters should be selected and be evaluated on their MPI score using BSS metrics for the 14 validation tracks.

The performance of the IRM1 (soft magnitude mask) and MPI (mixed-phase inversion) oracles will be shown for the best (and worst) sliCQT parameters, alongside different STFT window sizes. The intent is to show how the window size and time-frequency resolution of the STFT and sliCQT affect music demixing results per target (\cite{tftradeoff1}).

According to \textcite{randomgrid}, 60 iterations of random grid will find a statistically good set of hyperparameters in a case where an exhaustive grid search is impossible. The strategy chosen was to choose random hyperparameters for 60 iterations and select the highest MPI score, because an exhaustive grid search of the possible NSGT/sliCQT parameters would be too large. 

The random search is done for across 4 interesting frequency scales shown in section \ref{sec:freqscales}: the constant-Q or log scale, the variable-Q scale (log scale with offset), and the two psychoacoustic mel and Bark scales, and fmin between 10-130.0 hz and this and that\todo{finish this ofc}

\chaptertodo{
	seed per fscale but without results here\\
	describe why we limited by sllen 44100, 1-second processing, etc. (cite from demucs or cdae)
}

\subsubsection{GPU-accelerated BSS metrics with CuPy}

During the oracle evaluation, line-profiler showed that the slowest step was the BSS metrics calculation from the sigsep-mus-eval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} package. Listing \ref{lst:slowbssprofile} shows the line-profiler output of an evaluation of the IRM1 oracle mask with the STFT, using a window size of 4096, on the 14 MUSDB18-HQ validation tracks.

\chaptertodo{
	line profiler showing slow bss eval
}

The bottlenecks of the library use NumPy (\cite{numpy}) and SciPy (\cite{scipy}) operations, which are both Python numerical computation libraries which run on the the CPU. The CuPy (\cite{cupy}) library provides replacement functions of NumPy and SciPy which run on CUDA (\cite{cuda}), the compute framework for NVIDIA GPUs. This allows for acceleration of numerical operations through GPU parallelization.

Note that although PyTorch (\cite{pytorch}) also uses CUDA and was used in the neural network of this thesis, for this specific BSS evaluation library, CuPy was easier to work with as a drop-in NumPy/SciPy replacement.

\subsection{Open-Unmix with the sliCQ Transform}
\label{sec:slicqumx}

\todo[inline]{incorporate my sparsity hypothesis/justification for nsgt}

\todo[inline]{better hope umx shines here}

\subsubsection{Working with the ragged sliCQ Transform}

\ichfeedback{describe here how frequency bins are grouped by the same time-resolution, to produce a list of ``blocks'' of time-frequency coefficients}

\subsubsection{Convolutional neural network architecture}

\todo[inline]{grais and plumbley 2 papers can go here}

\subsubsection{Improved loss functions from CrossNet-Open-Unmix}

\end{document}
