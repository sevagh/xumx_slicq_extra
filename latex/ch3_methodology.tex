\documentclass[report.tex]{subfiles}
\begin{document}

\section{Methodology}

\subsection{sliCQT music demixing network design}

The proposed adaptation of Open-Unmix (\cite{umx}) to use the sliCQ Transform (sliCQT) (\cite{invertiblecqt, slicq}) is named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ}} and is the subject and main result of this thesis.

Open-Unmix (\cite{umx}) has a reference implementation released as an open-source Python codebase for general music demixing,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} which uses PyTorch (\cite{pytorch}) for GPU-accelerated numerical computation and deep learning, and includes a MUSDB18-HQ data loader (\cite{musdb18hq}) and BSS metrics evaluator (\cite{bss}), both necessary components for this thesis.

xumx-sliCQ will be based on the Open-Unmix PyTorch template, and as a result, all of the code developed will be in Python and with PyTorch code targeting the GPU. The rest of this chapter will be focused on the Python and PyTorch implementation details of xumx-sliCQ. The following tasks are required and will be described in each section of the chapter respectively:
\begin{tight_enumerate}
	\item
		Parameter selection for the sliCQT, in section \ref{sec:slicqparamsrch}
	\item
		PyTorch implementation of the sliCQT, in section \ref{sec:torchslicq}
	\item
		Using the sliCQT with selected parameters in Open-Unmix, in section \ref{sec:slicqumx}
\end{tight_enumerate}

\subsection{Choosing a sliCQT for music demixing}
\label{sec:slicqparamsrch}

The initial exploration is on whether the sliCQT is suitable as an STFT replacement for music demixing with magnitude spectrogram masking, since this is how Open-Unmix works. A simple approach is to experiment with the sliCQT replacing the STFT inside oracle mask estimators, which were described in \ref{sec:masksandoracles}. If the oracle experiments show promising results, such as the sliCQT oracle performance surpassing STFT oracle performance, it's a good first step towards replacing the STFT with the sliCQT in a neural network.

To recap, the oracle mask is a perfect time-frequency mask which is computed from ground-truth training data. The oracle is used to give an idea of the upper limit of audio quality from an algorithm or model for music demixing. The mask is typically applied to the magnitude transform (\cite{umx}), and the phase is discarded.

In this parameter exploration, the sliCQT does not need to run on the GPU -- that step is important in the deep learning network, which comes later. Therefore, we only require the following components to build the oracle testbench:

\begin{tight_enumerate}
	\item
		Reference NSGT/sliCQ (\cite{invertiblecqt}) library,\footnote{\url{https://github.com/grrrr/nsgt}} based on numpy and scipy, popular CPU-based Python numerical computation libraries (\cite{numpy, scipy})
	\item
		Python MUSDB18-HQ (\cite{musdb18hq}) loader library\footnote{\url{https://github.com/sigsep/sigsep-mus-db}}
	\item
		Python BSS metrics (\cite{bss}) evaluator library\footnote{\url{https://github.com/sigsep/sigsep-mus-eval/}} by the SigSep community
\end{tight_enumerate}

\subsubsection{Mix-phase inversion (MPI) oracle}

\textbf{N.B.} The words TFTransform (and iTFTransform for the inverse or backward transform) will be used to refer to either the STFT or sliCQT in the equations shown in this section, since both are complex and invertible time-frequency transforms with Fourier coefficients

As shown in section \ref{sec:masksandoracles}, the definition of the IRM1 oracle based on the STFT, or ideal ratio/soft mask with the magnitude spectrogram raised to the first power, for a mixed song waveform $x_{\text{mix}}[n]$ and one of its isolated sources $x_{\text{source}}[n]$, and the subsequent oracle-estimated source waveform $\hat{x}_{\text{source}}[n]$ are computed with the following equations in \ref{equation:irm1}:
\begin{align}\tag{1}\label{equation:irm1}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n]), X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), |X_{\text{source}}| = \text{abs}(X_{\text{source}})\\
	\nonumber & \text{IRM1}_{\text{source}} = \frac{|X_{\text{source}}|^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, IRM1}} = X_{\text{mix}} \cdot \text{IRM1}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, IRM1}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, IRM1}})
\end{align}

Similar to the IRM1 calculation, let's examine the real case where we only have the mix available, and the magnitude spectrogram of the source was estimated by a neural network. Let's call this $|X_{\text{source}}|_{\text{est}}$, and see how the source waveform is computed with ratio masks in equations \ref{equation:softneural}:
\begin{align}\tag{2}\label{equation:softneural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & \text{EstRatioMask}_{\text{source}} = \frac{{|X_{\text{source}}|_{\text{est}}}^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, est}} = X_{\text{mix}} \cdot \text{EstRatioMask}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

This is the soft mask output of Open-Unmix, which is supported by one of the configuration options. However, by default Open-Unmix prefers to use the estimated magnitude and the phase of the mixture, over soft masks. The source waveform is estimated using the source magnitude spectrogram and mix phase spectrogram, and show in equations \ref{equation:mpineural}:
\begin{align}\tag{3}\label{equation:mpineural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

In other words, the magnitude of the isolated source is combined with the phase, or the angle, of the mixed waveform, to produce a complex time-frequency transform, which is then inverted with the backward transform to obtain the estimated isolated source waveform. I discussed the idea of the mix-phase inversion oracle here\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/issues/83}} and here\footnote{\url{https://discourse.aicrowd.com/t/oracle-baselines/6211/4?u=sevagh}} with Stefan Uhlich and Fabian-Robert St{\"o}ter, authors of Open-Unmix.

There are some mentions in demixing or source separation literature for the ``noisy phase'' oracle (\cite{noisyphase1, noisyphase2}), which is a more speech-based definition -- recall that in speech separation, a common task is to separate speech from noise signals. In music demixing, referring to interfering musical instruments as ``noise'' is inappropriate, so we should call it the mix-phase inversion (or MPI) oracle.

Let's write out the equations for the MPI oracle derived from the mixed-phase neural network estimate equations from \ref{equation:mpineural}. The MPI oracle can be computed from ground-truth data, using the equations \ref{equation:mpioracle}:
\begin{align}\tag{4}\label{equation:mpioracle}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{source}}| = \text{abs}(X_{\text{source}}), \measuredangle{X_{\text{source}}} = \text{angle}(X_{\text{source}})\\
	\nonumber & \hat{X}_{\text{source, MPI}} = |X_{\text{source}}| \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, MPI}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, MPI}})
\end{align}

This is the first estimate of the mix-phase inversion strategy, and doesn't assume further post-processing of the estimates. It gives us the idea of the performance of a time-frequency transform of an isolated source when the phase is discarded and the magnitude is combined with the phase of the mixture.

The mix-phase inversion oracle (MPI) score is the target we will choose to maximize when searching for sliCQT parameters to beat the STFT.

\subsubsection{Frequency scale additions to the NSGT/sliCQT}

\subsubsection{Optimizing BSS metrics}

\chaptertodo{
as does museval: sigsep-mus-eval optimization in oracle section
}

\chaptertodo{
slice length and freq scales section goes to the parameter search
}

\subsubsection{MPI oracle parameter search testbench}

\chaptertodo{
	bayesian failed idea\\
	random iterations
}

\subsection{PyTorch GPU support for NSGT/sliCQT}
\label{sec:torchslicq}

The starting point for the sliCQ Transform used in this paper is the Python reference library\footnote{\url{https://github.com/grrrr/nsgt}} for the NSGT (\cite{slicq}) and sliCQ Transform. It was forked to my own GitHub profile,\footnote{\url{https://github.com/sevagh/nsgt}} where modifications were made to support its use in GPU deep learning, and to add some extra features such as new frequency scales.

\subsubsection{Matrix and ragged forms}

\todo[inline]{first describe the reference library's generator-based design for ragged and matrix}

ragged difficult to work with but:

\textcite{klapuricqt}
\begin{quote}
	There are at least three reasons why the CQT has not
	widely replaced the DFT in audio signal processing. Firstly,
	it is computationally more intensive than the DFT. Sec-
	ondly, the CQT lacks an inverse transform that would allow
	perfect reconstruction of the original signal from its trans-
	form coefficients. Thirdly, CQT produces a data structure
	that is more difficult to work with than the time-frequency
	matrix (spectrogram) obtained by using Short-Time Fourier
	transform in successive time frames. The last problem is
	due to the fact that in CQT, the time resolution varies for
	different frequency bins, in effect meaning that the ”sam-
	pling” of different frequency bins is not synchronized. In
	this paper, we propose solutions to these three problems.
\end{quote}

\subsubsection{Overlapping slices and plotting sliCQ spectrograms}

\ichfeedback{overlap-add 50\% slice business here too, showing its non-invertible, use Essentia to support argument, justifying how i need to add a final convolutional layer to grow by 50\%}

\todo[inline]{overlap essentia stuff: \url{https://mtg.github.io/essentia-labs/news/2019/02/07/invertible-constant-q/}}

\todo[inline]{show slicq overlap and plotting code from library}

\subsubsection{Frequency scales and slice length selection}

The reference library contains the octave and log scales, which are used for creating a constant-Q transform with logarithmic frequency spacing deriving from the 12-tone Western pitch scale. It also includes the Mel psychoacoustic scale.

In my fork of the NSGT library, two additional frequency scales were added:

\begin{enumerate}
	\item
		Bark psychoacoustic scale
	\item
		Variable-Q scale (\cite{variableq1,variableq2})
\end{enumerate}

\todo[inline]{intention to support minimum slice length support, Bark scale, VQ scale}


\subsubsection{PyTorch optimizations}

\todo[inline]{explain how this is necessary for neural networks and deep learning}

\todo[inline]{describe optimizations done here}

\todo[inline]{describe optimizations case by case or line by line?}

\subsubsection{Grouping frequency bins by time resolution}

\todo[inline]{call it the ragged transform}

\subsection{Open-Unmix with the sliCQ Transform}
\label{sec:slicqumx}

\todo[inline]{incorporate my sparsity hypothesis/justification for nsgt}

\todo[inline]{better hope umx shines here}

\subsubsection{Working with the ragged sliCQ Transform}

\ichfeedback{describe here how frequency bins are grouped by the same time-resolution, to produce a list of ``blocks'' of time-frequency coefficients}

\subsubsection{Convolutional neural network architecture}

\todo[inline]{grais and plumbley 2 papers can go here}

\subsubsection{Improved loss functions from CrossNet-Open-Unmix}

\end{document}
