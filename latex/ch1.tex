\documentclass[report.tex]{subfiles}
\begin{document}

\section{Introduction}
\label{ch:intro}

The study of acoustic signals forms the core of many fields including sonar, seismology, audio, music, and speech. Acoustic signals are functions of time, representing the evolving amplitude of the sound pressure wave \parencite{melbook}. Another way to characterize acoustic signals is by their frequency components, which can be computed from the Fourier Transform \parencite{melbook}.

One downside of the Fourier Transform is that it contains no temporal information; the Fourier Transform represents the frequency components of an acoustic signal by a sum of infinite sinusoids, but it does not describe how these frequencies evolve with time \parencite{gabordiagrams}. Conversely, many important signals such as speech and music contain frequency components that change with time \parencite{gabor1946}. For these signals, joint time-frequency analysis is required.

Time-frequency analysis is performed by multiplying the signal being studied by finite, consecutive windows of a short duration, and taking the Fourier Transform of each windowed section. This is also referred to as the Short-Time Fourier Transform (STFT) or spectrogram \parencite{melbook}. \textcite{melbook} describe the resulting spectral analysis as an important technique in analyzing and understanding audio that has been in use since the 1930s.

Music source separation is the task of extracting an estimate of one or more isolated sources or instruments (for example, drums or vocals) from musical audio \parencite{musicsepgood, musicmask}. The task of music demixing or unmixing considers the case where the musical audio is separated into an estimate of all of its constituent sources that can be summed back to the original mixture.\footnote{Thanks to Fabian-Robert St{\"o}ter for the discussion on the difference between music source separation, demixing, and unmixing.} Music demixing systems commonly split mixed songs into four sources: vocals, drums, bass, and other (guitar, piano, etc.), following the example of mixing and demixing datasets \parencite{sisec2016, otherdataset2, musdb18, musdb18hq}.

The STFT is a popular tool used by various music demixing systems throughout the years \parencite{musicsepgood, musicmask, sisec2018, fitzgerald1, umx, plumbley1, plumbley2}. However, the STFT is subject to a fixed and bounded time-frequency resolution, such that one cannot have maximal resolution in both time and frequency, and must trade them off by changing the duration of the window \parencite{gabor1946, gabordiagrams}.

The tradeoff in time-frequency resolution is an important consideration in music demixing \parencite{tftradeoff1, tftradeoff2}. Systems have been proposed that use multiple STFTs \parencite{fitzgerald2, driedger} or that replace the STFT with different time-frequency transforms \parencite{fitzgerald2, cqtseparation, bettermusicsep}, to improve results through the manipulation of time-frequency resolution.

\subsection{Motivation}
\label{sec:motivation}

Musical signals have characteristics that lead to conflicting requirements in the STFT. According to \textcite{doerflerphd}, music needs to be analyzed with long-duration windows in the low-frequency region for a high frequency resolution, because the bass notes lay the harmonic basis of a song. Conversely, the high-frequency region contains transients and broadband sounds, which are useful for timbre identification and rhythm; these transients have fast attacks and decays and need to be analyzed with short-duration windows \parencite{doerflerphd}. \citeauthor{cqtransient} similarly state that

\begin{quote}
	a well known disadvantage of the STFT is the rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies. In contrast to this we know that due to both musical and auditory aspects frequency resolution is preferred that increases from high to low frequencies (and vice versa for time resolution) \parencite[1]{cqtransient}.
\end{quote}

Figure \ref{fig:stfttradeoff} shows a musical glockenspiel signal analyzed with STFTs using different window sizes. Note how in Figure \ref{fig:stfttradeoff}(a), due to the high frequency resolution, the temporal events are blurry or smeared such that the times of note onsets are unclear. The inverse case is shown in Figure \ref{fig:stfttradeoff}(b), which contains sharp localization of note onsets from the high time resolution, but blurry or smeared frequency components.

\begin{figure}[ht]
	\centering
	\subfloat[Wide window STFT (93ms)]{\includegraphics[width=0.6563\textwidth]{./images-tftheory/tf_tradeoff_balasz2.png}}\\
	\subfloat[Narrow window STFT (6ms)]{\includegraphics[width=0.6563\textwidth]{./images-tftheory/tf_tradeoff_balasz1.png}}\\
	\caption{Different window size STFT spectrograms of a glockenspiel signal \parencite[1]{jaillet}}
	\label{fig:stfttradeoff}
\end{figure}

\textcite{dictionary} note that the first use of the STFT was by \textcite{gabor1946} to analyze a speech signal, which used fixed-size Gaussian windows. This was called the Gabor transform, and is now considered a special case of the STFT \parencite{dictionary}.

The Constant-Q Transform (CQT) was originally proposed by \textcite{jbrown} to analyze musical signals with a logarithmic frequency scale to better show the relationship between the fundamental frequency of a musical instrument and its harmonics. The resulting transform used long-duration windows in the low frequency regions and short-duration windows in the high frequency regions. The visual comparison of the original Constant-Q Transform and the Fourier Transform is shown in Figure \ref{fig:violin}. From the use of short windows in the high frequency regions, the CQT demonstrates good time resolution \parencite{cqtransient} in the region of transients and broadband signals, which are important for timbre and instrument identification \parencite{timbretransients1, timbretransients2}.

\begin{figure}[ht]
	\centering
	\subfloat[Linear-frequency Fourier transform]{\includegraphics[width=0.6563\textwidth]{./images-tftheory/violindft.png}}\\
	\subfloat[Constant-Q transform]{\includegraphics[width=0.6563\textwidth]{./images-tftheory/violincqt.png}}
	\caption{Violin playing the diatonic scale, $G_{3} \text{(196Hz)} - G_{5} \text{(784Hz)}$ \parencite[430]{jbrown}}
	\label{fig:violin}
\end{figure}

Building on the Gabor transform (or STFT), and using the Constant-Q Transform as the motivating application, \textcite{balazs} proposed the Nonstationary Gabor transform (NSGT), a time-frequency transform with varying time-frequency resolution and a perfect inverse. The NSGT is computed by varying the size of the window on which the Fourier Transform taken. The NSGT spectrogram of the musical glockenspiel signal is shown in Figure \ref{fig:nsgttradeoff}, where one can observe minimal blurriness (or good resolution) in both time and frequency, which is an improvement over the STFT spectrogram seen in Figure \ref{fig:stfttradeoff}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6563\textwidth]{./images-tftheory/tf_tradeoff_balasz3.png}
	\caption{NSGT spectrogram of a glockenspiel signal, varying window (6--93ms) \parencite[4]{jaillet}}
	\label{fig:nsgttradeoff}
\end{figure}

\subsection{Thesis objectives and related work}

STFT-based music demixing techniques may be limited by the fixed time-frequency resolution of the STFT. The objective of this thesis is to replace the STFT in a music demixing model with the sliCQT, which has a varying time-frequency resolution. Open-Unmix \parencite{umx} is a state-of-the-art deep neural network for music demixing based on the STFT, with an open-source reference implementation.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} The goal of this thesis is to adapt Open-Unmix to create a usable music demixing software application based on the sliCQT.

Several music demixing papers \parencite{fitzgerald1, driedger, tftradeoff1, tftradeoff2} describe the use of different window sizes of the STFT to improve source separation performance. By contrast, using the NSGT instead of the STFT may be of interest, as it is a single transform which can replace the need to explicitly trade-off of time and frequency resolution.

Other music demixing systems have been proposed that use the CQT instead of the STFT \parencite{fitzgerald2, cqtseparation, bettermusicsep}. However, these papers used implementations of the CQT which lacked a stable inverse \parencite{lackinverse}, and for which different approximate inversion schemes have been proposed \parencite{klapuricqt, fitzgeraldcqt}. The NSGT is a generalization of the CQT, and can be used to construct a CQT with perfect inverse, or CQ-NSGT \parencite{invertiblecqt, variableq1}.

The sliCQ Transform (sliCQT), whose name means \textit{sliced Constant-Q transform}, was introduced by \textcite{slicq} to compute the NSGT on slices of the audio signal instead of the entire signal, allowing realtime processing. The NSGT and sliCQT can in practice be constructed with any frequency scale that is monotonically increasing, and can use frequency scales that are appropriate for the task, for example, musical or psychoacoustic scales. Additionally, the NSGT and sliCQT have open-source reference implementations in MATLAB\footnote{\url{http://ltfat.org/doc/filterbank/cqt_code.html}} and Python,\footnote{\url{https://github.com/grrrr/nsgt}} allowing for reproducibility and further research.

\subsection{Contribution and results}

My first contribution in this thesis is the adaptation of the reference Python NSGT and sliCQT library\footnote{\url{https://github.com/grrrr/nsgt}} to use PyTorch,\footnote{\url{https://pytorch.org}} a modern deep learning framework with graphical processing unit (GPU) support. This allows the NSGT and sliCQT to be computed on the GPU, which, due to its capabilities for parallel computing, is the preferred device on which to train deep neural networks \parencite{pytorch}. The resulting library, which I released as open-source software,\footnote{\url{https://github.com/sevagh/nsgt}} allows future researchers to use the NSGT and sliCQT in any GPU-based machine learning or deep learning model.

My second contribution in this thesis is the adaptation of Open-Unmix, a deep neural network for music demixing\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} \parencite{umx}, to replace the STFT with the sliCQT. Ideas were also incorporated from CrossNet-Open-Unmix \parencite{xumx}, which is a variant of Open-Unmix, and the Convolutional Denoising Autoencoder \parencite{plumbley1}, a different deep neural network for music demixing. The final result, named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ/tree/main}} is a neural network that performs the task of music demixing using the sliCQT.

\subsection{Outline}

This thesis is organized as follows. In Chapter \ref{ch:background}, I will cover the background of acoustic signals, frequency analysis, and the important transforms in this thesis including the Fourier Transform, STFT, CQT, NSGT, and sliCQT. I will also present an overview of machine learning for audio and music signals, an introduction to software and Python code concepts, and a survey of music source separation and music demixing. In Chapter \ref{ch:methodology}, I will describe the methodology for the two objectives of the thesis, which are the PyTorch adaptation of the NSGT/sliCQT library, and the replacement of the STFT with the sliCQT inside Open-Unmix. In Chapter \ref{ch:experiment}, I will show and discuss the experimental results, and in Chapter \ref{ch:conclusion} I will conclude the thesis.

\end{document}
