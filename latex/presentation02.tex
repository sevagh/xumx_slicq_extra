\documentclass[usenames,dvipsnames]{beamer}
\usetheme{Boadilla}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{multimedia}
\usepackage{fancyvrb}
\usepackage{soul}
\usepackage{multicol}
\usepackage{optparams}
\usepackage{adjustbox}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning}
\newcommand{\foo}{\hspace{-2.3pt}$\bullet$ \hspace{5pt}}
\usepackage{subfig}
\usepackage[
    backend=biber,
    natbib=true,
    style=numeric,
    sorting=none,
    style=verbose-ibid,
    maxcitenames=1, %remove this outside of toy presentations
]{biblatex}
\addbibresource{citations.bib}
\usepackage{pgfpages}
\usepackage{xcolor}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{burgundy}{rgb}{0.5, 0.0, 0.13}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}
\setbeameroption{hide notes}
\input{variables.tex}

\title{Improving Open-Unmix for Music Source Separation}
\subtitle{Brainstorming session}
\author{Sevag Hanssian}
\institute{DDMAL, McGill}
\setbeamertemplate{navigation symbols}{}

\AtEveryBibitem{%
  \clearfield{pages}%
  \clearfield{volume}%
  \clearfield{number}%
  \clearlist{journal}%
  \clearfield{booktitle}%
}

\renewbibmacro{in:}{}

\AtEveryCitekey{%
  \clearfield{pages}%
  \clearfield{volume}%
  \clearfield{number}%
  \clearfield{doi}%
  \clearfield{journal}%
  \clearlist{journal}%
  \clearfield{booktitle}%
  \clearfield{isbn}%
  \clearfield{title}%
  \clearfield{url}%
\ifentrytype{article}{
    \clearfield{journal}%
}{}
\ifentrytype{inproceedings}{
    \clearfield{booktitle}%
}{}
}

\begin{document}

\begin{frame}
\maketitle
\end{frame}


\begin{frame}
	\adjustbox{valign=c}{{\usebeamerfont{frametitle}\usebeamercolor[fg]{frametitle}Timeline}}\hfill
	\adjustbox{valign=c}{
	\newcounter{year}
	\begin{tikzpicture}[yscale=0.5,%
		   year/.style={draw=red,text=blue,fill=yellow!20,shape=ellipse,inner sep=2pt},
		   description/.style={rectangle,align=left,text width=60mm,anchor=west},
		   timeline/.style={->,thick,red!50}]

	    \foreach \year/\desc [count=\y] in {%
	       01-2020/Looked at HPSS (harmonic-percussive source separation) in MUMT 501,%
	       01-2021/Implemented HPSS with CQT in MUMT 622,%
		01-2021/Studied the NSGT-sliCQ (perfect CQT implementation) in MUMT 622,%
	       05-2021/Decided on thesis topic: adapt Open-Unmix neural network to replace STFT with NSGT-sliCQ%
	       } { \ifnum\y=1 \node[description](\y){\desc};
		   \else\node[description,below=1ex of \z](\y){\desc};
		   \fi
		   \node[year](y-\y) [left=of \y] {\year};
		   \ifnum\y>1\draw[timeline] (y-\z)-- (y-\y);\fi
		   \global\let\z=\y% for drawing from last node
	       }

	\end{tikzpicture}
	}
\end{frame}

\begin{frame}
	\frametitle{Competition}
	By pure luck, there is a May-August 2021 competition by the creators of Open-Unmix: \href{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}\\
	\includegraphics[height=2.5cm]{./challenge.png}\\
	Deadlines:
	\begin{enumerate}
		\item
			\st{Round 1: May 3rd - June 13th, 12 PM UTC} - hidden dataset A, already finished
		\item
			Round 2: June 14th - July 31st, 12 PM UTC - hidden dataset B
		\item
			Team Freeze deadline: 23rd July, 12 PM UTC
	\end{enumerate}
	All submissions get evaluated on a full hidden dataset in August (meaning I have until July 31 to submit a winner)
\end{frame}

\begin{frame}
	\frametitle{Competition pt2}
	I made 1 submission to Round 1: tiny tweaks (LSTM $\rightarrow$ GRU, swap batchnorm and activations, etc.), added my Periphery stems (4 albums) dataset to MUSDB18-HQ to try to train with extra metal data\\
	\includegraphics[height=4cm]{./top5.png}
	\includegraphics[height=2.17cm]{./currentplace.png}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Music Source Separation}
	\begin{enumerate}
		\item
			When producing music, instruments are recorded separately (aka stems), and combined (linear mixture) to make mixed song -- commonly 4 sources, bass, drums, vocals, other, deriving from MUSDB18-HQ dataset \footcite{musdb18hq}, e.g.:
			\begin{verbatim}
			# ls MUSDB18-HQ/train/Night\ Panther\ -\ Fire/
			vocals.wav mixture.wav drums.wav other.wav bass.wav
			\end{verbatim}
		\item
			On the flip side, given a mixed song, we want to decompose it back into its stems (without having access to the stems)
	\end{enumerate}
	\begin{figure}[ht]
		\centering
		\subfloat{\includegraphics[height=2.5cm]{./mixing.png}}
		\hspace{0.1em}
		\subfloat{\includegraphics[height=2.5cm]{./demixing.png}}
		\caption{Mixing and ``demixing'' or source separation}
	\end{figure}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Neural networks for Music Source Separation}
	\begin{enumerate}
		\item
			\textbf{Ground truth:} source stems that are combined to make mixed song -- commonly 4 sources, bass, drums, vocals, other. Also very commonly 1 network per source
		\item
			\textbf{Network inputs:} mixed waveform and 1 source -- direct waveform, or a TFR (time-frequency representation)  e.g. STFT
		\item
			\textbf{Estimate source:} given the mixed input, learn how to extract a source. Common approach: learnable magnitude-STFT mask, e.g. same shape as spectrogram but $\in [0.0, 1.0]$, to multiply mixed spectrogram with
		\item
			\textbf{Back to waveform:} after estimating the source, convert it back to waveform -- can use masking or mix-phase inversion. Mixture supplies the STFT phase, estimates only use magnitude
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Visualizations}
	\begin{figure}[ht]
		\includegraphics[height=2.5cm]{./mask_simple.png}
		\caption{Example of a binary mask applied to an STFT}
	\end{figure}
	\begin{figure}[ht]
		\includegraphics[height=2.5cm]{./whynophase.png}
		\caption{Why we don't try to learn phase in neural networks}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Open-Unmix}
	Very open source, near-SOTA music source separation. Intended to be a ``platform'' for future MSS research\footcite{umx}
	\begin{figure}[ht]
		\includegraphics[height=3cm]{./umx1.png}
		\caption{Open-Unmix diagram}
	\end{figure}
	UMX estimates source magnitude STFT with a nonnegative mask (ReLU)\\
	4 networks, 1 per source (bass, drums, vocals, other)
\end{frame}

\begin{frame}
	\frametitle{Getting the stem waveform}
	Recall: output of unmix is the estimated \textbf{magnitude STFT} of the vocal stem\\\ \\

	Open-unmix uses phase of mixture:
	\begin{enumerate}
		\item
			\textbf{x, inference:} x = mix waveform
		\item
			\textbf{Estimate Ymag, vocals:}\\
			\qquad $X_{\text{mag}} = abs(\text{STFT}(x))$, $\hat{Y}_{\text{mag}} = \text{network}(X_{\text{mag}})$
		\item
			\textbf{Mix phase inversion:}\\
			\qquad $X_{\text{phase}} = atan2(\text{STFT}(x))$\\
			\qquad $\hat{Y}_{\text{complex}} = \hat{Y}_{\text{mag}} + j X_{\text{phase}} \rightarrow \hat{y} = \text{iSTFT}(\hat{Y}_{\text{complex}})$
		\item
			Evaluation: BSS (SDR, SAR, SIR, ISR) on $\hat{y}, y_{\text{groundtruth}}$
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Oracle, or ideal, performance}
	Mix phase oracle (hasn't been evaluated before). Given ground truths, \textbf{how good can source separation be?} Assuming a perfect estimate from the neural network\\

	Using ground-truth instead of network predictions:
	\begin{enumerate}
		\item
			\textbf{x, y ground truth:} x = mix waveform, y = vocals
		\item
			\textbf{Ideal mix phase inversion:}\\
			\qquad $X_{\text{phase}} = atan2(\text{STFT}(x))$\\
			\qquad $Y_{\text{mag}} = abs(\text{STFT}(y))$\\
			\qquad $Y_{\text{ideal complex}} = Y_{\text{mag}} + j X_{\text{phase}} \rightarrow y_{\text{ideal}} = \text{iSTFT}(Y_{\text{ideal complex}})$
		\item
			Evaluation: BSS (SDR, SAR, SIR, ISR) on $y_{\text{ideal}}, y_{\text{groundtruth}}$
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{STFT vs. NSGT oracle performance}
	Using SDR only (the most important score):\\
	STFT, UMX default (window = 4096): vocals 8.62, drums 7.07, bass 6.68, other 6.87\\

	NSGT (with maximum slice length capped at 8192 frames):\\

	Bass NSGT = Bark, 105 bins, 25.4-22050 Hz, slice = 7180 = 8.47\\
	Drums NSGT = Mel, 104 bins, 49.3-22050 Hz, slice = 7108 = 10.5\\
	Other NSGT = Bark, 64 bins, 90.0-22050 Hz, slice = 4416 = 13.94\\
	Vocals NSGT = Mel, 116 bins, 37.7-22050 Hz, slice = 8024 = 9.83\\\ \\

	\textbf{Hypothesis}:
	\begin{enumerate}
		\item
			If one can prepare a neural network that can estimate the magnitude-sliCQ-NSGT
		\item
			Then using the above configurations can get significant SDR gains since they perform considerably better in the mix-phase oracle
		\item
			UMX-STFT: estimate abs(STFT-4096) $\rightarrow$ \\
			\qquad UMX-NSGT: estimate abs(NSGT-Bark-105-25.4Hz-7180)
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{sliCQ in UMX, dimensionality visuals}
	\includegraphics[height=6.5cm]{./umxslicqdimred.png}
\end{frame}

\begin{frame}
	\frametitle{sliCQ spectrograms}
	\begin{figure}
		\subfloat[Overlap]{\includegraphics[height=4cm]{./spectrogram_overlap.png}}
		\subfloat[Flatten time, no overlap]{\includegraphics[height=4cm]{./spectrogram_nooverlap.png}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{sliCQ overlap -- 2 problems}
	\begin{enumerate}
		\item
			50\% overlap is not an invertible operation
		\item
			Attempts: learn from 50\% overlap:\\
			$\text{input} = 117 \times 16000$\\
			$\text{overlap-add} \rightarrow 117 \times 8000$ -- proper patterns here
			$117 \times 8000 \rightarrow 117 \times 8000 \text{ mask}$\\
			Then grow the learned mask to the 2x dimension of the original transform:\\
			$\text{ ConvTranspose2d(stride=(1,2))}(117 \times 8000) \rightarrow 117 \times 16000$\\
			Also tried Linear layers, etc.
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{NSGT-sliCQ inside Open-Unmix}
	NSGT-sliCQ choices:
	\begin{itemize}
		\item
			Reduce to 2d - flatten 2 out of 3 axes -- same dimensionality as an STFT
		\item
			Treat as 3-dimensional sequence of 2d time-frequency frames!\\
			2-d spectrogram = image (RGB x width x height $\rightarrow$ mono/stereo x time x frequency)\\
			3-d sliCQ = video (temporal frame x mono/stereo x (2d time x frequency)\\
			$\text{Slice} \times (\text{Frequency} \times \text{Time-in-slice})$
	\end{itemize}
	Input size:
	\begin{enumerate}
		\item
			input stft, 6s audio = abs(STFT(4096)) = $258 \times 2049 \approx 500,000$
		\item
			input slicq, 6s audio = abs(sliCQ(mel,116,37.7)) = $67 \times 117 \times 244 \approx 1,912,716$
		\item
			UMX has ~1 million parameters for 500,000-sized input. Scale accordingly? Do I necessarily need a model that has 4 million parameters, or similar?
		\item
			Leaning towards CNN for large, structured inputs
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{STFT in UMX, visual}
	\includegraphics[height=3cm]{./umxstftlstm.png}\\
	Adjacent STFT frequency vectors have a temporal overlap (hop) with the previous -- so, the LSTM is already somewhat dealing with overlapping information
\end{frame}


\begin{frame}
	\frametitle{3d -> 2d sliCQ in UMX LSTM, option 1}
	\includegraphics[height=2.7cm]{./umxslicqlstm1.png}\\
	Big problem here is the time sequence goes from \textasciitilde 250 to \textasciitilde 16,000 -- no matter what vanishing gradient, gradient clipping, GRU, activations, etc. you play with, 16000 may be simply intractable for an RNN for this problem?\\
	Ideas tried (not very successfully):
	\begin{enumerate}
		\item
			Conv layers to reduce time dimension
		\item
			Max and average pooling to reduce time dimension
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{3d -> 2d sliCQ in UMX LSTM, option 2}
	\includegraphics[height=2.7cm]{./umxslicqlstm2.png}\\
	Alternative ideas include:
	\begin{enumerate}
		\item
			Linear decoder for $117 \times 244 = 28548 \rightarrow 512$, dim reduction of 28548 ``flattened time-frequency'' coefficients within each time step/slice
		\item
			Convolutional decoder for $117 \times 244 \rightarrow 10 \times 24$, dim reduction\\
			temporally evolving sequence of 67 slices of $10 \times 24 = 240$ ``time-frequency'' feature maps
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{3d sliCQ CDAE, option 3}
	\includegraphics[height=2.7cm]{./umxslicqcdae.png}\\
	\textbf{n.b.} My personal preferred/favorite idea. Why?
	\begin{enumerate}
		\item
			Exploit natural 3d structure of sliCQ, versus concatenations/reshapings that may not be correct
		\item
			Low params, powerful, and expressive - used with STFT to good effect \footcite{plumbley1, plumbley2}
		\item
			Not dealing with two distinct temporal dimensions in an LSTM
		\item
			I've done something similar \href{https://github.com/sevagh/MiXiN}{https://github.com/sevagh/MiXiN} -- not award-winning, but it worked, kinda...
	\end{enumerate}
\end{frame}

\begin{frame}[fragile]
	\frametitle{Debugging ideas}
	The ``golden test'': overfit to 2 training examples (2x 6s segments) + 1 validation example (1x 12s segment)
	\begin{verbatim}
        /home/sevagh/musdbdebug/
        └── train
            ├── Actions - One Minute Smile
            ├── Night Panther - Fire
            └── Steven Clark - Bounty
	\end{verbatim}
	Gradient debugging with Tensorboard
\end{frame}

\begin{frame}
	\frametitle{Resources}
	umx-junkyard: all my wild experiments (conv2d,3d, lstm, gru, dilations, maxpools, etc.)\\
	\includegraphics[height=6cm]{./junkyard.png}\\
	easy to copy-paste blocks of code from various configurations
\end{frame}

\begin{frame}
	\frametitle{Tensorboard}
	\begin{figure}
		\centering
		\subfloat[Loss = MSE, SI-SDR = source sep performance]{\includegraphics[width=3cm]{./tboard1.png}}
		\subfloat[Spectrograms of X, Y, Yest]{\includegraphics[width=3cm]{./tboard2.png}}
		\subfloat[Audio clips of validation separation]{\includegraphics[width=3cm]{./tboard3.png}}
		\subfloat[Gradients]{\includegraphics[width=3cm]{./tboard4.png}}
	\end{figure}
\end{frame}

\end{document}
