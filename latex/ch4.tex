\documentclass[report.tex]{subfiles}
\begin{document}

\section{Experiment}
\label{sec:experiment}

\todo[inline]{ch4 ich feedback: section introductions, and don't mix discussions or conclusions, separate those into ch5. list all hyperparams}

This section contains the results of the experiments described in the previous section \ref{sec:methodology}. Refer to appendix \ref{appendix:computerspec} for the hardware and software specifications of the computer on which the experiments and results in this section were generated from.

\subsection{Benchmarks for GPU accelerations}

The following sections show benchmark results to demonstrate the speedup of the sliCQT and BSS eval libraries achieved from their GPU ports described in sections \ref{sec:torchslicq} and \ref{sec:fasterbsscupy} respectively.

\subsubsection{Faster NSGT/sliCQT with PyTorch}

In section \ref{sec:torchslicq}, it was described that the sliCQT Python library uses the CPU-based NumPy library, and that it was necessary to port the code to the GPU-enabled PyTorch deep learning framework to allow it to run directly on the GPU in the deep neural network of Open-Unmix.

The first benchmark script is for the PyTorch implementation of the NSGT/sliCQT library from section \ref{sec:torchslicq}. My copy of the library contains the PyTorch implementation and the benchmark scripts.\footnote{\url{https://github.com/sevagh/nsgt}, \url{https://github.com/sevagh/nsgt/blob/main/examples/benchmark.py}, \url{https://github.com/sevagh/nsgt/blob/main/examples/run_bench.sh}} The benchmark performs the forward and backward sliCQT using the Bark scale with 50 bins between 20--22,050 Hz. The sliCQT is taken on a 3:54 minute mixed song from the MUSDB18-HQ dataset, ``Zeno - Signs.'' The sliCQT parameters were chosen so that the transform of the song occupied a maximum of 7.2 GB of memory and could fit on the device with the least memory, the NVIDIA 2070 Super. The computation time was measured and averaged across 100 iterations. The cost of the memory transfer of the song to the GPU was not included in the measurement. Tables \ref{table:nsgttorchresultsmatrix} and \ref{table:nsgttorchresultsragged} show the benchmark results for the matrix and ragged sliCQT respectively.

\begin{table}[ht]
	\centering
	\caption{Execution times for the matrix form of the forward + backward sliCQT}
	\label{table:nsgttorchresultsmatrix}
	\begin{tabular}{ |l|l|l|l| }
	 \hline
		NSGT library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU, multithreaded & 60.56  \\
	 \hline
		Original & CPU, single-threaded & 9.50  \\
	 \hline
		PyTorch & CPU & 3.10  \\
	 \hline
		PyTorch & GPU (NVIDIA 2070 Super) & 2.62 \\
	 \hline
		PyTorch & GPU (NVIDIA 3080 Ti) &  2.40 \\
	 \hline
\end{tabular}
\end{table}

\begin{table}[ht]
	\centering
	\caption{Execution times for the ragged form of the forward + backward sliCQT}
	\label{table:nsgttorchresultsragged}
	\begin{tabular}{ |l|l|l|l| }
	 \hline
		NSGT library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU, multithreaded & 51.95  \\
	 \hline
		Original & CPU, single-threaded & 8.72  \\
	 \hline
		PyTorch & GPU (NVIDIA 2070 Super) & 2.52 \\
	 \hline
		PyTorch & GPU (NVIDIA 3080 Ti) &  2.38 \\
	 \hline
		PyTorch & CPU & 1.85  \\
	 \hline
\end{tabular}
\end{table}

The execution time of the transform improved with PyTorch. The GPUs have an advantage over the CPU in the matrix form of the transform, but not in the ragged form. This can be explained by the fact that the ragged transform is a list of tensors. To perform operations on the ragged transform requires looping over the list which negates some of the benefits of GPU parallelism. Even though the GPU is not faster than the CPU for the tested parameters of ragged sliCQT, having the transform on the GPU allows us to use the sliCQT inside a PyTorch neural network. The original library has a multithreaded option which performs worse than the default single-threaded behavior, but the PyTorch CPU performance beats both.

\subsubsection{Faster BSS metrics with CuPy}

In section \ref{sec:fasterbsscupy}, it was described that the BSS eval step was the slowest part of evaluating oracle performance, which is the core step in the sliCQT parameter search. Replacing the CPU-based NumPy and SciPy libraries with the GPU-enabled CuPy library for math operations would speed up the oracle evaluations and allow the sliCQT parameter search script to run more efficiently.

The second benchmark script is for the GPU-accelerated BSS metrics computation using the CuPy library from section \ref{sec:fasterbsscupy}. Recall that the copy of the library\footnote{\url{https://github.com/sevagh/sigsep-mus-eval}} contains the modifications made for this thesis. For the 14 validation tracks of MUSDB18-HQ, the benchmark script\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/blob/main/mss_evaluation/mss-oracle-experiments/oracle_eval/benchmark_cupy_eval.py}}  creates a random estimate (using \Verb#numpy.randn# to create an audio signal) and computes the BSS metrics. It measures the time for each BSS metric computation, summed across 14 tracks and repeated for 10 iterations. The final time is the total time for all 14 tracks, divided by 10 to account for the iterations. Three copies of the script were run; one with the argument \Verb#--disable-cupy# to use the CPU code with SciPy and NumPy functions on a Ryzen 5950X with 64GB RAM, one with the argument \Verb#--cuda-device=0# to use the NVIDIA 3080 Ti GPU, and one with the argument \Verb#--cuda-device=1# to use the NVIDIA RTX 2070 Super GPU.

The results were that the regular BSS metrics evaluation on the CPU took 1,485.77 seconds, compared to the CuPy GPU-accelerated code which took 585.33 seconds with the NVIDIA 3080 Ti GPU and 738.39 seconds with the weaker NVIDIA 2070 Super GPU. The speedup from CuPy is \textasciitilde 2-2.5x over the Ryzen 5950X CPU.

\newpagefill

\subsection{Best sliCQT parameters with MPI oracle}

As described in section \ref{sec:slicqparamsrch}, a random parameter search was undertaken to choose the parameters of the sliCQT. The parameter search script\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/v2021/scripts/slicq_explore.py}, \url{https://github.com/sevagh/xumx-sliCQ/blob/v2021/docs/slicq_params.md}} computes the median SDR score of the MPI oracle across all four targets and the 14 validation tracks of MUSDB18-HQ. The script ran for 60 iterations using random combinations of sliCQT parameters chosen from the ranges in Table \ref{table:slicqparams} in section \ref{sec:slicqparamsrch}.

The best-performing sliCQT parameters chosen by the script used the following parameters: Bark scale, 262 bins, 32.9--22,050 Hz. These parameters were used in the final neural network, xumx-sliCQ.

\begin{figure}[ht]
	\centering
	\subfloat[Best sliCQT from MPI oracle parameter search]{\includegraphics[width=0.675\textwidth]{./images-gspi/slicqt_good.png}}\\
	\subfloat[STFT with UMX defaults]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_stft_4096.png}}
	\caption{Magnitude spectrogram comparison; sliCQT vs. STFT}
	\label{fig:bipolarslicqs}
\end{figure}

The full oracle evaluation on these sliCQT parameters are shown in the boxplot in Figure \ref{fig:oraclebssboxplot}, alongside different STFT window sizes. The performance of both the MPI (mix-phase inversion) and the IRM1 (soft magnitude mask) oracles are shown. The MPI oracle was described in section \ref{sec:noisyphaseoracle}, and the IRM1 oracle was described in section \ref{sec:masksandoracles}. These oracles represent both possible configurations of UMX, which can use either the mix-phase or ratio mask to compute the final waveform.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-bss/oracle_boxplot.pdf}
	\caption{Boxplot for oracle mask evaluations}
	\label{fig:oraclebssboxplot}
\end{figure}

The slice length and transition length for the 262-bin Bark scale sliCQT with frequency limits of 32.9--22,050 Hz are automatically chosen to be 18,060 and 4,514 samples respectively, using the automatic slice length picker described in section \ref{sec:improvelib}.

The median SDR achieved by the chosen sliCQT parameters in the MPI oracle was 7.42 dB, beating the 6.23 dB achieved by the STFT with the UMX default window and overlap of 4,096 and 1,024 samples respectively. This indicates that a music demixing system that uses the sliCQT with these parameters has the potential to surpass STFT-based performance.

Keep in mind that to translate this theoretical boost to a real advantage in the final model, a neural network architecture must be chosen that can produce a good estimate of the target magnitude sliCQT. Note the sharper clarity of musical events in both time and frequency in the sliCQT spectrogram in Figure \ref{fig:bipolarslicqs}(a) compared to the STFT spectrogram in Figure \ref{fig:bipolarslicqs}(b). We hope that this added information gives the neural network an advantage when learning how to demix music from the sliCQT representation over the STFT.

\newpagefill

\subsection{xumx-sliCQ neural network}

This section will show the training procedure and final trained performance of the xumx-sliCQ neural network, whose design was described in section \ref{sec:neuralnet}.

\subsubsection{Learning parameters and training curves}

This section will cover the details of the hyperparameters of xumx-sliCQ related to the learning of the neural network.

The training uses an Adam optimizer which runs for 1,000 iterations (or epochs), with a learning rate of 0.001 and a weight decay of 0.00001. There is also an adaptive learning rate scheduler with a decay patience of 80 and a decay gamma of 0.3. These were copied from UMX and X-UMX, unchanged.

Recall from section \ref{sec:xumxinc} that X-UMX includes the cross-domain loss (CL), with a mixing coefficient between the magnitude spectrogram loss and the time-domain SDR loss. The mixing coefficient in X-UMX is 10.0, but in xumx-sliCQ it is set to 0.1 to reflect the observed order of magnitude difference in the MSE loss of the STFT and the sliCQT.

xumx-sliCQ reports 6,669,912 trainable parameters in total from the torchinfo\footnote{\url{https://github.com/TylerYep/torchinfo}} package. The training curves can be seen in Figure \ref{fig:networkloss}, visualized using Tensorboard,\footnote{\url{https://www.tensorflow.org/tensorboard/}} a training visualization tool.

\begin{figure}[ht]
	\centering
	\subfloat[Train loss]{\includegraphics[width=\textwidth]{./images-neural/train_loss.png}}\\
	\subfloat[Validation loss]{\includegraphics[width=\textwidth]{./images-neural/valid_loss.png}}
	\caption{Tensorboard loss curves for xumx-sliCQ, 1,000 epochs}
	\label{fig:networkloss}
\end{figure}

\newpagefill

\subsubsection{Wiener-EM post-processing results}
\label{sec:wienerconfigs}

We previously discussed in section \ref{sec:postprocessing} that after getting the initial estimate with the xumx-sliCQ neural network, there is a post-processing Wiener-EM step that could be done with the sliCQT or the STFT. In this section, we will discuss the execution times and SDR scores of the different configurations of Wiener-EM post-processing.

Note that the default number of iterations for the Wiener-EM step is one, in UMX, X-UMX, and xumx-sliCQ. Anything above one iteration had worse demixing results, and was computationally slower as well. This is shown in Table \ref{table:wienerem}, where five different configurations of xumx-sliCQ's post-processing Wiener-EM were tested on a single track, ``Al James - Schoolboy Facination.''

\begin{table}[ht]
	\centering
	\caption{SDR of Wiener-EM post-processing with the STFT and sliCQT}
	\label{table:wienerem}
	\begin{tabular}{ |l|l|l|l| }
	 \hline
		Wiener-EM representation & Iterations & Median SDR & Execution time (s) \\
	 \hline
	 \hline
		sliCQT, ragged & 1 & 2.221 & 479.104 \\
	 \hline
		sliCQT, zero-padded matrix & 1 & 2.216 & 268.061  \\
	 \hline
		STFT & 1 & 2.143 & 221.839  \\
	 \hline
		sliCQT, ragged & 2 & 1.941 & 775.51  \\
	 \hline
		STFT & 2 & 1.827 & 245.906  \\
	 \hline
\end{tabular}
\end{table}

To briefly discuss these results, the zero-padded matrix form of the sliCQ-Wiener-EM was significantly faster than looping over the ragged form, with a negligible drop in SDR. For this reason, the zero-padded sliCQ-Wiener-EM is the chosen implementation of sliCQT-based Wiener-EM in the final model, and the ragged sliCQ-Wiener-EM was discarded for being too computationally slow.

In xumx-sliCQ, a parameter called \Verb#slicq_wiener#\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/v2021/xumx_slicq/model.py\#L380}} was added to switch between the zero-padded sliCQ-Wiener-EM and STFT-Wiener-EM strategies. The default chosen is STFT-Wiener-EM for the best tradeoff of music demixing performance and execution speed.

For both STFT and sliCQT-based Wiener-EM, two iterations were worse than one iteration. The computation time of the inference step will be shown in section \ref{sec:inferenceperf} to discuss the tradeoff between the STFT and sliCQT Wiener-EM.

\subsubsection{Music demixing results}
\label{sec:demixresults}

In this section, the music demixing results of the trained models UMX, X-UMX, and xumx-sliCQ will be compared.

The BSSv4 scores for the demixing results generated from the evaluation script\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/blob/main/mss_evaluation/mss-oracle-experiments/oracle_eval/trained_models.py}} were computed on the test set of the MUSDB18-HQ dataset, and are shown in Figure \ref{fig:bssboxplot}. Table \ref{table:bsseval} contains the details of every model evaluated in the boxplot with their label, and the median SDR across the four targets and 50 test tracks. ``-WEM'' denotes ``Wiener expectation-maximization,'' the post-processing iterative refinement step of the four targets.

To maximize result reproducibility, all pretrained models and code were downloaded from their public hosted locations shown in Table \ref{table:bsseval} and stored in a separate repository to generate the results.\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/tree/main/mss_evaluation}}

\begin{table}[ht]
	\centering
	\caption{Evaluated pretrained models in the BSS boxplot}
	\label{table:bsseval}
	\begin{tabular}{ |p{2.5cm}|l|l|p{3.5cm}|p{3.5cm}| }
	 \hline
		Project & Legend & SDR & Code repository & Pretrained model \\
	 \hline
	 \hline
		CrossNet-Open-Unmix & xumx & 5.54 & \url{https://github.com/sony/ai-research-code/tree/master/x-umx} & \url{https://nnabla.org/pretrained-models/ai-research-code/x-umx/x-umx.h5} \\
	 \hline
		Open-Unmix & umx & 4.64 & \url{https://github.com/sigsep/open-unmix-pytorch} & \url{https://zenodo.org/record/3370489} \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ sliCQT-WEM} & slicq-wslicq & 3.71 & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ STFT-WEM} & slicq-wstft & 3.60 & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
\end{tabular}
\end{table}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-bss/boxplot_full.pdf}
	\caption{Boxplot of UMX, X-UMX, and xumx-sliCQ alongside the oracles}
	\label{fig:bssboxplot}
\end{figure}

The table shows that xumx-sliCQ performed worse than UMX and X-UMX, with both configurations of Wiener-EM.

\newpagefill

\subsubsection{Model size and inference performance comparison}
\label{sec:inferenceperf}

Finally, in this section, we will measure the size on disk and inference running times of the models.

The pretrained UMX-HQ model\footnote{\url{https://zenodo.org/record/3370489}} for the PyTorch deep learning framework has a size on disk of 137 MB for all four targets combined.

The pretrained X-UMX model\footnote{\url{https://nnabla.org/pretrained-models/ai-research-code/x-umx/x-umx.h5}} for the NNabla deep learning framework has a size on disk of 136 MB for the four combined targets, which is almost the same as the size of the UMX PyTorch weights. However, the pretrained X-UMX model was omitted from the running time comparison since NNabla cannot be fairly compared to PyTorch.

The pretrained xumx-sliCQ model\footnote{\url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model}} for the PyTorch deep learning framework has a size on disk of 28 MB, making xumx-sliCQ the smallest network from the comparison.

To report the inference performance, only the CPU was used. The CPU is a more universal device for performing inference, as almost every computing device (desktop, laptop, server, smartphone, etc.) has a CPU while not all have deep-learning capable GPUs \parencite{deepcpuinf, deepcpuinf2}.

The time to perform the separation of a mixed song into four stems (including the Wiener-EM step) was averaged across the first 10 songs of the 50-song test set of MUSDB18-HQ. Table \ref{table:infperf} shows the measured times. For xumx-sliCQ, both the STFT and sliCQT configurations of the post-processing Wiener-EM step were measured, denoted as STFT-WEM and sliCQT-WEM respectively.

\begin{table}[ht]
	\centering
	\caption{Execution times for the inference of the models}
	\label{table:infperf}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Model & Device & Time (s) \\
	 \hline
	 \hline
		UMX & CPU & 27.26  \\
	 \hline
		xumx-sliCQ, STFT-WEM & CPU & 55.59  \\
	 \hline
		xumx-sliCQ, sliCQT-WEM & CPU & 104.54  \\
	 \hline
\end{tabular}
\end{table}

The STFT-WEM configuration of xumx-sliCQ runs \textasciitilde2x slower than UMX, and the sliCQT-WEM configuration runs \textasciitilde4x slower than UMX. We note that the sliCQT-WEM configuration only adds \textasciitilde0.1 dB to the median SDR score compared to the STFT-WEM variant, which is a poor tradeoff considering it doubles the running time.

\end{document}
