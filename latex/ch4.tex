\documentclass[report.tex]{subfiles}
\begin{document}

\section{Experiment and discussion}
\label{sec:experiment}

This section contains the results of the experiments described in Section \ref{sec:methodology}, and a discussion of the results. Refer to Appendix \ref{appendix:computerspec} for the hardware and software specifications of the computer which produced the results in this section.

The goal of this thesis was to use the sliced Constant-Q Transform (sliCQT) in CrossNet-Open-Unmix (X-UMX), a neural network for music demixing. X-UMX represents musical signals with the Short-Time Fourier Transform (STFT), which is limited by a fixed time-frequency resolution. In contrast, the sliCQT has a varying time-frequency resolution, which may be more suitable for musical applications such as music demixing.

To use the sliCQT in a neural network, I first ported the sliCQT library to use PyTorch, a Python framework for GPU-based deep neural networks. The porting effort was described in Section \ref{sec:torchslicq}. In Section \ref{sec:gpuexperimentpytorch}, I will show benchmark results of the port compared to the original library.

In Section \ref{sec:slicqparamsrch}, I described a random parameter search for the sliCQT which used the mix-phase inversion (MPI) oracle to pick sliCQT parameters. The MPI strategy, used by X-UMX and also by my proposed model xumx-sliCQ, combines the magnitude spectrogram of the estimate with the phase of the mix. Choosing sliCQT parameters that give the best MPI oracle score might result in a higher overall music demixing performance from the neural network, since the MPI oracle represents the upper limit of performance for any model that uses the MPI strategy. In Section \ref{sec:slicqparamresults}, I will show the sliCQT parameters that were chosen by the parameter search script. In Section \ref{sec:fasterbsscupy}, I used the GPU-accelerated CuPy library in the BSS (Blind Source Separation) metrics library to speed up the parameter search, and I will show the benchmark results of the CuPy BSS metrics in Section \ref{sec:cupyportbss}.

After porting the sliCQT to PyTorch and choosing the sliCQT parameters, the next step was to train and evaluate the xumx-sliCQ neural network. First, Section \ref{sec:hyperparams} will list all of the training hyperparameters. Next, Section \ref{sec:networktraining} will describe the training times and losses of the network architectures described in Section \ref{sec:slicqarches}. Section \ref{sec:demixresults} will show the music demixing results of the fully trained xumx-sliCQ model, using both post-processing configurations described in Section \ref{sec:postprocessing}. Finally, Section \ref{sec:inferenceperf} will describe the inference performance and model size on disk.

\subsection{PyTorch port of the sliCQT}
\label{sec:gpuexperimentpytorch}

The first step was to port the sliCQT to PyTorch, a GPU-based neural network framework for Python, so that it could be used in a neural network. In Section \ref{sec:torchslicq}, I described how I ported the original CPU-based NumPy implementation of the reference sliCQT Python library to PyTorch. In this section, I will show benchmark results of the PyTorch port.

The benchmark script for the PyTorch implementation of the sliCQT performs the forward and backward sliCQT using the Bark scale with 50 bins between 20--22,050 Hz. The sliCQT is taken on a single 3:54 minute song from the MUSDB18-HQ dataset, ``Zeno - Signs.'' The sliCQT parameters were chosen so that the transform of the song occupied a maximum of 7.2 GB of memory and could fit on the device with the least memory, the NVIDIA 2070 Super GPU. The computation time was repeated for 100 iterations and averaged. The cost of the memory transfer of the song to the GPU was not included in the measurement. Table \ref{table:nsgttorchresultsragged} shows the benchmark results.

\begin{table}[ht]
	\centering
	\caption{Execution times for the forward + backward ragged sliCQT}
	\label{table:nsgttorchresultsragged}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU, multithreaded & 51.95 \\
	 \hline
		Original & CPU, single-threaded & 8.72 \\
	 \hline
		PyTorch & GPU (NVIDIA 2070 Super) & 2.52 \\
	 \hline
		PyTorch & GPU (NVIDIA 3080 Ti) & 2.38 \\
	 \hline
		PyTorch & CPU & 1.85 \\
	 \hline
\end{tabular}
\end{table}

To discuss these results, I will first note that the execution time of the transform improved with PyTorch in every case. The transform computed with PyTorch on the CPU was \textasciitilde4.7x faster than the single-threaded CPU implementation of the original library. The transforms computed with PyTorch on the GPUs were \textasciitilde3.5x and \textasciitilde3.7x faster than the single-threaded CPU implementation of the original library respectively. The original library has a multithreaded option which performs worse than the default single-threaded behavior, which was an unexpected result.

Even though the sliCQT computed with PyTorch on the GPU was not faster than the CPU for the tested parameters, having the transform on the GPU allows the use of the sliCQT inside any PyTorch-based neural network, which was the original goal of the port.

\subsection{Best sliCQT parameters with MPI oracle}
\label{sec:slicqparamresults}

In Section \ref{sec:slicqparamsrch}, I described the random search to choose the parameters of the sliCQT to use in xumx-sliCQ. Since the goal of xumx-sliCQ is to try to surpass X-UMX, which uses the STFT, the first step is to try to the sliCQT that has the most potential to surpass the STFT in music demixing. This was done by choosing the best Signal-to-Distortion (SDR) score on the mixed-phase inversion (MPI) oracle waveforms of validation tracks of MUSDB18-HQ. The MPI strategy is when the estimate of the magnitude spectrogram of the target is combined with the phase of the original mixed audio, described in Section \ref{sec:noisyphaseoracle}. The MPI strategy is used in both X-UMX and xumx-sliCQ.

The parameter search script ran for 60 iterations using random combinations of sliCQT parameters chosen from the ranges in Table \ref{table:slicqparams} in Section \ref{sec:slicqparamsrch}. The best-performing sliCQT parameters chosen by the script used the following parameters: Bark scale, 262 bins, 32.9--22,050 Hz. These parameters were used in the final neural network, xumx-sliCQ. The slice length and transition length were chosen to be 18,060 and 4,514 samples respectively, using the automatic slice length picker described in Section \ref{sec:improvelib}. Figure \ref{fig:bipolarslicqs} shows a visual comparison of the magnitude spectrograms of the best-performing sliCQT parameters alongside the STFT with the X-UMX default window size of 4,096 samples.

\begin{figure}[ht]
	\centering
	\subfloat[Best sliCQT from MPI oracle parameter search]{\includegraphics[width=0.675\textwidth]{./images-gspi/slicqt_good.png}}\\
	\subfloat[STFT, X-UMX default]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_stft_4096.png}}
	\caption{Magnitude spectrogram comparison; sliCQT vs. STFT}
	\label{fig:bipolarslicqs}
\end{figure}

The BSS metrics of the MPI oracle evaluation of these sliCQT parameters is shown in the boxplot in Figure \ref{fig:oraclebssboxplot}, compared to the results from the STFT with different window sizes. The hypothesis of this thesis is that the varying time-frequency resolution of the sliCQT may provide an advantage in a musical application over the fixed time-frequency resolution of the STFT, and that the window size of the STFT must be changed to trade time and frequency resolution. In some music source separation papers, a short window STFT is used for a high time resolution, and a long window STFT is used for a high frequency resolution \parencite{fitzgerald2, driedger}. In particular, \textcite{driedger} use a window size of 256 for the short window STFT and 4096 for the long window STFT, and \textcite{fitzgerald2} use a window of 1024 for the short window STFT and 16384 for the long window STFT. Therefore, to demonstrate the effect of time-frequency resolution on music demixing, I compared the best sliCQT chosen from the parameter search with different STFT window sizes, selecting powers of two between 256 and 16384 in the boxplot. Table \ref{table:winsizes} shows the evaluated transforms and their parameters. Note that the range of window sizes between 256--16384 contain 4096, which is the STFT window size of X-UMX.

\begin{table}[ht]
	\centering
	\caption{Time-frequency transforms compared in the MPI oracle boxplot}
	\label{table:winsizes}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Transform name & Transform & Parameters \\
	 \hline
	 \hline
		slicqt-bark-262-32 & sliCQT & Bark scale, 262 bins, 32.9--22050 Hz \\
	 \hline
		stft-256 & STFT & 256 sample window size \\
	 \hline
		stft-512 & STFT & 512 sample window size \\
	 \hline
		stft-1024 & STFT & 1024 sample window size \\
	 \hline
		stft-2048 & STFT & 2048 sample window size \\
	 \hline
		stft-4096 & STFT & 4096 sample window size \\
	 \hline
		stft-8192 & STFT & 8192 sample window size \\
	 \hline
		stft-16384 & STFT & 16384 sample window size \\
	 \hline
\end{tabular}
\end{table}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-bss/oracle_boxplot.pdf}
	\caption{Boxplot for MPI oracle mask evaluations}
	\label{fig:oraclebssboxplot}
\end{figure}

The median SDR achieved by the chosen sliCQT parameters in the MPI oracle was 7.42 dB, surpassing the 6.23 dB achieved by the STFT with the X-UMX default window and overlap of 4,096 and 1,024 samples respectively.

To discuss these results, the MPI oracle result indicates that a music demixing system that uses the sliCQT with 262 frequency bins between 32.9--22050 Hz on the Bark scale has the potential to surpass a system that uses the STFT with any window size between 256--16,384 samples. To translate this potential to a real advantage in the final model, it is still a challenge to find a neural network architecture that can create a good estimate of the magnitude sliCQT spectrograms of the targets.

I stated as a hypothesis in this thesis that the additional musical information in the sliCQT might give the neural network an advantage when learning how to perform the task of music demixing, compared to the STFT. Note the sharper clarity of musical events in both time and frequency in the sliCQT spectrogram in Figure \ref{fig:bipolarslicqs}(a) compared to the STFT spectrogram in Figure \ref{fig:bipolarslicqs}(b).

\subsection{CuPy acceleration of BSS metrics}
\label{sec:cupyportbss}

In Section \ref{sec:fasterbsscupy}, the slowest functions of the BSS metrics library that used CPU-based NumPy and SciPy operations were swapped with their equivalent functions in CuPy, taking advantage of GPU acceleration. This was done to speed up the development cycle and execution time of the parameter search script. As a bonus side effect, running the full BSS evaluations in Section \ref{sec:demixresults} were also sped up as a result of the CuPy-accelerated BSS metrics.

The benchmark script for the CuPy BSS metrics library creates a random estimated waveform for the four targets of the 14 validation tracks of MUSDB18-HQ, and computes the BSS metrics. The computation time was repeated for 10 iterations and averaged. Three copies of the benchmark script were run; one with the argument \Verb#--disable-cupy# to use the CPU code with SciPy and NumPy functions on the CPU, one with the argument \Verb#--cuda-device=0# to use the NVIDIA 3080 Ti GPU, and one with the argument \Verb#--cuda-device=1# to use the NVIDIA RTX 2070 Super GPU. The results are shown in Table \ref{table:cupybssresults}.

\begin{table}[ht]
	\centering
	\caption{Execution times for the BSS metrics evaluation}
	\label{table:cupybssresults}
	\begin{tabular}{ |l|l|l| }
	 \hline
		BSS library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU & 1,485.77 \\
	 \hline
		CuPy & GPU (NVIDIA 2070 Super) & 738.39 \\
	 \hline
		CuPy & GPU (NVIDIA 3080 Ti) & 585.33 \\
	 \hline
\end{tabular}
\end{table}

To discuss the results, there is a \textasciitilde2-2.5x speedup when calculating BSS metrics with the GPU compared to the CPU, which represent a significant reduction in the time taken to evaluate music demixing systems. This speedup, aside from speeding up the parameter search and music demixing evaluation in this thesis, can be useful for larger-scale evaluation campaigns like SiSEC \parencite{sisec2018} or MDX \parencite{mdx21}

\newpagefill

\subsection{xumx-sliCQ neural network}
\label{sec:nnresults}

In this section, I will show the hyperparameters, training procedure, and final trained performance of the xumx-sliCQ neural network, whose design was described in Section \ref{sec:neuralnet}.

\subsubsection{Hyperparameters in the training script}
\label{sec:hyperparams}

This section will cover the details of the hyperparameters of the xumx-sliCQ training script. Table \ref{table:xumxslicqparams} contains a full list of all the hyperparameters, their default values, and the justification for the choice of default value if it was not copied from X-UMX. Parameters that were introduced or modified as necessary to support the use of the sliCQT in xumx-sliCQ will point to the section of the thesis that explains the parameter and its chosen default value. Note that the choice of the Bidirectional Long Short-Term Memory (Bi-LSTM) and Convolutional Denoising Autoencoder (CDAE) network architectures of xumx-sliCQ, shown in Section \ref{sec:neuralnet}, has been made a parameter in the training script.

\begin{table}[ht]
	\centering
	\caption{Hyperparameters in the xumx-sliCQ training script}
	\label{table:xumxslicqparams}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Parameter & Default & Origin \\
	 \hline
	 \hline
		Dataset & MUSDB18-HQ & Copied from X-UMX \\
	 \hline
		Learning rate & 1e-3 & Copied from X-UMX \\
	 \hline
		Learning rate decay patience & 80 & Copied from X-UMX \\
	 \hline
		Learning rate decay gamma & 0.3 & Copied from X-UMX \\
	 \hline
		Weight decay & 1e-5 & Copied from X-UMX \\
	 \hline
	 	Seed & 42 & Copied from X-UMX \\
	 \hline
	 	Bandwidth & 16000 Hz & Copied from X-UMX \\
	 \hline
	 	Number of channels & 2 & Copied from X-UMX \\
	 \hline
	 	Number of workers for dataloader & 4 & Copied from X-UMX \\
	 \hline
		Epochs & 1,000 & Copied from X-UMX \\
	 \hline
		Patience & 1,000 & Copied from X-UMX \\
	 \hline
		Frequency scale & Bark & Sections \ref{sec:slicqparamsrch}, \ref{sec:slicqparamresults} \\
	 \hline
		Frequency bins & 262 & Sections \ref{sec:slicqparamsrch}, \ref{sec:slicqparamresults} \\
	 \hline
	 	Minimum frequency & 32.9 Hz & Sections \ref{sec:slicqparamsrch}, \ref{sec:slicqparamresults} \\
	 \hline
		Sequence duration & 1 s & Section \ref{sec:replacestft} \\
	 \hline
		Use Bi-LSTM instead of CDAE & False & Section \ref{sec:slicqarches} \\
	 \hline
		Mixing coefficient & 0.1 & Section \ref{sec:xumxinc} \\
	 \hline
\end{tabular}
\end{table}

The optimizer used is the Adam optimizer with an adaptive learning rate scheduler, copied directly and unchanged from X-UMX.

\subsubsection{Network architecture and training results}
\label{sec:networktraining}

In this section, I will describe the neural network architectures and training results for both the Bi-LSTM and CDAE architectures of xumx-sliCQ. I will use the torchinfo\footnote{\url{https://github.com/TylerYep/torchinfo}} package to get a count of the total trainable parameters of the neural network, and show the loss curves of the training process with Tensorboard,\footnote{\url{https://www.tensorflow.org/tensorboard/}} a neural network training visualization tool.

For the Bi-LSTM configuration of xumx-sliCQ, there are 1,889,512 trainable parameters in total. The training time per epoch within the first 10 epochs was 53 minutes, which would result in a hypothetical total of 36 days to train the full 1000 epochs. I did not fully train this configuration, since a training time of 36 days is not feasible for this thesis.

For the CDAE configuration of xumx-sliCQ, there are 6,669,912 trainable parameters in total. The training time per epoch took 350 seconds, or almost 6 minutes, representing a total of 100 hours or 4 days of training. The total training curves for the full 1000 epochs can be seen in Figure \ref{fig:networkloss}. The lowest validation loss achieved was -0.449 at epoch 583.

The music demixing performance of the trained model of xumx-sliCQ with the CDAE architecture will be shown in Section \ref{sec:demixresults}, and the execution time of inference and the size of the model on disk will be shown in Section \ref{sec:inferenceperf}.

\begin{figure}[ht]
	\centering
	\subfloat[Train loss]{\includegraphics[width=\textwidth]{./images-neural/train_loss.png}}\\
	\subfloat[Validation loss]{\includegraphics[width=\textwidth]{./images-neural/valid_loss.png}}
	\caption{Tensorboard loss curves for xumx-sliCQ using the CDAE architecture}
	\label{fig:networkloss}
\end{figure}

\subsubsection{Music demixing results}
\label{sec:demixresults}

In this section, I will compare the music demixing results of the trained models of UMX, X-UMX, and xumx-sliCQ.

In Section \ref{sec:xumx}, I described how X-UMX (CrossNet-Open-Unmix) is an evolution of UMX (Open-Unmix), and that for convenience throughout Section \ref{sec:methodology} and \ref{sec:experiment}, I would use X-UMX to refer to the concepts of both UMX and X-UMX. In the results shown in this and the next section, I will treat UMX and X-UMX separately. They have a different music demixing performance and inference performance, due to the differences in their network architectures, shown in Figure \ref{fig:umxandxumx}, and the different deep learning frameworks used in their reference implementations.

The evaluated trained model of xumx-sliCQ uses the CDAE architecture. The Bi-LSTM architecture of xumx-sliCQ was not evaluated, due to requiring 36 days to train the full 1,000 epochs. The CDAE architecture was trained fully in four days, which is an acceptable training time for this thesis.

The BSS metrics for the demixing results of UMX, X-UMX, and xumx-sliCQ were computed on the 50-song test set of the MUSDB18-HQ dataset, and the boxplot is shown in Figure \ref{fig:bssboxplot}. Table \ref{table:bsseval} contains the details of the evaluated models, including the median SDR across all tracks and targets. The results of xumx-sliCQ are computed for both of the Wiener Expectation Maximization (Wiener-EM) post-processing strategies, using the sliCQT or STFT. Wiener-EM will be abbreviated to WEM in the tables that follow.

To maximize result reproducibility, all pretrained models and code were downloaded from their public hosted locations shown in Table \ref{table:bsseval} and stored in a separate repository to generate the results.\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/tree/main/mss_evaluation}}

\begin{table}[ht]
	\centering
	\caption{Evaluated pretrained models in the BSS boxplot}
	\label{table:bsseval}
	\begin{tabular}{ |p{2.5cm}|l|l|p{4cm}|p{4cm}| }
	 \hline
		Project & Legend & SDR & Code repository & Pretrained model \\
	 \hline
	 \hline
		CrossNet-Open-Unmix & xumx & 5.54 & \url{https://github.com/sony/ai-research-code/tree/master/x-umx} & \url{https://nnabla.org/pretrained-models/ai-research-code/x-umx/x-umx.h5} \\
	 \hline
		Open-Unmix & umx & 4.64 & \url{https://github.com/sigsep/open-unmix-pytorch} & \url{https://zenodo.org/record/3370489} \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ sliCQT-WEM} & slicq-wslicq & 3.71 & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ STFT-WEM} & slicq-wstft & 3.6 & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
\end{tabular}
\end{table}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-bss/boxplot_full.pdf}
	\caption{Boxplot of UMX, X-UMX, and xumx-sliCQ alongside the oracles}
	\label{fig:bssboxplot}
\end{figure}

To summarize and discuss the results, xumx-sliCQ scored a median SDR of 3.6 dB across the 4 targets and 50 songs of the MUSDB18-HQ test set with the STFT Wiener-EM step, and 3.71 dB with the sliCQT Wiener-EM step. UMX and X-UMX scored 4.64 and 5.54 dB respectively. xumx-sliCQ failed in its objective to improve the music demixing performance of UMX and X-UMX with both of its post-processing strategies.

\newpagefill

\subsubsection{Model size and inference performance comparison}
\label{sec:inferenceperf}

In this section, I will show measurements of inference time and the size on disk of the compared models.

The pretrained UMX-HQ model\footnote{\url{https://zenodo.org/record/3370489}} for the PyTorch deep learning framework has a size on disk of 137 MB for all four targets combined. The ``-HQ'' suffix denotes that this the pretrained model of UMX that was trained on MUSDB18-HQ, as opposed to the original MUSDB18. MUSDB18-HQ is same the dataset used for X-UMX and xumx-sliCQ. The pretrained X-UMX model\footnote{\url{https://nnabla.org/pretrained-models/ai-research-code/x-umx/x-umx.h5}} for the NNabla deep learning framework has a size on disk of 136 MB for the four combined targets, which is almost the same as the size of the UMX PyTorch weights. The pretrained xumx-sliCQ model\footnote{\url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model}} for the PyTorch deep learning framework has a size on disk of 28 MB, making xumx-sliCQ the smallest network from the comparison.

To report the inference performance, only the CPU was used. The CPU is a more universal device for performing inference, as almost every computing device (desktop, laptop, server, smartphone, etc.) has a CPU while not all have deep-learning capable GPUs \parencite{deepcpuinf, deepcpuinf2}. The time to perform the separation of a mixed song into four stems (including the Wiener-EM step) was averaged across the first 10 songs of the 50-song test set of MUSDB18-HQ. Table \ref{table:infperf} shows the measured times. For xumx-sliCQ, both the STFT and sliCQT configurations of the post-processing Wiener-EM step were measured, denoted as STFT-WEM and sliCQT-WEM respectively.

\begin{table}[ht]
	\centering
	\caption{Execution times of CPU inference and model sizes}
	\label{table:infperf}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Model & Size on disk & Time (s) \\
	 \hline
	 \hline
		UMX & 137 MB & 27.26 \\
	 \hline
		xumx-sliCQ, STFT-WEM & 28 MB & 55.59 \\
	 \hline
		xumx-sliCQ, sliCQT-WEM & 28 MB & 104.54 \\
	 \hline
		X-UMX & 136 MB & 528.19 \\
	 \hline
\end{tabular}
\end{table}

To discuss these results, I will first note that X-UMX shows an anomalously high execution time which is \textasciitilde20x slower than UMX. This is because it uses a different deep learning framework, NNabla, compared to the PyTorch-based UMX and xumx-sliCQ. The STFT Wiener-EM configuration of xumx-sliCQ is \textasciitilde2x slower than UMX, and the sliCQT Wiener-EM configuration is \textasciitilde4x slower than UMX. Wiener-EM with the sliCQT only adds \textasciitilde0.1 dB to the median SDR score compared to the Wiener-EM with the STFT, while doubling the running time. The pretrained model of xumx-sliCQ is \textasciitilde4.9x smaller on disk than both of the original STFT-based models, using 28 MB instead of the 137 and 136 MB of UMX and X-UMX respectively.

\end{document}
