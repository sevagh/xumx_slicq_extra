\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{ch:background}

In this thesis, my starting point is Open-Unmix \parencite{umx}, a deep neural network for music demixing written in the Python programming language. Open-Unmix uses the Short-Time Fourier Transform (STFT) as its representation of musical signals, which is subject to the limitations of the time-frequency uncertainty principle, or time-frequency tradeoff. I will be exploring the sliced Constant-Q Transform (sliCQT) as a replacement of the STFT, and my hypothesis is that using the sliCQT instead of the STFT will improve the music demixing performance of Open-Unmix due to the varying time-frequency resolution of the sliCQT that is more suitable for musical or auditory analysis.

In this chapter, I will introduce the theoretical background necessary to understand my goal and hypothesis.

In Section \ref{sec:timedomain}, I will give an overview of acoustic signals, including the time-domain waveform and discrete-time signals in digital systems.

In Section \ref{sec:freqdomain}, I will introduce the frequency domain, which is alternative representation of acoustic signals. I will start by describing frequency analysis, the Fourier transform, and joint time-frequency analysis, leading to the ubiquitous Short-Time Fourier Transform (STFT). Continuing from the STFT, I will describe nonuniform time-frequency transforms that were designed for music analysis, including the Constant-Q Transform (CQT), the Nonstationary Gabor Transform (NSGT), and the sliced Constant-Q Transform (sliCQT).

In Section \ref{sec:ml}, I will give an overview of machine learning and deep learning, and how these methods are typically applied to audio or musical applications, commonly in the form of convolutional neural networks (CNN) or recurrent neural networks (RNN).

In Section \ref{sec:softcode}, I will give an overview of important concepts in software and code relevant to this thesis, including Python and Git version control.

In Section \ref{sec:musicsep}, I will describe the tasks of music source separation and music demixing. I will provide a definition of the tasks, their motivations and uses, a survey of historical approaches to computational audio source separation, and trends in STFT-based music source separation leading to the current state of the art.

\subsection{Acoustic signals and the time-domain waveform}
\label{sec:timedomain}

\citeauthor{discretebook} define a signal as a function of one or more variables which ``conveys information about the state or behavior of a physical system'' \parencite[8]{discretebook}. Some examples of signals are a 2-dimensional image, which is a brightness function of two spatial variables, and a speech signal, which is a function of time. \citeauthor{moore} describes the more specific case of a sound or audio signal as being the description of the vibration of an object and how it impresses this vibration upon the ``surrounding medium (usually air) as a pattern of changes in pressure'' \parencite[2]{moore}.

According to \textcite{melbook}, the waveform is a natural and mathematically convenient representation of a signal. For acoustic signals, the waveform represents the continuously varying pattern of the signal as a function of the continuous variable $t$, which represents time. A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers \parencite{discretebook}. The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$. A continuous-time signal and its discrete representation are shown in Figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform.]{\includegraphics[width=0.8438\textwidth]{./images-tftheory/continuoustime.png}}\\
	\subfloat[Discrete-time waveform.]{\includegraphics[width=0.8438\textwidth]{./images-tftheory/discretetime.png}}
	\caption{A continuous-time signal and its discrete-time representation sampled with $T = 125\mu s$ \parencite[10]{discretebook}.}
	\label{fig:discretecontinuous}
\end{figure}

Continuous-time signals are converted into discrete-time representations by two processes: sampling and quantization. Points on the continuous time axis of the waveform are \textit{sampled} periodically, and the amplitude values of the waveform at these sampled points are \textit{quantized} to find the closest digital number \parencite{melbook}. Time sampling is controlled by the sampling period $T \text{ seconds}$, or the sampling rate $F_{s} = \sfrac{1}{T} \text{ Hz}$, which define the periodicity of the sampling. Amplitude quantization is controlled by the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x_{a}(t)$ is defined by $x[n] = x_{a}(nT)$, where $n$ is an integer and $T = \sfrac{1}{F_{s}}$ is the sampling period. The Nyquist-Shannon sampling theorem \parencite{discretebook}, described independently by \textcite{nyquist1928} and \textcite{shannon1948}, states that the maximum frequency of a signal that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \sfrac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency.

Aliasing is one of the pitfalls of choosing an inappropriate sampling rate for the frequencies present in the signal under observation \parencite{dspfirst}. The diagram in Figure \ref{fig:aliasing} shows the phenomenon of aliasing due to undersampling, which occurs when a time-domain signal is undersampled, i.e., $F_{\text{sig}} > F_{\text{nyq}}$, and the continuous-time signal cannot be reconstructed accurately.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4688\textwidth]{./images-tftheory/aliasing_undersampling.png}
	\caption{An undersampled cosine wave (red) and the resulting incorrect reconstruction (black) \parencite[82]{dspfirst}.}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \citeauthor{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values'' \parencite[190]{discretebook}. The second variable that relates to the quantization process is the number of quantization levels. The quantization operation is represented as $ \hat{x}[n] = Q(x[n])$, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation \parencite{discretebook}. An A/D or analog-to-digital converter (ADC) circuit and its operation on a waveform is shown in Figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC).]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer.]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\vspace{0.1em}
	\subfloat[Waveform results of the ADC process.]{\includegraphics[width=0.7969\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations \parencite[188, 190, 192]{discretebook}.}
	\label{fig:adccircuit}
\end{figure}

\subsection{Transforms of acoustic signals}
\label{sec:freqdomain}

\citeauthor{moore} states that:

\begin{quote}
	[a]lthough all sounds can be specified by their variation in pressure with time, it is often more convenient, and more meaningful, to specify them in a different way when the sounds are complex. This method is based on a theorem by Fourier, who proved that almost any complex waveform can be analyzed, or broken down, into a series of sinusoids with specific frequencies, amplitudes, and phases. This is done using a mathematical procedure called the Fourier Transform \parencite[4]{moore}.
\end{quote}

Throughout this section, the description of the Fourier Transform of acoustic signals and the evolution of further transforms that build on it will be covered in detail.

Also, throughout this section and the rest of this chapter, an example waveform will be used for illustrative purposes. This is the glockenspiel waveform,\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} which is used in various audio signal processing papers on the topic of time-frequency \parencite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt, wmdct}, because the glockenspiel contains both tonal and transient properties, which have conflicting needs for time and frequency resolution in their analysis. Figure \ref{fig:glockwaveform} shows the discrete-time waveform of the glockenspiel signal. Throughout this section, demonstrations of each transform will use this same glockenspiel signal as the input $x[n]$. The signal has a total duration of 5.94 seconds, and is sampled with a rate of 44,100 Hz.

\begin{figure}[ht]
	\centering
	\subfloat[Glockenspiel waveform.]{\includegraphics[width=0.656325\textwidth]{./images-gspi/glock_waveform.png}}\\
	\subfloat[Glockenspiel notes.]{\includegraphics[width=0.9375\textwidth]{./images-gspi/sheetmusic.png}}
	\caption{Glockenspiel waveform in (a) playing a C minor pentatonic melody with 15 strikes. The struck notes are $C, Eb, G, Bb, C, Eb, G, Bb$, shown in (b).}
	\label{fig:glockwaveform}
\end{figure}

\subsubsection{Frequency analysis and the Fourier Transform}
\label{sec:freqanal}

The Fourier Transform originated as an integral transform in mathematics, which are a class of ``useful tools for solving problems involving certain types of partial differential equations (PDE), mainly when their solutions on the corresponding domains of definition are difficult to deal with'' \parencite[54]{fourierhistory}. The Fourier Transform was originally introduced by Joseph Fourier in his earlier papers \parencite{fourierhist1, fourierhist2}, and fully expanded and collected in his seminal work on heat \parencite{fourierheat}. The connection of the Fourier Transform to music is described by \citeauthor{fouriermusic}, who state that:

\begin{quote}
	[b]eyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contribution represents some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string \parencite[461]{fouriermusic}.
\end{quote}

The continuous-time Fourier Transform (CTFT) of a time-domain acoustic waveform is defined by equation \eqref{equation:ctft}, and its inverse is defined by equation \eqref{equation:ictft} \parencite[308]{dspfirst}:
\begin{align}
	X(\omega) = \int_{-\infty}^{\infty}{x(t)e^{-j\omega t}\mathit{dt}} \tag{1}\label{equation:ctft} \\
	x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{X(\omega)e^{j\omega t}\mathit{d\omega}} \tag{2}\label{equation:ictft}
\end{align}

\textcite{dspfirst} refer to $X(\omega)$ as the frequency-domain representation of the signal $x(t)$. Equation \eqref{equation:ictft} defines the signal $x(t)$ in terms of a sum of infinitely many complex-exponential signals with $X(\omega)$ controlling the amplitude and phases of these signals. The continuous-time Fourier Transform provides a one-to-one mapping of the time domain to the frequency domain; it is a complex-valued function of $\omega$, which is the variable that represents the angular frequency in radians. The Fourier Transform can be expressed in the rectangular form in equation \eqref{equation:rect} or polar form in equation \eqref{equation:polar} \parencite[49]{discretebook}:
\begin{align}
	X(e^{j\omega}) = X_{\text{real}}(e^{j\omega}) + j X_{\text{imag}}(e^{j\omega}) \tag{3}\label{equation:rect} \\
	X(e^{j\omega}) = |X(e^{j\omega})|e^{j\angle X(e^{j\omega})} \tag{4}\label{equation:polar}
\end{align}

The quantities $|X(e^{j\omega})|$ and $\angle X(e^{j\omega})$ are referred to as the magnitude and phase respectively. The Fourier Transform is also referred to as the spectrum, while its magnitude and phase are the magnitude and phase spectra, respectively \parencite{discretebook}.

Signals need to be transformed from the continuous to the discrete domain via sampling to be processed digitally or computationally, which was covered previously in Section \ref{sec:timedomain}. The discrete-time Fourier Transform (DTFT) is is defined by equation \eqref{equation:dtft}, and its inverse is defined by equation \eqref{equation:idtft} \parencite[289]{melbook}:

\begin{align}
	X(\omega) = \sum_{n = -\infty}^{\infty}{x[n]e^{-j\omega n}} \tag{5}\label{equation:dtft} \\
	x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}{X(\omega)e^{j\omega n}\mathit{d\omega}} \tag{6}\label{equation:idtft}
\end{align}

\todo[inline]{derived from a sampled version of the continuous-time Fourier Transform and also explain DFT here describe DTFT -> DFT since i use DFT all over the paper}

The number of points, or samples, of the DFT, determines the frequency resolution, also called $\mathit{df}$ or $\mathit{\Delta f}$, which is the frequency spacing between each point in the resulting spectrum \parencite{discretebook}. The frequency resolution of an $N$-point DFT is $\mathit{df} = \sfrac{F_{s}}{N}$, where $F_{s}$ is the sampling rate of the input signal. An illustration of the magnitude and phase spectra of the DFT are shown in Figure \ref{fig:glockdft}, using two different lengths of DFT to show the difference in the low and high frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[2,048-point DFT, $\mathit{df} = 21.533 Hz$.]{\includegraphics[width=0.8906\textwidth]{./images-gspi/glock_dft_2048.png}}\\
	\subfloat[262,144-point DFT, $\mathit{df} = 0.168 Hz$.]{\includegraphics[width=0.8906\textwidth]{./images-gspi/glock_dft_262144.png}}
	\caption{DFT of the Glockenspiel waveform, using a low frequency resolution with 2,048 points in (a), and a high frequency resolution with 262,144 points in (b). Note the more detailed frequency components shown in the higher frequency resolution transform in (b).}
	\label{fig:glockdft}
\end{figure}

For a real-valued input signal $x[n]$, the spectrum has the conjugate symmetry property, i.e., the magnitudes of the DFT coefficients are mirrored symmetrically around the center frequency, while the phases of the DFT coefficients are mirrored anti-symmetrically around the center frequency \parencite{dspfirst}. This center bin corresponds to the Nyquist frequency $\sfrac{F_{s}}{2}$ where $F_{s}$ is the sampling rate of the signal. For a given DFT of $N$ points, the points of the output spectrum between 0--$\sfrac{N}{2}+1$ correspond to 0--$\sfrac{F_{s}}{2}$ Hz, and the points $\sfrac{N}{2}+1$--N correspond to the same frequency bins in reverse order, i.e., $\sfrac{F_{2}}{2}$--0 Hz. In practice, therefore, only the first $\sfrac{N}{2}+1$ points of the $N$-point DFT of a real signal are useful.

According to \textcite{skiena}, algorithms in computer science are often described by their time complexity using a hypothetical computer where simple operations take one time step. The ``Big-O'' notation, or $O(N)$, provides the upper bound of algorithm running time in relation to number of input elements. In \textcite[Chapter~9]{discretebook} it is described that in its original formulation, the algorithmic complexity of the DFT is $O(N^{2})$, or in other words, the running time of the algorithm grows proportionally the size of the input signal squared \parencite{skiena}.  Starting from legendary mathematician Carl Friedrich Gauss in 1805 \parencite{gausshist}, and reaching its most famous formulation published by \textcite{cooleytukey}, a family of efficient algorithms for the computation of the DFT by computing a series of smaller DFTs, known collectively as the Fast Fourier Transform (FFT), reduced this computation time to $O(N \log{N})$. This resulted in the FFT becoming one of the most important algorithms of the 20th century \parencite{ffttopten}.

\newpagefill

\subsubsection{Joint time-frequency analysis with the Gabor Transform and Short-time Fourier Transform (STFT)}
\label{sec:jointtfa}

Continuing from the discussion of the DFT in the previous Section \ref{sec:freqanal}, \citeauthor{discretebook} state that ``often, in practical applications of sinusoidal signal models, the signal properties (amplitudes, frequencies, and phases) will change with time. For example, nonstationary signal models of this type are required to describe radar, sonar, speech, and data communication signals. A single DFT estimate is not sufficient to describe such signals...'' \parencite[714]{discretebook}.

Dennis Gabor's seminal signal processing paper, \textit{The Theory of Communication}, introduced significant and far-reaching concepts in the time and frequency analysis of acoustic signals \parencite{gabor1946}. \citeauthor{gabor1946} quotes famed American telecommunication engineer John Carson \parencite{carsonfamous} to describe the limitations of the Fourier Transform:

\begin{quote}
	[t]he foregoing solutions [of the Fourier Transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren \parencite[431]{gabor1946}.
\end{quote}

According to \citeauthor{korpel}, ``Gabor came to the conclusion that the difficulty lay in our mutually exclusive formulations of time analysis and frequency analysis ... he suggested a new method of analyzing signals in which time and frequency play symmetrical parts'' \parencite[3624]{korpel}.

Gabor derived the principal of time-frequency uncertainty from the Heisenberg uncertainty principle in quantum physics, which states that the more precisely the position of an electron is determined, the less precisely the momentum is known, and conversely \secondaryciteindirect{heisenberg1927}{hallm}. \citeauthor{gabor1946} states that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit'' \parencite[432]{gabor1946}. This is referred to as the time-frequency uncertainty principle or the Gabor limit. Gabor defines the unit of time-frequency information $\Delta t \Delta f$ as the \textit{logon}, where $\Delta t$ and $\Delta f$ are defined as ``the uncertainties inherent in the definition of the epoch t and frequency f of an oscillation'' \parencite[432]{gabor1946}.

In order to demonstrate the mutually exclusive formulations of time and frequency, which leads to Gabor's time-frequency uncertainty principle, Figure \ref{fig:gaborfirst}(a) shows the unit impulse contrasted with the DC-component DFT. The DFT spectrum of a ``sinusoid of zero frequency'' \parencite[13]{dspfirst} has only one nonzero value in the frequency domain at the 0 Hz frequency component (direct current, or DC), but has an infinite extent in the time domain. The unit impulse is the ``simplest [time-domain] sequence because it has only one nonzero value, which occurs at n = 0'' \parencite[107]{dspfirst}, but has an infinite extent in the frequency domain.

Another way of visualizing the tradeoff of time and frequency is shown in Figure \ref{fig:gaborfirst}(b), where the frequency, or periodicity, of the sine wave is more apparent over longer periods of time $\Delta t$, but the detail of the individual time-domain sample values become lost.

\begin{figure}[ht]
	\centering
	\subfloat[Mutually exclusive formulations of time and frequency by two extremes, the unit impulse (bottom left) and the 0 Hz cosine DFT spectrum (top right).]{\includegraphics[width=0.5625\textwidth]{./images-tftheory/gabor13.png}}\\
	\subfloat[The periodicity of the sine wave is more apparent with a longer $\Delta t$, at the expense of temporally localized samples.]{\includegraphics[width=0.5625\textwidth]{./images-tftheory/gabor2.png}}
	\caption{Time-frequency tradeoff intuitions \parencite[103, 106]{gabor2}.}
	\label{fig:gaborfirst}
\end{figure}

The result of the time-frequency uncertainty principle is a consequence of how the Fourier Transform is used to swap between the mutually exclusive domains of time and frequency. Several psychacoustic studies have shown that humans can exhibit better time-frequency resolution than the Gabor limit, indicating that there are processes involved in the perception of sounds that cannot be explained by the Fourier Transform alone. \citeauthor{psycho2} describes one of these experiments:

\begin{quote}
	It is concluded that models based on a place (spectral) analysis should be subject to a limitation of the type $\Delta f \cdot d \ge \text{constant}$, where $\Delta f$ is the frequency difference limen for a tone pulse of duration d. [...] It was found that at short durations the product of $\Delta f$ and d was about one order of magnitude smaller than the minimum predicted [...] \parencite[610]{psycho2}.
\end{quote}

More recently, according to \citeauthor{psycho1}:

\begin{quote}
	[w]e have conducted the first direct psychoacoustical test of the Fourier uncertainty principle in human hearing, by measuring simultaneous temporal and frequency discrimination. Our data indicate that human subjects often beat the bound prescribed by the uncertainty theorem, by factors in excess of 10 \parencite[4]{psycho1}.
\end{quote}

\citeauthor{psycho1} goes on to state that ``most sound analysis and processing tools today continue to use models based on spectral theories... [w]e believe it is time to revisit this issue'' \parencite[4]{psycho1}.

When performing joint time-frequency analysis, it is preferable to minimize time-frequency uncertainty, or to set the \textit{logon} ($\Delta t \Delta f$) to its lowest possible value. \citeauthor{gabor1946} asks:

\begin{quote}
	What is the shape of the signal for which the product $\Delta t \Delta f$ actually assumes the smallest possible value? [... it is] the modulation product of a harmonic oscillation of any frequency with a pulse of the form of the probability function \parencite[435]{gabor1946}.
\end{quote}

Gabor performed joint time-frequency analysis by multiplying overlapping, temporally consecutive portions of the input signal with shifted copies of the Gaussian window function (i.e., the probability function), and by taking the Fourier Transform of the windowed segments of the signal. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described by equations \eqref{equation:gabort}:
\begin{flalign}\tag{7}\label{equation:gabort}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)]\\
	\nonumber G_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha f n}
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform \parencite{dictionary}.

The STFT, or Short-Time Fourier Transform, has been described independently from Gabor's work \parencite{stftindie}, but additional research in the 1980s \parencite{dictionary} led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described by equations \eqref{equation:stft}:
\begin{flalign}\tag{8}\label{equation:stft}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)]\\
	\nonumber X_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-am)e^{-j2\pi f n}
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $am$, and $a$ is the hop size between successive time-shifts of the window \parencite{dictionary}. Note how similar equations \eqref{equation:gabort} and \eqref{equation:stft} are, which is expected since the original Gabor transform is the STFT with a Gaussian window. In practice, the STFT allows the use of different windows and overlap sizes \parencite{stftinvertible}, as long as the constant overlap-add (COLA) constraint is respected \parencite{cola}. Figure \ref{fig:stftdiagram} shows how a windowed Fourier Transform (i.e., Gabor transform or STFT) is performed on a waveform.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\includegraphics[width=0.7500\textwidth]{./images-tftheory/stft_diagram.png}
		\caption[Forward and inverse STFT or Gabor transform.]{Forward and inverse STFT or Gabor transform.\footnote[1]{\url{https://www.mathworks.com/help/signal/ref/iscola.html}}}
		\label{fig:stftdiagram}
	\end{minipage}
\end{figure}

Figure \ref{fig:gabortf} shows different sizes of \textit{logon} in the time-frequency plane, and how the Gabor transform and the STFT with higher frequency or higher time resolution appear on the time-frequency plane.

\begin{figure}[ht]
	\centering
	\subfloat[Pure time domain, pure frequency domain, and Gabor's time-frequency tiles of $\Delta t \Delta f = 1$.]{\includegraphics[height=3.4cm]{./images-tftheory/gabor3.png}}\\
	\subfloat[High frequency resolution vs. high time resolution.]{\includegraphics[height=3.2cm]{./images-tftheory/gabor4.png}}
	\caption{Different tiling of the time-frequency plane \parencite[326, 327]{gabordiagrams}.}
	\label{fig:gabortf}
\end{figure}

The number of output frequency bins of the STFT is the same as the DFT, since the output of the STFT is a matrix of columns where each column contains the DFT coefficients from one window of the input signal. The rows represent the frequency bins and the columns represent the time windows or frames. Section \ref{sec:freqanal} described that the output coefficients of an N-point DFT for a real-valued input signal $x[n]$ were conjugate-symmetrically mirrored around the center bin, such that bins 0 to $(\sfrac{N}{2}+1)$ corresponded to the frequency range of 0 to $(\sfrac{F_{s}}{2})$ Hz, and bins $(\sfrac{N}{2}+2)$ to 0 were redundant. The STFT has the same behavior, such that an STFT with a window size of N, which is the same as the N points of the DFT for each window of the signal, outputs $(\sfrac{N}{2}+1)$ non-redundant frequency bins. For an $N$-point DFT, the frequency spacing between each bin $\mathit{df} = \sfrac{F_{s}}{N}$, which is that of the DFT shown in Section \ref{sec:freqanal}. The frequency in Hz corresponding to each bin is $f \text{ (Hz)} = \text{bin}\times\sfrac{F_{s}}{N}$. The frequency bins 0 to $(\sfrac{N}{2}+1)$ therefore correspond to the frequency range of 0 to $(\sfrac{F_{s}}{2})$ Hz. Figure \ref{fig:stftdiag} shows the rectangular matrix of coefficients which is the output of the STFT, and the frequency bins and frequencies in Hz corresponding to the rows of the matrix.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4688\textwidth]{./images-blockdiagrams/stft_diag.png}
	\caption{Rectangular matrix output of the STFT.}
	\label{fig:stftdiag}
\end{figure}

A key characteristic of the STFT is its fixed time-frequency resolution across the entire frequency spectrum. To change the time-frequency resolution, the window size must be changed. Figure \ref{fig:stfts1} shows the STFT with the Hamming window and Figure \ref{fig:stfts2} shows the Gabor transform, which is the STFT with the Gaussian window, intending to show that the STFT and Gabor transform are practically identical to each other. Three different window sizes are shown for each: 128 samples, 2,048 samples, and 16,384 samples, which at the sample rate of the glockenspiel signal (44,100 Hz) represent $\sfrac{128}{44,100} =$ 2.9 ms, 46.44 ms, and 371.52 ms respectively. The different window sizes show a visual demonstration of the time-frequency tradeoff. The Hamming window was chosen because it is the default window in the MATLAB \Verb#spectrogram# function.\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} The Gaussian and Hamming windows are shown together in Figure \ref{fig:gaussvshamm}.

The time-frequency tradeoff of the STFT is a result of taking the Fourier transform of fixed-size windows applied to the input signal. In upcoming sections, I will show time-frequency transforms that use variable-size windows, resulting in a time-frequency resolution that varies across the frequency spectrum. These transforms are also called nonuniform, to distinguish from the uniform or fixed time-frequency resolution of the STFT. Section \ref{sec:cqt} introduces the Constant-Q Transform (CQT), a time-frequency transform designed for music analysis. Section \ref{sec:theorynsgt} and Section \ref{sec:theoryslicqt} show the Nonstationary Gabor Transform (NSGT) and sliced Constant-Q Transform (sliCQT), which are time-frequency transforms with varying time-frequency resolution motivated by the CQT. Section \ref{sec:raggedtf} will show how nonuniform time-frequency transforms differ from the uniform STFT.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\includegraphics[width=0.7500\textwidth]{./images-tftheory/hamming_vs_gauss.png}
		\caption[2,048-sample Hamming and Gaussian windows. The Gaussian window is truncated, and uses a width factor of $2.5$.]{2,048-sample Hamming and Gaussian windows. The Gaussian window is truncated and uses a width factor of $2.5$.\footnote[1]{\url{https://www.mathworks.com/help/signal/ref/gausswin.html}}}
		\label{fig:gaussvshamm}
	\end{minipage}
\end{figure}

\newpagefill

\begin{figure}[ht]
	\centering
	\subfloat[STFT, 128-sample Hamming.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_hamm_128.png}}\\
	\subfloat[STFT, 2,048-sample Hamming.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_hamm_2048.png}}\\
	\subfloat[STFT, 16,384-sample Hamming.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_hamm_16384.png}}
	\caption{Magnitude spectrograms with the Hamming window STFT, shown in order of the smallest to largest window size. The horizontal lines (i.e., frequency components) are becoming sharper from the increasing frequency resolution, while the vertical lines (i.e., temporal events or note onsets) are becoming blurrier from the decreasing time resolution.}
	\label{fig:stfts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[Gabor transform, 128-sample Gaussian.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_gauss_128.png}}\\
	\subfloat[Gabor transform, 2,048-sample Gaussian.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_gauss_2048.png}}\\
	\subfloat[Gabor transform, 16,384-sample Gaussian.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_gauss_16384.png}}
	\caption{Magnitude spectrograms with the Gabor transform, i.e., the STFT with a Gaussian window. Note that they look identical to the STFT with a Hamming window in Figure \ref{fig:stfts1}. The different window functions are compared in Figure \ref{fig:gaussvshamm}.}
	\label{fig:stfts2}
\end{figure}

\newpagefill

\subsubsection{Constant-Q Transform (CQT)}
\label{sec:cqt}

\citeauthor{cqtransient} stated that the problem with the STFT is its ``rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies'' \parencite[1]{cqtransient}. As described in Section \ref{sec:motivation}, music should be analyzed with a high frequency resolution in the low-frequency region, and with a high time resolution in the high-frequency region \parencite{doerflerphd, cqtransient}. 

The Constant-Q Transform was proposed by \textcite{jbrown, msp} to analyze musical signals with a logarithmic frequency scale that matched the notes of a musical pitch scale. A demonstration of a violin analyzed with the DFT and CQT was shown in Figure \ref{fig:violin}.

The name ``Constant-Q'' refers to the constant ratio of the frequency being analyzed to the frequency resolution of analysis, or $\sfrac{f}{\delta f} = Q$, such that the frequency resolution increases with the center frequency to maintain the $Q$ factor. The transform uses long-duration windows in the low frequency regions and short-duration windows in the high frequency regions, resulting in good time resolution for transients \parencite{cqtransient}. Figure \ref{fig:jbrowncqt} shows some properties of the linearly-spaced DFT spectrum compared to the Constant-Q Transform, and the different window sizes used for each frequency bin of interest. Equations \eqref{equation:jbrowncqt} describe how the different windows $W[n, k]$ are applied to the signal:
\begin{align}\tag{9}\label{equation:jbrowncqt}
	\nonumber & \text{window length} = N[k] = \frac{f_{s}}{f_{k}}Q\\
	\nonumber & W[k, n] = \rho + (1 - \rho)\cos\big(\frac{2\pi n}{N[k]}\big)
\end{align}

The parameter $\rho$ in the window function $W[k, n]$ in equation \eqref{equation:jbrowncqt} should be set to 0.5 for a Hann window or $\sfrac{25}{46}$ for a Hamming window.

\begin{figure}[ht]
	\centering
	\subfloat[Properties of DFT, CQT.]{\includegraphics[width=0.6094\textwidth]{./images-tftheory/dftvcqt.png}}\\
	\subfloat[Window sizes for computing the CQT.]{\includegraphics[width=0.6094\textwidth]{./images-tftheory/qwindowchanges.png}}
	\caption{Various aspects of the original CQT \parencite[427, 428]{jbrown}.}
	\label{fig:jbrowncqt}
\end{figure}

This first CQT implementation was not designed to be invertible, until \textcite{klapuricqt} introduced an algorithm for an approximate inverse of the CQT back to the time-domain waveform, with an inversion error of $\approx 10^{-3}$. The approximate inverse for the CQT was also approached differently by \textcite{fitzgeraldcqt}.

\citeauthor{klapuricqt} describe the CQT as ``a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal'' \parencite[1]{klapuricqt}. Frequency bins that are geometrically spaced are also logarithmically spaced with a base of two \parencite{geometriclog}. The bins-per-octave setting of the CQT is related to the Constant-Q ratio by the formula $Q = 2^{\sfrac{1}{\text{bins}}}$. For example, for 12 bins-per-octave, this results in a $Q$ factor of 1.059, which is the well-known ``12th root of two'' based on the Western chromatic scale with equal temperament \parencite{westernpitch1, westernpitch2}.

\textcite{invertiblecqt} implemented the Constant-Q Transform with the Nonstationary Gabor Transform (NSGT) \parencite{balazs}, which allowed for perfect reconstruction for the first time. This is also called the CQ-NSGT, or Constant-Q Nonstationary Gabor Transform.

\textcite{slicq} followed up with a realtime algorithm, the \textit{sliCQ} or ``sliced Constant-Q'' transform (sliCQT), which can process the input signal in fixed-size slices, as opposed to operating on the entire input signal like the NSGT. Finally, \textcite{variableq1} introduced the Variable-Q scale and improved the phase of the transform. From \textcite{invertiblecqt}, the total frequency bins of the CQ-NSGT or sliCQT can be derived from the bins-per-octave, and the minimum and maximum frequencies of the desired frequency scale, shown in equation \eqref{equation:bpo}:
\begin{align}
	K = [B \log_{2}\big(\frac{\xi_{\text{max}}}{\xi_{\text{min}}}\big) + 1]\tag{10}\label{equation:bpo}
\end{align}

where $K$ is the total bins of the CQT, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. For example, for $B = 12 \text{ bins-per-octave}$, $\xi_{\text{min}} = 83 \text{ Hz}$ and $\xi_{\text{max}} = 22{,}050 \text{ Hz}$, the result is $K \approx 97 \text{ total frequency bins}$.

In the landscape of music analysis libraries, Essentia,\footnote{\url{https://essentia.upf.edu/}} LTFAT,\footnote{\url{https://ltfat.org/}} and the MATLAB Wavelet Toolbox\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} have converged on the CQ-NSGT \parencite{invertiblecqt, slicq, variableq1}. However, librosa\footnote{\url{https://librosa.org/}} still uses the older implementation with the approximate reconstruction \parencite{klapuricqt}. Finally, \textcite{invertiblecqt} have released an open-source reference Python implementation\footnote{\url{https://github.com/grrrr/nsgt}} of the CQ-NSGT and the sliCQT.

Several examples of magnitude spectrograms of the glockenspiel signal are generated using the STFT, shown in Figure \ref{fig:cqtvsstft1}, and the CQT, shown in Figure \ref{fig:cqtvsstft2}. The spectrograms generated from the standard STFT spectrogram\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} and the CQ-NSGT implementation of the CQT from the MATLAB Wavelet Toolbox. Note how the CQT spectrograms show more spectral information than the STFT spectrograms at all frequency resolutions.

As mentioned throughout this section, the best implementation choice for the CQT is the NSGT, which allows for a perfect inverse transform. Section \ref{sec:theorynsgt} will introduce and describe the general NSGT, and Section \ref{sec:theoryslicqt} will introduce and describe the sliCQT, which is a realtime variant of the NSGT. The NSGT and sliCQT allow the analysis of audio with an arbitrary nonuniform frequency scale not limited to the Constant-Q scale, which will give me more freedom in this thesis to find a frequency scale that performs best for music source separation applications.

\begin{figure}[ht]
	\centering
	\subfloat[STFT spectrogram, window size 256.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_stft_256.png}}\\
	\subfloat[STFT spectrogram, window size 1,024.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_stft_1024.png}}\\
	\subfloat[STFT spectrogram, window size 4,096.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_stft_4096.png}}
	\caption{STFT magnitude spectrograms of the glockenspiel signal. Three different window sizes are shown in increasing order, demonstrating an increasing frequency resolution (sharper horizontal lines) and decreasing time resolution (blurrier vertical lines).}
	\label{fig:cqtvsstft1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins-per-octave.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_cqt12.png}}\\
	\vspace{-0.35em}
	\subfloat[CQT spectrogram, 24 bins-per-octave.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_cqt24.png}}\\
	\vspace{-0.35em}
	\subfloat[CQT spectrogram, 48 bins-per-octave.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/glock_cqt48.png}}
	\vspace{-0.35em}
	\caption{CQT magnitude spectrograms of the glockenspiel signal. Three different bins-per-octave are shown in increasing order. The time and frequency resolution is varied within a single spectrogram, unlike the fixed time-frequency resolution of each spectrogram in Figure \ref{fig:cqtvsstft1}. Using more bins-per-octave increases the highest frequency resolution and decreases the lowest time resolution.}
	\label{fig:cqtvsstft2}
\end{figure}

\newpagefill

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

\textcite{doerflerphd} in their dissertation analyzed music with multiple Gabor dictionaries (i.e., STFTs). Figure \ref{fig:dorflertradeoff} shows the time-frequency tradeoff of the short and long window STFT analyses of the glockenspiel signal for its transient and tonal characteristics.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8438\textwidth]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal \parencite[20]{doerflerphd}.}
	\label{fig:dorflertradeoff}
\end{figure}

As summarized by \citeauthor{adaptivecqt}, ``[t]he definition of multiple Gabor frames, which is comprehensively treated in [D{\"o}rfler 2002], provides Gabor frames with analysis techniques with multiple resolutions... The nonstationary Gabor frames [...] are a further development; [...] they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas'' \parencite[2]{adaptivecqt}. In other words, a single Nonstationary Gabor transform can replace the need for multiple distinct Gabor transforms (or STFTs).

\citeauthor{dictionary} describe the shift from the term \textit{transform}, (e.g., STFT), to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began a ``fundamental move from transforms to dictionaries for [...] signal representation'' \parencite[1049]{dictionary}. Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task'' \parencite[1049]{dictionary}.

Using multiple transforms, such as using two Gabor transforms (or STFTs) with a small and large window for the transient and tonal properties of music, is also called an \textit{overcomplete} dictionary \parencite{dictionary}, where there is a lot of redundancy in the transform domain.

The advantage of the CQT over such approaches is that it contains these desirable transform properties in one single transform. The NSGT is a time-frequency transform with varying time-frequency resolution, whose motivating application is the CQT \parencite{jaillet, balazs}. It is constructed from frame theory \parencite{frametheory}, a mathematical technique for computing redundant, stable ways of representing a signal \parencite{framesintro}. The NSGT can use nonuniform time and frequency spacing in its time-frequency analysis of a signal, and it has a perfect inverse operation with practically no reconstruction error.

The construction of nonstationary Gabor frames relies on three properties of the windows and time-frequency shift parameters used \parencite[2]{balazs}:
\begin{tight_itemize}
	\item
		``The signal $f$ of interest is localized at time- (or frequency-) positions $n$ by means of multiplication with a compactly supported (or limited bandwidth, respectively) window function $g_{n}$''
	\item
		``The Fourier Transform is applied on the localized pieces $f \cdot g_{n}$. The resulting spectra are sampled densely enough in order to perfectly re-construct $f \cdot g_{n}$ from these samples''
	\item
		``Adjacent windows overlap to avoid loss of information. At the same time, unnecessary overlap is undesirable. We assume that $0 < A \le \sum_{n \in \mathbb{Z}}|g_{n}(t)|^{2} \le B < \infty$, a.e. (almost everywhere), for some positive A and B''
\end{tight_itemize}

These requirements lead to invertibility of the frame operator and therefore to perfect reconstruction. \citeauthor{balazs} continues on to say that

\begin{quote}
	[m]oreover, the frame operator is diagonal and its inversion is straightforward. Further, the canonical dual frame has the same structure as the original one. Because of these pleasant consequences following from the three above-mentioned requirements, the frames satisfying all of them will be called painless nonstationary Gabor frames and we refer to this situation as the painless case \parencite[1482]{balazs}
\end{quote}

The derivation of the NSGT starts from the definition of the Gabor transform from Section \ref{sec:jointtfa}. In the standard Gabor transform, the same window function (also called the Gabor atom or Gabor function) is shifted in time to cover entire signal \parencite{adaptivecqt}, described by equation \eqref{equation:stationarygab}:
\begin{align} \tag{11}\label{equation:stationarygab}
g_{m, n}(t) = g(t - na)e^{2\pi j m b t}
\end{align}

As stated by \citeauthor{adaptivecqt}, ``[w]e will indicate such a frame as \textit{stationary}, since the window used for time-frequency shifts does not change and the time-frequency shifts form a lattice of $a \times b$'' \parencite[3]{adaptivecqt}. Figure \ref{fig:uniformtflattice} shows the resulting uniform $a \times b$ tiling of the time-frequency plane.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4688\textwidth]{./images-tftheory/stationarygabor.png}
	\caption{Uniform time-frequency resolution of the stationary Gabor transform \parencite[3]{adaptivecqt}.}
	\label{fig:uniformtflattice}
\end{figure}

The definition of the multiple Gabor systems of \textcite{doerflerphd} continues from the stationary Gabor transform. Given the stationary Gabor atom, where $a$ and $b$ are the time-frequency shift parameters and $f(t)$ is the reconstructed input signal, the Gabor transform is described by equations \eqref{equation:onegabor}:
\begin{align}
	\nonumber g_{m,n}(t) &= g(t - na)e^{j2\pi m b t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{m,n \in \mathbb{Z}}c_{m,n}g_{m,n}(t) \tag{12}\label{equation:onegabor}
\end{align}

An overcomplete system that uses $R$ distinct STFTs (or Gabor transforms) with different window sizes, where $a_{r}$ and $b_{r}$ are the time-frequency shift parameters for each window size, can be described with equations \eqref{equation:multigabor}:
\begin{align}
	\nonumber g_{m,n}^{r}(t) &= g(t - na_{r})e^{j2\pi m b_{r} t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{r=0}^{R-1}\sum_{m,n \in \mathbb{Z}}c^{r}_{m,n}g^{r}_{m,n}(t) \tag{13}\label{equation:multigabor}
\end{align}

For a resolution that changes with time, the Nonstationary Gabor atom is chosen from a set of functions $\{g_{n}\}$ and a fixed frequency sampling step $b_{n}$, shown in equation \eqref{equation:irregulartime}:
\begin{align}\tag{14}\label{equation:irregulartime}
	g_{m,n}(t) &= g_{n}(t)e^{j2\pi m b_{n}t}, m,n \in \mathbb{Z}
\end{align}

\citeauthor{balazs} describe the system in equation \eqref{equation:irregulartime} further:

\begin{quote}
	[...] the functions $\{g_{n}\}$ are well-localized and centered around time-points $a_{n}$. This is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{n}$ for each position $a_{n}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position \parencite[1485]{balazs}.
\end{quote}

For a resolution that changes with frequency, the Nonstationary Gabor atom is chosen from a family of functions $\{h_{m}\}$, which are ``well-localized band-pass functions with center frequency $b_{n}$'' and a fixed time sampling step $a_{m}$, shown in equation \eqref{equation:irregularfrequency} \parencite[1486]{balazs}:
\begin{align}\tag{15}\label{equation:irregularfrequency}
	h_{m,n}(t) = h_{m}(t - na_{m}), m,n \in \mathbb{Z}
\end{align}

Figure \ref{fig:nonuniformtflattices} show both the cases of the varying resolution by time and by frequency in terms of sampled points on the time-frequency grid.

\begin{figure}[ht]
	\centering
	\subfloat[Gabor atoms that change with time.]{\includegraphics[width=6.25cm]{./images-tftheory/irregulartime.png}}
	\hspace{1em}
	\subfloat[Gabor atoms that change with frequency.]{\includegraphics[width=6.25cm]{./images-tftheory/irregularfrequency.png}}
	\caption{Varying time-frequency sampling of the Nonstationary Gabor transform \parencite[1485, 1487]{balazs}.}
	\label{fig:nonuniformtflattices}
\end{figure}

The NSGT coefficients for a signal of length $L$ are given by an FFT of length $M_{n}$ for each window $g_{n}$. For each window, there are $L$ window operations and $O(M_{n} \cdot \log(M_{n}))$ FFT operations. The overall algorithmic complexity of the NSGT is therefore $O(N \cdot (M \log(M)))$ for $N$ windows, where $M = \text{max}(M_{n})$.

I believe that the general NSGT is more interesting to study than the CQ-NSGT (Constant-Q NSGT), as it can be constructed with nonuniform frequency scales besides the Constant-Q scale, such as the psychoacoustic mel and Bark scales \parencite{melbook}, or the Variable-Q scale, which combines the Constant-Q scale with the psychoacoustic concept of the equivalent rectangular bandwidth (ERB) \parencite{variableq1, variableq2}. In essence the NSGT can be thought of as a filterbank \parencite{variableq1}.

Throughout the rest of this thesis, the NSGT and sliCQT implementation used will be the open-source, reference Python library.\footnote{\url{https://github.com/grrrr/nsgt}} The reference library contains two example scales: the Constant-Q/logarithmic scale, and the mel psychoacoustic scale. Figure \ref{fig:bunchansgts1} demonstrates the NSGT with the Constant-Q scale, and Figure \ref{fig:bunchansgts2} demonstrates the NSGT with the mel scale. For both figures, a low and high frequency resolution transform using 100 and 500 total frequency bins, respectively, is generated, using a frequency range of 20--22,050 Hz. Details of the frequency scales used to generate the NSGT spectrograms are shown in Table \ref{table:nsgtfreqsandqs}.

In addition to the Constant-Q and mel scales included in the reference library, I will describe additional frequency scales that may be interesting for music or audio analysis.

The Variable-Q scale \parencite{variableq1, variableq2} is the same as the Constant-Q scale, except with a small fixed frequency offset, denoted by gamma or $\gamma \text{ (Hz)}$, added to each frequency bin. \citeauthor{variableq1} provide the motivation for the Variable-Q scale:
\begin{quote}
	... [The] CQT has several advantages over STFT when analysing music signals. However, one considerable practical drawback is the fact that the analysis/synthesis atoms get very long towards lower frequencies. This is unreasonable both from a perceptual viewpoint and from a musical viewpoint. Auditory filters in the human auditory system are approximately Constant-Q only for frequencies above 500 Hz and smoothly approach a constant bandwidth towards lower frequencies. Accordingly, music signals generally do not contain closely spaced pitches at low frequencies, thus the Q-factors (relative frequency resolution) can safely be reduced towards lower frequencies, which in turn improves the time resolution \parencite[5]{variableq1}.
\end{quote}

Both \textcite{variableq1} and \textcite{variableq2} cite the ERBlet transform \parencite{erblet} as a psychoacoustic transform, which in turn motivates the Variable-Q Transform. \textcite{variableq1, variableq2} provide the following equations \eqref{equation:variablebw} for computing the bandwidth $B_{k}$ of the frequency bin (or filter channel) $k$ where $b$ is the bins-per-octave or bpo:
\begin{align}\tag{16}\label{equation:variablebw}
	\nonumber & B_{k} = \sigma f_{k} + \gamma, \sigma = 2^{\frac{1}{b}} - 2^{\frac{1}{b}}
\end{align}

The effect is to widen the bandwidths of the windows at the low frequency bins significantly, since the offset is comparable in its order of magnitude to the lower frequency bandwidths. \textcite{variableq1} show some examples with $\gamma = [0, 3, 6.6, 10, 30] \text{ Hz}$. As the center frequency increases, the effect of the small offset becomes negligible. This results in a widening of the Q-factors in the low frequency bins, but it becomes Constant-Q in the high frequency bins. 

Finally, the Bark psychoacoustic scale is introduced to complement the included mel scale, because the Bark scale has been used with success in music source separation \parencite{barkjust1} and in percussion instrument classification \parencite{barkjust2}. In this thesis, I use the formula shown in equation \eqref{equation:barktan} to convert between Bark and Hz frequencies \parencite{barktan}:
\begin{align}\tag{17}\label{equation:barktan}
	\nonumber & f_{\text{Bark}} = 6 \cdot arcsinh \Big(\frac{f_{\text{Hz}}}{600}\Big), f_{\text{Hz}} = 600 \cdot sinh \Big(\frac{f_{\text{Bark}}}{6}\Big)
\end{align}

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, Constant-Q scale, 100 bins.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_cqlog_100.png}}\\
	\subfloat[NSGT, Constant-Q scale, 500 bins.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_cqlog_500.png}}
	\caption{Constant-Q NSGT spectrograms of the glockenspiel signal.}
	\label{fig:bunchansgts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, mel scale, 100 bins.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_mel_100.png}}\\
	\subfloat[NSGT, mel scale, 500 bins.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_mel_500.png}}
	\caption{mel-scale NSGT spectrograms of the glockenspiel signal.}
	\label{fig:bunchansgts2}
\end{figure}

\begin{table}[ht]
	\centering
	\caption{Different frequencies and Q-factors for various NSGT scales.}
	\label{table:nsgtfreqsandqs}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & First five frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 20.00, 21.47, 23.04, 24.73, 26.54 \\
	 \hline
	 mel & 100 & 20.00, 45.56, 72.02, 99.42, 127.80 \\
	 \hline
	 Constant-Q & 500 & 20.00, 20.28, 20.57, 20.86, 21.16 \\
	 \hline
	 mel & 500 & 20.00, 25.00, 30.03, 35.10, 40.21 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & Last five frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 16,614.38, 17,832.63, 19,140.20, 20,543.64, 22,050.00 \\
	 \hline
	 mel & 100 & 19,087.44, 19,789.79, 20,517.07, 21,270.17, 22,050.00 \\
	 \hline
	 Constant-Q & 500 & 20,845.91, 21,140.62, 21,439.50, 21,742.61, 22,050.00 \\
	 \hline
	 mel & 500 & 21,428.92, 21,582.58, 21,737.31, 21,893.11, 22,050.00 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & First five Q-factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06 \\
	 \hline
	 mel & 100 & 0.40, 0.88, 1.34, 1.78, 2.21 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 mel & 500 & 2.01, 2.49, 2.97, 3.45, 3.92 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & Last five Q-factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06 \\
	 \hline
	 mel & 100 & 13.83, 13.85, 13.86, 13.88, 13.89 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 mel & 500 & 69.97, 69.98, 70.00, 70.02, 70.03 \\
	 \hline
\end{tabular}
\end{table}

\newpagefill

\subsubsection{sliCQ Transform (sliCQT)}
\label{sec:theoryslicqt}

The NSGT processes the entire input signal at once. In cases where the input signal must be processed in fixed-size chunks, such as realtime streaming, the sliCQ Transform (sliCQT, or sliced Constant-Q Transform) was created \parencite{invertiblecqt, slicq}. The slicing operation is shown in Figure \ref{fig:slicqtukeys}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5156\textwidth]{./images-misc/slicq_windows.png}
	\caption{Slicing the input signal with 50\% overlapping Tukey windows. N is the slice length and M is the transition area \parencite{slicq}.}
	\label{fig:slicqtukeys}
\end{figure}

Additionally, the ``slicing windows are symmetrically zero-padded to length $2N$, reducing time-aliasing significantly'' \parencite[10]{slicq}, with the effect that adjacent slices need to be 50\% overlap-added with each other to create the final spectrogram. There is no inverse operation for the 50\% overlap-add provided in the paper. Figure \ref{fig:slicqoverlaps} demonstrates this characteristic of the sliCQT.

Note the terminology of the sliCQT, which uses slice length instead of the window length of the STFT, and transition length or transition area instead of the overlap or hop length of the STFT. Slice and transition are used to distinguish the larger slice-wise operation of the sliCQT from the local windowing and overlapping done \textit{within} each slice to achieve the desired time-frequency resolution. The real overlap between slices can vary up to a maximum of the transition area.

\begin{figure}[ht]
	\centering
	\subfloat[Adjacent slices placed side-by-side without overlap-adding.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_overlap_flatten.png}}\\
	\subfloat[Adjacent slices with 50\%-overlap-add.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_overlap_proper.png}}
	\caption{sliCQT spectrograms demonstrating the necessary slice 50\% overlap-add due to the symmetric zero-padding of each slice.}
	\label{fig:slicqoverlaps}
\end{figure}

\subsubsection{Ragged time-frequency transforms}
\label{sec:raggedtf}

\citeauthor{klapuricqt} state that the consequence of nonuniform frequency analysis, such as that seen in the CQT or NSGT, leads to:
\begin{quote}
	... a data structure that is more difficult to work with than the time-frequency matrix (spectrogram) obtained by using Short-Time Fourier transform in successive time frames. The last problem is due to the fact that in CQT, the time resolution varies for different frequency bins, in effect meaning that the ``sampling'' of different frequency bins is not synchronized \parencite[1]{klapuricqt}.
\end{quote}

This same statement also applies to the NSGT, where different frequency bins have a different time resolution. An illustration is shown in Figure \ref{fig:raggedslicqt}. The shape of the transform can be referred to as an irregular or ragged matrix.\footnote{\url{https://xlinux.nist.gov/dads/HTML/raggedmatrix.html}}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8438\textwidth]{./images-misc/slicq_shape.png}
	\caption{Illustration of the ragged NSGT, where frequency bins are grouped by their time resolution. The green-colored matrix represents the frequency bins analyzed with the highest frequency resolution and lowest time resolution, resulting in frequencies spaced close together and the lowest number of temporal frames. Moving upwards, the yellow, pink, and blue matrices have a decreasing frequency resolution and increasing time resolution, resulting in an increasing spacing between frequencies and an increasing number of temporal frames.}
	\label{fig:raggedslicqt}
\end{figure}

Note that to produce a spectrogram like the NSGT in Figure \ref{fig:raggedslicqt}, the slices returned by the sliCQT must be overlap-added as described in Section \ref{sec:theoryslicqt} and shown in Figure \ref{fig:slicqoverlaps}. The NSGT outputs the spectrogram directly, without any further modifications needed.

\newpagefill

\subsection{Machine learning and deep learning for music signals}
\label{sec:ml}

Machine learning (ML) is a technique for modeling complex, unknown systems from data \parencite[105]{introtoml}:

\begin{quote}
	In many scientific disciplines, the primary objective is to model the relationship between a set of observable quantities (inputs) and another set of variables that are related to these (outputs). [...] Machine learning provides techniques that can automatically build a computational model of these complex relationships by processing the available data and maximizing a problem dependent performance criterion.
\end{quote}

According to \citeauthor{introtodl}, deep learning (DL) ``is the subfield of machine learning that is devoted to building algorithms that explain and learn a high and low level of abstractions of data that traditional machine learning algorithms often cannot'' \parencite[1]{introtodl}. A characteristic of deep networks is that they have many hidden layers, named so ``because we do not necessarily see what the inputs and outputs of these neurons are explicitly beyond knowing they are the output of the preceding layer,'' \parencite[2]{introtodl} to model more complex relationships between the input and output. Figure \ref{fig:fcdn} shows a deep network with hidden layers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6094\textwidth]{./images-neural/dnn.png}
	\caption{Deep neural network with hidden layers \parencite[2]{introtodl}.}
	\label{fig:fcdn}
\end{figure}

\citeauthor{introtodl} states that ``most machine learning algorithms as they exist now focus on function optimization, and the solutions yielded do not always explain the underlying trends within the data nor give the inferential power that artificial intelligence was trying to get close to'' \parencite[1]{introtodl}. However, machine learning methods have achieved success in many fields including natural language processing \parencite{nlpml}, computer vision \parencite{cvml}, and audio \parencite{audiodeeplearning}, indicating that data-driven approaches are useful, despite falling short of general artificial intelligence \parencite{generalai}.

Optimization techniques have been used in statistical, approximate, or stochastic signal processing \parencite{stochasticsp, statisticalsp} for decades \parencite{optsp}. Some of the classic examples include basis pursuit and matching pursuit \parencite{dictionary1, dictionary2}. Signal processing techniques and machine and deep learning are compatible with one another and can be used together \parencite{mlsp1, mlsp2}. According to \citeauthor{mldspmix}, signal processing and machine learning should both be considered in the study of acoustic signals, shown in Figure \ref{fig:dspmlmix}:

\begin{quote}
	Whereas physical models are reliant on rules, which are updated by physical evidence (data), machine learning (ML) is purely data-driven. By augmenting ML methods with physical models to obtain hybrid models, a synergy of the strengths of physical intuition and data-driven insights can be obtained \parencite[3591]{mldspmix}.
\end{quote}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6094\textwidth]{./images-neural/dspmlmix.jpg}
	\caption{Synergy of physical models and machine learning methods \parencite[3591]{mldspmix}.}
	\label{fig:dspmlmix}
\end{figure}

In machine learning, the input data is typically split into a training set and a test set \parencite{introtoml}. The model makes predictions with its initial parameters using the training input. It measures how correct its prediction was by using a loss function to compare its predicted output to the training (or ground truth) output. Contemporary machine learning is dependent on the field of optimization \parencite{boyd2004convex, mlopt1, mlopt2} for the loss function and parameter updates \parencite{sgd}. The performance of the network is measured by its ability to generalize on the test set, which was data not seen during training. There is sometimes a third set called the validation set, which is also unseen during training and typically used to perform hyperparameter tuning \parencite{splitvaliddata}. Hyperparameters are the parameters of a machine learning model that are defined by the user \parencite{introtodl}. The user tries to maximize the performance of the model on the validation set by modifying the hyperparameters.

Music signals are a form of acoustic signal, and the domain of music signal processing is inseparable from the larger field of signal processing \parencite{musicsp}. It follows that ML and DL techniques can be extended to music signal processing applications. \textcite{audiodeeplearning} describe contemporary deep learning architectures that are used for audio applications. They state that models from the field of computer vision, originally designed for 2D images where pixels are related spatially, may not exactly fit audio waveforms, where amplitude values are related temporally. However, convolutional neural networks, recurrent neural networks, and sequence-to-sequence models are three popular types of network architecture that have been adapted with varying levels of success to the audio and music domains \parencite{audiodeeplearning}.

A new music source separation model, which is the result of this thesis, will be presented in Chapter \ref{ch:methodology}. It will have two variants, one based on a convolutional neural network (CNN) architecture, and one based on a recurrent neural network (RNN) architecture. To provide the necessary background, I will give an overview of convolutional neural networks (CNN) for audio and music applications in Section \ref{sec:cnn}, I will give an overview of recurrent neural networks (RNN) for audio and music applications in Section \ref{sec:rnn}.

\subsubsection{Convolutional neural networks (CNN)}
\label{sec:cnn}

Convolutional neural networks (CNN) ``are based on convolving their input with learnable kernels'' \parencite[3]{audiodeeplearning}. These are useful for both images and audio because of their ability to exploit spatial or temporal correlation in data \parencite{cnns}. Figure \ref{fig:cnnbasic} shows how a learnable convolution kernel slides over the pixels of the input image.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\subfloat{\includegraphics[width=0.5391\textwidth]{./images-neural/sliding_conv_1.png}}\\
		\subfloat{\includegraphics[width=0.5391\textwidth]{./images-neural/sliding_conv_2.png}}
		\caption[$3 \times 3$ convolution kernel sliding over patches of input pixels.]{$3 \times 3$ convolution kernel sliding over patches of input pixels.\footnote[1]{\url{https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks}}}
		\label{fig:cnnbasic}
	\end{minipage}
\end{figure}

Figure \ref{fig:convtranspose} shows how a convolutional and transpose convolutional or deconvolutional layer are the inverse operation of each other.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7500\textwidth]{./images-neural/convtranspose.png}
	\caption{Deconvolution operation \parencite[11]{convtranspose}.}
	\label{fig:convtranspose}
\end{figure}

The network architecture and feature map for an audio CNN is shown in Figure \ref{fig:audiocnn}, where the extracted feature map is visualized in the spectral domain.

\begin{figure}[ht]
	\centering
	\subfloat[CNN architecture for audio waveforms {\parencite[7]{audiocnn2}}.]{\includegraphics[width=0.8438\textwidth]{./images-neural/audio_cnn.png}}\\
	\subfloat[Audio spectral features learned by each layer {\parencite[4]{audiocnn3}}.]{\includegraphics[width=0.8438\textwidth]{./images-neural/audio_cnn2.png}}
	\caption{Example of an audio CNN architecture and spectral feature maps.}
	\label{fig:audiocnn}
\end{figure}

For audio applications, both 1D convolutions (also referred to as temporal convolutions) applied directly to the audio waveform and 2D convolutions applied to time-frequency transforms (most commonly the STFT) are both used \parencite{tcn, 2dconv}. To describe convolutional layers, \citeauthor{convguide} state that ``images, sound clips and many other similar kinds of data have an intrinsic structure,'' and share the following properties \parencite[6]{convguide}:

\begin{tight_enumerate}
	\item
		They are stored as multi-dimensional arrays (e.g., the STFT or sliCQT)
	\item
		They feature one or more axes for which ordering matters (e.g., time and frequency for a spectrogram)
	\item
		The channel axis is used to access different views of the data (e.g., the left and right channels of a stereo audio track)
\end{tight_enumerate}

The STFT and sliCQT of stereo (2-channel) music produce a 2-channel spectrogram, such that the total dimensions of the 2D time-frequency spectrogram are $\text{time} \times \text{frequency} \times \text{channel}$. The 2D convolution kernels slide across the spectrograms to learn a feature representation, and the number of output channels determines how many feature maps are created. 

The kernel parameters define the size and movement of the 2D convolution kernel in the time and frequency dimensions in each output channel. Besides the kernel size, the kernel has these additional parameters: stride, dilation, and padding \parencite{convguide}. Figure \ref{fig:convdiags} shows the behavior of these different kernel parameters. The stride is the amount by which the kernel moves; a stride larger than one implies \textit{subsampling}, because it dictates how much of the output is retained \parencite{convguide}. The dilation defines ``holes'' or gaps in the kernel to increase the receptive field \parencite{convguide}, which is a computationally cheap way to increase the amount of data points considered in a feature map. The padding defines zeros concatenated to the beginning and end of an axis.

\begin{figure}[ht]
	\centering
	\subfloat[2D convolution kernel.]{\includegraphics[width=0.9375\textwidth]{./images-neural/conv1.png}}\\
	\subfloat[2D convolution kernel with padding > 0.]{\includegraphics[width=0.9375\textwidth]{./images-neural/conv2.png}}\\
	\subfloat[2D convolution kernel with stride > 1.]{\includegraphics[width=0.9375\textwidth]{./images-neural/conv3.png}}\\
	\subfloat[2D convolution kernel with dilation > 1.]{\includegraphics[width=0.9375\textwidth]{./images-neural/conv4.png}}
	\caption{Different behaviors of kernel parameters \parencite[14, 29]{convguide}.}
	\label{fig:convdiags}
\end{figure}

In addition to the convolution operator, another operator commonly used in CNN architectures is the pooling layer. Like the strided convolution shown in Figure \ref{fig:convdiags}(c), the role of the pooling operator is to reduce the dimensionality of the input ``by using some function to summarize subregions, such as taking the average or the maximum value'' \parencite[10]{convguide}. The inverse of a pooling or strided convolution layer, which reduce the dimensionality of the input, is the up-sampling layer, which ``is the principal way to recover the resolution of the downsampled feature map'' \parencite[2]{hao2020indexnet}. According to \textcite{hao2020indexnet}, up-sampling can be implemented with a transpose convolution (or deconvolution) layer shown in Figure \ref{fig:convtranspose}, na\"ive interpolation, or unpooling layers.

\subsubsection{Recurrent neural networks (RNN)}
\label{sec:rnn}

For time series data, for which the audio waveform is an exemplar given the temporal evolution of its amplitude, recurrent neural networks (RNNs) are useful since the input sequence of data is introduced back into the network with a cyclic or recurrent connection, and outputs are predicted based on past (or future, if the network is bi-directional) values of the sequence \parencite{rnns}. RNNs are also used commonly in sequence-to-sequence, or seq2seq, models \parencite{seq2seqs}.

In the audio noise suppression model RNNoise,\footnote{\url{https://jmvalin.ca/demo/rnnoise/}} the usefulness of the RNN and its Long Short-Term Memory (LSTM) \parencite{lstm1} and Gated Recurrent Unit (GRU) \parencite{gru1} variants for audio are described:

\begin{quote}
	Recurrent neural networks (RNN) are very important [...] because they make it possible to model time sequences instead of just considering input and output frames independently. [...] RNNs were heavily limited in their ability because they could not hold information for a long period of time [...] [These] problems were solved by the invention of gated units, such as the Long Short-Term Memory (LSTM), the Gated Recurrent Unit (GRU), and their many variants.
\end{quote}

Figure \ref{fig:rnndiags} shows some example RNN architectures, as well as an illustration of the additional complexity in the gated LSTM and GRU units, which allow them to surpass the simple RNN.

\begin{figure}[ht]
	\centering
	\subfloat[Regular RNN with cyclic connections to past data {\parencite[3]{birnn}}.]{\includegraphics[width=0.5391\textwidth]{./images-neural/simple_rnn.png}}\\
	\subfloat[Bi-directional LSTM with cyclic connections to past and future data {\parencite[3]{birnn}}.]{\includegraphics[width=0.5859\textwidth]{./images-neural/birnn.png}}\\
	\subfloat[Different recurrent units {\parencite[7]{lstmrnngru}}.]{\includegraphics[width=0.9375\textwidth]{./images-neural/gates.png}}
	\caption{RNN diagrams.}
	\label{fig:rnndiags}
\end{figure}

\newpagefill

\subsection{Software and code concepts}
\label{sec:softcode}

In this section, I will give an overview of important concepts in Python and general software and coding. The overview will cover the topics that are relevant to the methodology and experiments of this thesis in Chapter \ref{ch:methodology} and Chapter \ref{ch:experiment} respectively.

In Section \ref{sec:pythonbasics}, I will describe concepts of the Python programming language that are most relevant to this thesis, including data structures, numerical and scientific libraries, library managers for installing academic software, and random number generation.

For all of the software written for this thesis, I used Git to track changes, and made the code open-source and available on GitHub for transparency into my methodology and results. In Section \ref{sec:gitvcs}, I will describe version control and Git, and in Section \ref{sec:ossgithub}, I will describe open-source software and GitHub, a social website for sharing code that use Git.

\subsubsection{Python programming language}
\label{sec:pythonbasics}

Python\footnote{\url{https://www.python.org/}} is a general-purpose programming language. It is an interpreted language,\footnote{\url{https://www.python.org/doc/essays/blurb/}} which means that there is no compilation step required, and the Python code or script written by a user can be executed right away with the Python interpreter. The Python interpreter can also run statements directly for quick prototyping, without needing to write a script:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
Python 3.9.6 (default, Jul 16 2021, 00:00:00)
[GCC 11.1.1 20210531 (Red Hat 11.1.1-3)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print('this is python')
this is python
\end{BVerbatim}
\end{listing}

The Python interpreter is widely distributed on most modern operating systems (OS) like Linux, Windows, and OS-X. Python has support for various high-level and user-friendly data structures. Python has seen widespread adoption in academia, and especially in numerical contexts \parencite{pythonscience}.

One of the core features of the Python language is the \textit{list}.\footnote{\url{https://docs.python.org/3/tutorial/datastructures.html\#more-on-lists}} The list in Python corresponds to the array data structure in computer science \parencite{skiena}. It allows for the construction of an ordered list of objects. For example, a discrete-time speech signal may be described by a list of its amplitude values:

\hfill \Verb#speech_signal = [0.15, 0.28, 0.57, 0.98, -0.59]#

Python has a rich ecosystem of academic software libraries. NumPy\footnote{\url{https://numpy.org/}} and SciPy\footnote{\url{https://scipy.org/}} are used for numerical computation and parallelized matrix operations that run on the CPU (central processing unit). Matplotlib\footnote{\url{https://matplotlib.org/}} is a plotting library. Tensorflow\footnote{\url{https://www.tensorflow.org/}} and PyTorch\footnote{\url{https://pytorch.org/}} are libraries for machine learning and deep learning, which also have numerical computation and parallelized matrix operations that are similar to NumPy and SciPy, except that they can run on the GPU (graphical processing unit). The use of the GPU allows for more efficient, faster, and larger parallelized matrix operations, which are essential for modern machine learning and deep learning techniques.

In NumPy, the \textit{ndarray}\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html}} is the core data structure representing an n-dimensional array of objects, which can be numerical (such as int16 or float32) or general objects (such as strings). The same speech signal represented by the Python list above can be represented by a 1-dimensional ndarray of 64-bit floating-point values:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
>>> import numpy
>>> speech_signal = numpy.asarray([0.15, 0.28, 0.57, 0.98, -0.59])
>>> print(speech_signal.dtype)
float64
>>> print(speech_signal.shape)
(5,)
\end{BVerbatim}
\end{listing}

The underlying values are stored in formats that allow for efficient numerical computations, which is why using NumPy ndarrays for numerical computation is preferred to using regular Python data structures such as lists that are not designed for speed \parencite{ndarrayfast}.

In PyTorch and Tensorflow (and in general machine learning), a similar concept to the ndarray is the \textit{tensor}. The tensor originates from the field of physics \parencite{whatistensor}. The tensor is also an n-dimensional numerical array, and NumPy ndarrays are interchangeable with both PyTorch and Tensorflow tensors.\footnote{\url{https://www.tensorflow.org/guide/tf_numpy}, \url{https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html}}

Tensors can be used for numerical computations using the GPU in both PyTorch and TensorFlow, much like the ndarray with NumPy and SciPy on the CPU. For example, \Verb#torch.cos()# computes the cosine of a tensor. Additionally and most importantly, the inputs and outputs to the machine learning and deep learning constructs supported by these libraries are represented as tensors.

The \Verb#random# module of Python contains a random number generator (RNG).\footnote{\url{https://docs.python.org/3/library/random.html}} The \Verb#seed# function initializes the random number generator,\footnote{\url{https://docs.python.org/3/library/random.html\#bookkeeping-functions}} which is, in fact, pseudorandom \parencite{pseudorng}. Pseudorandom means that given the same starting seed, the exact same sequence of numbers are generated deterministically. The seed is usually a parameter to a script or Python program, such that the user can recreate the same RNG sequences for testing purposes by using the same seed.

To manage third-party libraries and packages for Python, a package manager is suggested. The built-in way of managing a sandboxed Python environment is the virtual environment.\footnote{\url{https://docs.python.org/3/tutorial/venv.html}} This creates an isolated copy of Python where the user can install packages without risking the stability of the operating system (OS). The Python dependency manager tool, \Verb#pip#,\footnote{\url{https://pip.pypa.io/en/stable/cli/pip_install/\#requirements-file-format}} can be used to install a list of third-party packages. Packages are published to the public Python Package Index (PyPi) by the authors.\footnote{\url{https://pypi.org/}} An example of a \Verb#pip# file, conventionally called \Verb#requirements.txt#, is shown in Code Listing \ref{lst:reqtxt}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
mir_eval==0.6
tabulate==0.8.7
numpy==1.19.4
\end{BVerbatim}
	\caption{Example pip requirements.txt file.}
	\label{lst:reqtxt}
\end{listing}

There is another popular Python dependency manager called Conda.\footnote{\url{https://docs.conda.io/en/latest/}} Conda environments are also popular for creating reproducible Python environments for academic software. Conda supports channels for third-party package authors to publish and distribute their packages to users, similar to PyPi.\footnote{\url{https://conda.io/projects/conda/en/latest/user-guide/concepts/channels.html}} An example Conda environment file is shown in Code Listing \ref{lst:condayml}. Conda files can also support the \Verb#pip# syntax, since \Verb#pip# is a native Python tool. The user can combine the best of both worlds by using Conda and \Verb#pip# syntax simultaneously in their Conda environment file.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
name: test-conda-env

channels:
  - default

dependencies:
  - python=3.9
  - cudatoolkit=11
  - pip
  - pip:
    - norbert>=0.2.0
\end{BVerbatim}
	\caption{Example Conda environment.yml file.}
	\label{lst:condayml}
\end{listing}

Pip is installed by default with Python, but Conda needs to be installed separately. For the code in this thesis, both \Verb#pip# and Conda files are used and provided to help the readers replicate the necessary Python environments.

\subsubsection{Version control systems and git}
\label{sec:gitvcs}

Version control systems (VCS) are systems used for tracking and managing changes to code files, and storing the history of changes in a database \parencite{gitbook}. \citeauthor{gitbook} state that the benefits of using a VCS is that it ``allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more. Using a VCS also generally means that if you screw things up or lose files, you can easily recover'' \parencite[1]{gitbook}.

According to \textcite{gitbook}, version control systems are either centralized or distributed. In a distributed version control systems (DVCS), each person's copy of the code contains the full history of changes. This means that people can work on their own private copy of the code, and then sync changes with a central server, rather than depending on the central server for viewing the history like in a centralized version control system (CVCS). This allows people to work independently of the central server, until they are ready to sync changes back with the other collaborators.

Git is a popular tool for DVCS written by Linus Torvalds, who is also known for having created Linux. How Git works is that it ``thinks of its data more like a series of snapshots of a miniature filesystem... every time you commit, or save the state of your project, Git basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot'' \parencite[6]{gitbook}. To work on a code project that uses Git, first the project, or \textit{repository}, must be cloned to your local system. You make local changes and then commit them. Finally, you can push your commits back to the central server to share with the other collaborators, often with a descriptive message describing what you changed. Figure \ref{fig:git} shows how Git tracks a file and how a Git workflow should look.

\begin{figure}[ht]
	\centering
	\subfloat[How git stores snapshots of files.]{\includegraphics[width=0.7500\textwidth]{./images-misc/git.png}}\\
	\subfloat[Recommended git workflow.]{\includegraphics[width=0.5625\textwidth]{./images-misc/git2.png}}
	\caption{How git works \parencite[6, 8]{gitbook}.}
	\label{fig:git}
\end{figure}

All of the code for this thesis were stored in Git repositories to keep track of historical changes. It conveniently allows the author to reference the Git history to describe the evolution of the project and the incremental progress.

\subsubsection{Open-source software and GitHub}
\label{sec:ossgithub}

\citeauthor{floss} state that ``free and open source software (FOSS) is any computer program released under a licence that grants users rights to run the program for any purpose, to study it, to modify it, and to redistribute it in original or modified form'' \parencite[1]{floss}. Free and open-source software is a merging of two distinct ideas: free software\footnote{\url{https://www.fsf.org/}} and open-source software.\footnote{\url{https://opensource.org/}} While these have nuanced differences when it comes to commercial licensing, code visibility, and availability, in simplistic terms FOSS implies code that is free and open for users to read and modify. Releasing code as open-source is becoming more popular in academia, with initiatives like Papers With Code\footnote{\url{https://paperswithcode.com/}} and the Journal of Open Source Software.\footnote{\url{https://joss.theoj.org/}} According to \textcite{floss}, FOSS software is a natural complement to academic publications, given concerns with reproducibility of results.

GitHub\footnote{\url{https://github.com}} is a social website, designed for sharing and browsing software projects that use Git for version control, intended for users to share and collaborate on open-source code. \citeauthor{gitbook} describe GitHub\footnote{\url{https://github.com/}} as ``the single largest host for Git repositories, and [... a] central point of collaboration for millions of developers and projects'' \parencite[131]{gitbook}. GitHub contains tools for working with Git repositories, including social elements for personal profiles or organizations. Open-source projects can be made public on GitHub such that anybody can read and download the source code. For a project hosted on GitHub, there is a file browser built into the website, which allows one to view the code and change history of a project. In this thesis, the GitHub code browser will be linked wherever appropriate. An example of the GitHub code browser showing a file from the PyTorch project's Git repository\footnote{\url{https://github.com/pytorch/pytorch}} is shown in Figure \ref{fig:githubpytorch}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-misc/github.png}
	\caption{GitHub's code browser showing a file from the PyTorch source code.}
	\label{fig:githubpytorch}
\end{figure}

\newpagefill

\subsection{Music source separation and demixing}
\label{sec:musicsep}

In this section, I will provide an overview of music source separation and music demixing. I will start by providing definitions, motivations, and uses of the tasks in Section \ref{sec:musicsepdefsmotivations}. In Section \ref{sec:musicsepbackground}, I will give an overview of historical computational approaches to general audio source separation, leading to current trends in music source separation and demixing. In Section \ref{sec:musicsepdatasets}, I will describe the MUSDB18 and MUSDB18-HQ datasets used for music source separation research, and in Section \ref{sec:evalbss}, I will describe the BSS evaluation metrics used for evaluating music source separation methods.

In Section \ref{sec:masksandoracles}, I will describe the technique of time-frequency masking, which is a popular strategy used for spectrogram-based music source separation, and oracle estimators, which are methods for estimating the upper limit of quality achievable by time-frequency masking. In Section \ref{sec:noisyphaseoracle}, I will describe the noisy phase strategy, which is a strategy used by spectrogram-based music source separation models, where the model only estimates the magnitude STFT of the target and uses the phase STFT of the original mixed audio, or the ``noisy phase.'' In Section \ref{sec:hpss}, I will describe Harmonic/Percussive Source Separation (HPSS), which is a simple algorithm for music source separation based on the STFT spectrogram that exemplifies the time-frequency tradeoff of the STFT. 

A new music source separation model, which is the result of this thesis, will be presented in Chapter \ref{ch:methodology}. In Section \ref{sec:ml}, I described that the model will have two variants, one based on a convolutional neural network (CNN) architecture, and one based on a recurrent neural network (RNN) architecture. The RNN model architecture is based on Open-Unmix and CrossNet-Open-Unmix, two related models for music source separation, which will be respectively shown in Section \ref{sec:umx} and Section \ref{sec:xumx}. The CNN model architecture is based on the Convolutional Denoising Autoencoder (CDAE), which will be shown in Section \ref{sec:cdae}.

\subsubsection{Task definition and motivations}
\label{sec:musicsepdefsmotivations}

Typical music recordings are mono or stereo mixtures, with multiple sound objects (drums, vocals, etc.) sharing the same track \parencite{musicsepintro1}. To manipulate the individual sound objects, the stereo audio mixture needs to be separated into a track for each different sound source, in a process called audio source separation.

According to \citeauthor{mdx21}, audio source separation and music source separation have important uses in the world today:
\begin{quote}
	Audio source separation has been studied extensively for decades as it brings benefits in our daily life, driven by many practical applications, e.g., hearing aids, speech diarization, etc. In particular, music source separation (MSS) attracts professional creators because it allows the remixing or reviving of songs to a level never achieved with conventional approaches such as equalizers. Suppressing vocals in songs can also improve the experience of a karaoke application, where people can enjoy singing together on top of the original song (where the vocals were suppressed), instead of relying on content developed specifically for karaoke applications \parencite[1]{mdx21}.
\end{quote}

A common idea in survey papers is that the domains of speech enhancement and music source separation are both important subproblems of audio source separation \parencite{musicsepintro1, musicmask}.

I provided definitions for music source separation and music demixing in Chapter \ref{ch:intro}, to distinguish them from the more general problem of audio source separation:

\begin{quote}
Music source separation is the task of extracting an estimate of one or more isolated sources or instruments (for example, drums or vocals) from musical audio. The task of music demixing or unmixing considers the case where the musical audio is separated into an estimate of all of its constituent sources that can be summed back to the original mixture.
\end{quote}

Music demixing can be considered as the reverse of the mixing process of \textit{stems} in a recording studio, shown in Figure \ref{fig:mixingdiagrams}. First, I will provide a definition of what a stem is from online materials published by music production companies: a stem is a grouping of individually recorded instrument tracks that have been combined together in a common category.\footnote{\url{https://www.izotope.com/en/learn/stems-and-multitracks-whats-the-difference.html}, \url{https://blog.landr.com/stems-in-music/}} For example, a drum stem could include all of the tracks of a drum kit (e.g., snare, tom, hihat), and a vocal stem could include all of the vocal tracks from the different singers in the song.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\subfloat{\includegraphics[height=4cm]{./images-mss/mixdemix.png}}
		\caption[Music mixing and demixing block diagrams.]{Music mixing and demixing block diagrams.\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}}}
		\label{fig:mixingdiagrams}
	\end{minipage}
\end{figure}

Music demixing can also be viewed as a combination of multiple music source separation subproblems for all of the desired target stems. To illustrate the example from Figure \ref{fig:mixingdiagrams}: extracting \textit{one} of the guitar, drum, keyboard, or vocal sources (or stems) from the mixed song is music source separation, but extracting \textit{all four} from the mixed audio at the same time is music demixing.

There are many motivations for performing this separation. \citeauthor{musicsepgood} describe that ``we can remix the balance within the music [...] to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1-channel surround sound system... We might also want to change the spatial location of a musical instrument within the mix'' \parencite[31]{musicsepgood}.

\subsubsection{Computational approaches}
\label{sec:musicsepbackground}

Computational source separation has a history of at least 50 years \parencite{musicmask, musicsepintro1}, originating from the tasks of computational auditory scene analysis (CASA) and blind source separation (BSS). In CASA, the goal is to ``computationally extract individual streams from one or two recordings of an acoustic scene'' \parencite[12]{casabook}, based on the definition of ASA (auditory scene analysis) given by \textcite{bregman}. BSS, first defined by \textcite{jutten}, solves a subproblem of CASA which aims to recover the sources of a ``mixture of multiple, statistically independent sources that are received with separate sensors'' \parencite[190]{casabook}. The term ``blind'' refers to there being no prior knowledge of what the sources are, and how they were mixed together. In CASA and BSS, therefore, the mixed audio contains unknown sources combined in unknown ways that must be separated.

By contrast, in music source separation and music demixing, the sources are typically known, or have known characteristics. That is to say, in music source separation, the task is not to separate all of the distinct sources in the mixture, but to extract a predefined set of sources. Some forms of music source separation try to extract harmonic and percussive sources \parencite{fitzgerald1, fitzgerald2, driedger}, which will be described in greater detail in Section \ref{sec:hpss}. Another common set of separation sources are the four sources defined by the MUSDB18 \parencite{musdb18} dataset: vocals, drums, bass, and other. The MUSDB18 dataset will be described in more detail in Section \ref{sec:musicsepdatasets}. Additionally, in music source separation, the mixing process is generally assumed to be a simple linear mixture \parencite{musicsepgood}, shown in equation \eqref{equation:linearmixunitygain}:
\begin{align}
	x_{\text{mix}} = x_{\text{source1}} + x_{\text{source2}} + ... \tag{20}\label{equation:linearmixunitygain}
\end{align}

A popular algorithm in BSS is Independent Component Analysis (ICA) \parencite{musicmask, musicsepgood, musicsepintro1}, which exploits spatial information of the sources, and assumes the sources to be independent. This technique can be used when there are as many channels in the mixture (corresponding to differently placed microphones) as the number of sources. \textcite{ica1, ica2} describe ICA algorithms, and \textcite{blind1, blind2} provide an in-depth review on the history of BSS. Figure \ref{fig:icaposition} shows an example of the positional considerations in a typical ICA system.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/positional.png}
	\caption{Position-based source separation \parencite[35]{musicsepgood}.}
\label{fig:icaposition}
\end{figure}

According to \citeauthor{musicsepintro1}, ICA techniques arose more typically for speech denoising \parencite{speechsep}, and they make assumptions that cannot be generalized easily to music:

\begin{quote}
	[systems which aim] to recover clean speech from noisy recordings [...] can be seen as a particular instance of source separation. [...] many algorithms assume the audio background can be modeled as stationary. However, the musical sources are characterized by a very rich, nonstationary spectrotemporal structure. This prohibits the use of such methods. Musical sounds often exhibit highly synchronous evolution over both time and frequency, making overlap in both time and frequency very common. Furthermore, a typical commercial music mixture violates all the classical assumptions of ICA. Instruments are correlated (e.g., a chorus of singers), there are more instruments than channels in the mixture, and there are nonlinearities in the mixing process (e.g., dynamic range compression) \parencite[1]{musicsepintro1}.
\end{quote}

Techniques more specific to music were developed as a necessity to deal with these differences \parencite{musicseptechniques1, musicseptechniques2}. For cases where ICA cannot be applied, musical source models are more popular, which are ``model-based approaches that attempt to capture the spectral characteristics of the target source can be used'' \parencite[36]{musicsepgood}. Sources are assumed to be sufficiently different from each other, or sparse in their spectral representation \parencite{musicsepgood}, such that they can be extracted with time-frequency masks applied in the spectral domain. Figure \ref{fig:sepgood} shows how different sources have unique spectral patterns.

\begin{figure}[ht]
	\centering
	\subfloat[Mixed spectrogram.]{\includegraphics[width=0.6328\textwidth]{./images-mss/mss1.png}}\\
	\subfloat[Source spectrograms.]{\includegraphics[width=0.6328\textwidth]{./images-mss/mss2.png}}
	\caption{Sparsity of music sources in the spectral domain \parencite[32]{musicsepgood}.}
\label{fig:sepgood}
\end{figure}

Kernel Additive Modeling (KAM) is the simplest form of music source modeling \parencite{musicsepgood}. To estimate a music source at a given time-frequency point, KAM ``select[s] a set of time-frequency bins, which, given the nature of the target source (e.g., percussive, harmonic, or vocals) are likely to be similar in value. This set of time-frequency bins is termed a proximity kernel'' \parencite[36]{musicsepgood}. A well-known example of a KAM-based music separation algorithm is the median-filtering Harmonic/Percussive Source Separation (HPSS) algorithm \parencite{fitzgerald1}.

Spectrogram factorization models are more sophisticated than KAM, and the most popular spectrogram factorization model is Nonnegative Matrix Factorization (NMF) \parencite{musicmask, musicsepgood}. According to \citeauthor{musicsepgood}, NMF ``attempts to factorize a given nonnegative matrix into two nonnegative matrices,'' and it can be applied to the magnitude spectrogram of the mix, $M$, to separate it into frequency weight matrices $W$ and time activation matrices $H$ \parencite[37]{musicsepgood}. A survey on NMF techniques by \textcite{nmfpaper} covers the algorithm in more detail. Most recently, data-driven approaches based on machine learning and deep learning have significantly surpassed past approaches \parencite{musicsepgood, sisec2018}. Figure \ref{fig:spectraldemix} shows different techniques for spectral demixing.

\begin{figure}[ht]
	\centering
	\subfloat[KAM vs. NMF for spectral music demixing.]{\includegraphics[width=0.8438\textwidth]{./images-mss/kamvnmf.png}}\\
	\subfloat[Deep neural networks for spectral music demixing.]{\includegraphics[width=0.8438\textwidth]{./images-mss/mssdnn.png}}
	\caption{Techniques for spectral music demixing \parencite[36, 38]{musicsepgood}.}
	\label{fig:spectraldemix}
\end{figure}

\subsubsection{Public datasets}
\label{sec:musicsepdatasets}

The most popular music stem dataset used by the Signal Separation Evaluation Campaign (SiSEC) and SigSep is the MUSDB18 dataset \parencite{musdb18}, and more recently the HQ (high-quality) version \parencite{musdb18hq}. MUSDB18-HQ contains stereo wav files sampled at 44,100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets \parencite{sisec2016, otherdataset2}.

The MUSDB18-HQ dataset has fixed train, validation, and test subsets of data. The dataset is organized into \Verb#train# and \Verb#test# folders, and a subset from the training folder is defined to be the validation set by the Python MUSDB18-HQ loader library.\footnote{https://github.com/sigsep/sigsep-mus-db}

\subsubsection{Evaluation measures}
\label{sec:evalbss}

To fairly rank and evaluate different systems for music source separation, an objective measurement of the source separation quality is necessary. \textcite{bss, bss2} introduced the BSS (Blind Source Separation) Eval metrics, a ``new numerical performance criteria that can help evaluate and compare algorithms when applied on [blind audio source separation] problems'' \parencite[1463]{bss}. BSS metrics have been widely used in papers introducing new music source separation models \parencite{umx, xumx, dannasep, choi2021, kong2021decoupling, demucs} and in large-scale evaluation campaigns of diverse music source separation systems \parencite{sisec2016, sisec2018, mdx21}. Due to their prominence in source separation literature, I decided to use the BSS metrics in this thesis to measure the performance of my proposed model.

There are four distinct metrics that comprise BSS:

\begin{tight_itemize}
\item
	\textbf{SDR:} \textbf{S}ignal to \textbf{D}istortion \textbf{R}atio
\item
	\textbf{SIR:} \textbf{S}ignal to \textbf{I}nterference \textbf{R}atio
\item
	\textbf{SAR:} \textbf{S}ignal to \textbf{A}rtifacts \textbf{R}atio
\item
	\textbf{ISR:} Source \textbf{I}mage to \textbf{S}patial distortion \textbf{R}atio
\end{tight_itemize}

Out of these four scores, SDR is the single global score that is commonly used to summarize the overall performance of a music demixing system \parencite{sdruseful}. The SDR can be computed from equation \eqref{equation:sdrinstr}:
\begin{align}
	\nonumber & \mathit{SDR}_{\text{instr}} = \\
	&10 \log_{10}\frac{\sum_{n}\big(s_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n)\big)^{2}}{\sum_{n}\big(s_{\text{instr, left}}(n) - \hat{s}_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n) - \hat{s}_{\text{instr, right}}(n)\big)^{2}} \tag{18}\label{equation:sdrinstr}
\end{align}

The SDR score is a relative scale given in decibels or dB, where $s_{\text{instr}}(n)$ denotes the ground truth waveform of the instrument, $\hat{s}_{\text{instr}}(n)$ is the estimate, and left and right refer to the two channels in the stereo dataset of MUSDB18-HQ. Given the four stems (vocals, drums, bass, other) of MUSDB18-HQ, the SDR is computed for each stem from equation \eqref{equation:sdrinstr} above. Then, the four SDR scores are combined for a total song score in equation \eqref{equation:sdrsong}:
\begin{align}
	\mathit{SDR}_{\text{song}} = \frac{1}{4}(\mathit{SDR}_{\text{bass}} + \mathit{SDR}_{\text{drums}} + \mathit{SDR}_{\text{vocals}} + \mathit{SDR}_{\text{other}}) \tag{19}\label{equation:sdrsong}
\end{align}

In the 2018 Signal Source Separation Evaluation Compaign (SiSEC), \textcite{sisec2018} introduced an evolution of BSS metrics, called BSS v4, which reduced the computational cost over the original BSS metrics used in SiSec 2016 \parencite{sisec2016}. The new BSS v4 metrics were also made available in the Python libraries museval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} and bsseval,\footnote{\url{https://github.com/sigsep/bsseval}} which were used in this thesis.

\subsubsection{Time-frequency masking and oracle estimators}
\label{sec:masksandoracles}

Diverse music source separation algorithms based on spectrograms use the technique of time-frequency masking \parencite{musicsepgood, musicmask}. In this section, I will describe the concept of time-frequency masking, and oracle estimators, which can be used to compute the maximum possible quality of a given time-frequency mask from ground truth signals. Two different music source separation systems based on time-frequency masking will be described in upcoming sections: Harmonic/Percussive Source Separation \parencite{fitzgerald1}, a KAM (kernel additive modeling) algorithm for music source separation that will be presented in Section \ref{sec:hpss}, and Open-Unmix \parencite{umx}, a DNN (deep neural network) for music source separation that will be presented in Section \ref{sec:umx}.

\textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, or its real-valued magnitude, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins. A soft mask, or ratio mask, has real values $\in [0.0, 1.0]$, and a binary mask, or hard mask, has logical values (i.e., only zero and one). To compute a binary mask, there must be an additional real-valued parameter, $\theta$, which is the separation factor; values below $\theta$ are set to 0, and values above $\theta$ are set to 1. According to \textcite{masking}, soft masks generally produce a higher quality of sound. An illustration of spectral masking is shown in Figure \ref{fig:simplemasks}, and an example of soft and hard masking is shown in Figure \ref{fig:masks}.

\begin{figure}[ht]
       \centering
        \begin{minipage}{1.\textwidth}
               \renewcommand\footnoterule{} % optional removing footnote bar
               \renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
               \includegraphics[width=0.9375\textwidth]{./images-mss/mask_simple.png}
		\caption[Simple example of applying a time-frequency mask to a spectrogram.]{Simple example of applying a time-frequency mask to a spectrogram.\footnote[1]{\url{https://source-separation.github.io/tutorial/basics/tf_and_masking.html}}}
               \label{fig:simplemasks}
       \end{minipage}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7500\textwidth]{./images-mss/maskdemo.png}
	\caption{Results of a soft and binary oracle mask applied for speech denoising \parencite[71]{masking}. When the binary mask is applied, there is a more dramatic separation, and there is starker contrast between the non-zero and zero parts of the resulting spectrogram. When the soft mask is applied, the separation is more gentle, and a range of zero to non-zero values can be seen in the resulting spectrogram.}
	\label{fig:masks}
\end{figure}

An example of a soft or ratio mask, used by \textcite{fitzgerald1, fitzgerald2}, is given by equation \eqref{equation:softmask}:
\begin{align}
	M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|^{p}}{|\hat{S}_{\text{interference}}|^{p} + |\hat{S}_{\text{target}}|^{p}}\tag{21}\label{equation:softmask}
\end{align}

where $\hat{S}$ represents the complex-valued spectrogram and $p$ represents the raised power ($p = 1$ is the magnitude spectrogram, and $p = 2$ is the power spectrogram). The binary or hard mask used by \textcite{driedger} is given by equation \eqref{equation:hardmask}:
\begin{align}
	M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|}{|\hat{S}_{\text{interference}}| + \epsilon} \le \beta\tag{22}\label{equation:hardmask}
\end{align}

where $\beta$ is the separation factor. Note the inclusion of machine epsilon in the denominator, to avoid division by zero. One advantage of the hard mask's separation factor is that a third residual component can be extracted. This is shown for two arbitrary sources a and b in equations \eqref{equation:residual}:
\begin{align}\tag{23}\label{equation:residual}
	\nonumber & M_{\text{a}} = \frac{|\hat{S}_{\text{a}}|}{|\hat{S}_{\text{b}}| + \epsilon} > \beta, M_{\text{b}} = \frac{|\hat{S}_{\text{b}}|}{|\hat{S}_{\text{a}}| + \epsilon} \ge \beta\\
	\nonumber & M_{\text{residual}} = 1 - (M_{\text{a}} + M_{\text{b}})
\end{align}

The oracle mask, or oracle estimator, is a perfect time-frequency mask which is computed from ground-truth data. The use of the oracle estimator is to give an idea of the upper limit of audio quality from an algorithm or machine learning model for source separation or demixing. Typically, the mask is computed from and applied to the magnitude spectrograms \parencite{fitzgerald1, fitzgerald2, driedger, umx, plumbley1, plumbley2}. Discarding the phase of the complex STFT is a choice made for simplicity, although the phase may be important for source separation applications \parencite{ditchphase}.

To illustrate the calculation of the oracle mask, I will describe a simple case of a mixed song consisting of a vocal and drum track. To compute oracles, note that it is necessary to have access to the ground truth isolated source recordings of vocals and drums, in addition to the mix.

The waveforms $x_{v}[n]$ and $x_{d}[n]$ denote the isolated vocal and drum tracks respectively. The mixed song is defined by the waveform $x_{m}[n] = x_{v}[n] + x_{d}[n]$. To apply the music demixing task to this waveform, I want my algorithm or model to take the mixed waveform $x_{m}[n]$ as an input, and estimate the two source waveforms of vocals $\hat{x}_{v}[n]$ and drums $\hat{x}_{d}[n]$.

Several interesting oracle masks can be calculated from the STFTs of the mix and ground truths of two sources shown in equations \eqref{equation:targetstfts}:
\begin{align}\tag{24}\label{equation:targetstfts}
	\nonumber X_{m} &= \mathit{STFT}(x_{m})\\
	\nonumber X_{v} &= \mathit{STFT}(x_{v})\\
	\nonumber X_{d} &= \mathit{STFT}(x_{d})
\end{align}
 
\textcite{sisec2018} report the performance of several oracles called IRM1, IRM2, IBM1, and IBM2. These acronyms can be understood as follows; the ``I'' stands for Ideal, ``R|B'' denotes a Ratio (soft) vs. Binary (hard) mask, ``M'' stands for Mask, and the trailing number is the $p$th power which the magnitude spectrogram is raised to. For example, the IRM1 is a soft mask between the magnitude spectrograms (since the magnitude spectrogram raised to the power of one is simply itself), and IBM2 is a hard mask between the power spectrograms (i.e., the magnitude spectrogram raised to the power of two).

The oracles for the vocal source (and the same equations apply to the drum track) are computed from equations \eqref{equation:vocaloracles}, noting that a typical default value for $\theta$ is 0.5:
\begin{align}\tag{25}\label{equation:vocaloracles}
	\nonumber \mathit{IRM1}_{v} &= \frac{|X_{v}|^{1}}{|X_{m}|^{1}}\text{,\qquad}
	\nonumber \mathit{IRM2}_{v} = \frac{|X_{v}|^{2}}{|X_{m}|^{2}}\\
	\nonumber \mathit{IBM1}_{v} &= \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} \ge \theta
	\end{cases},
	\nonumber \mathit{IBM2}_{v} = \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} \ge \theta
	\end{cases}
\end{align}

To estimate the time-domain waveform from an oracle mask, the complex STFT of the mixed waveform, $X_{m}$, is multiplied by the mask, and the resultant complex STFT is inverted back to the time-domain waveform using equation \eqref{equation:cplxinvert}:
\begin{align}\tag{26}\label{equation:cplxinvert}
	\nonumber \hat{X}_{v\text{, IRM1}} &= X_{m} \cdot \text{IRM1}_{v}, \hat{x}_{v\text{, IRM1}} = \mathit{iSTFT}(\hat{X}_{v\text{, IRM1}})
\end{align}

In this section, I mentioned that discarding the phase of the complex STFT and applying masks only on the magnitude spectrogram is a common choice in music source separation \parencite{fitzgerald1, fitzgerald2, driedger, umx, plumbley1, plumbley2}. This is done to avoid working with the phase of the STFT \parencite{ditchphase}. Figure \ref{fig:noisyphase} shows how the phase spectrograms of an audio signal and random noise look similar to one another.

\begin{figure}[ht]
       \centering
        \begin{minipage}{1.\textwidth}
               \renewcommand\footnoterule{} % optional removing footnote bar
               \renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
               \includegraphics[width=0.9375\textwidth]{./images-mss/whynophase.png}
		\caption[Phase spectrograms of an audio signal on the left and random noise on the right, showing that phase is difficult to model.]{Phase spectrograms of an audio signal on the left and random noise on the right, showing that phase is difficult to model.\footnote[1]{\url{https://source-separation.github.io/tutorial/basics/phase.html}}}
               \label{fig:noisyphase}
       \end{minipage}
\end{figure}

Systems that only estimate the magnitude STFT of a source need a phase STFT to create a complex STFT, which can be inverted back to a time-domain waveform. In demixing and source separation literature, there is a strategy called ``noisy phase'' \parencite{noisyphase1, noisyphase2}, where the estimated magnitude STFT of the source is combined with the phase STFT of the original mixed (or noisy) audio. The ``noisy phase'' strategy will be described in further detail next.

\subsubsection{Noisy phase or mix-phase inversion (MPI)}
\label{sec:noisyphaseoracle}

In this section, I will describe the ``noisy phase'' strategy in music source separation, which is the strategy of estimating only the magnitude STFT of the source, and using the phase of the original mixed (or noisy) audio. I will also show the design of an oracle estimator for the ``noisy phase'' strategy to estimate its upper limit of quality. This is important for understanding the upper limit of performance of Open-Unmix \parencite{umx}, a model for music source separation that will be described in Section \ref{sec:umx}, and that forms the base of the new model proposed by this thesis in Chapter \ref{ch:methodology}.

The name ``noisy phase'' originates from speech separation, where a common task is to separate speech from noise. In music demixing, referring to interfering musical instruments as ``noise'' is inappropriate, and throughout this section and the rest of this thesis, I will use the term ``mix-phase inversion'' (MPI) to refer to the same idea.

Equations \eqref{equation:mpineural} show how the phase of the mixture and magnitude of the estimated source are used to create a waveform by some arbitrary music demixing system:
\begin{align}\tag{27}\label{equation:mpineural}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \mathit{MusicDemixingSystem}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}} = \mathit{iSTFT}(\hat{X}_{\text{source, est}})
\end{align}

In other words, the magnitude of the isolated source is combined with the phase, or the angle, of the mixed waveform, to produce a complex time-frequency transform, which is then inverted with the backward transform to obtain the estimated isolated source waveform. 

The equations for the MPI oracle can be derived from equations \eqref{equation:mpineural}. The MPI oracle can be computed from ground-truth data, using equations \eqref{equation:mpioracle}:
\begin{align}\tag{28}\label{equation:mpioracle}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}})\\
	\nonumber & X_{\text{source}} = \mathit{STFT}(x_{\text{source}})\\
	\nonumber & \hat{X}_{\text{source, MPI}} = |X_{\text{source}}| \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, MPI}} = \mathit{iSTFT}(\hat{X}_{\text{source, MPI}})
\end{align}

The MPI oracle waveform should provide an idea of the upper limit of performance of a music source separation system that uses the MPI strategy.

\subsubsection{Harmonic/Percussive Source Separation}
\label{sec:hpss}

A simple case of music source separation is Harmonic/Percussive Source Separation (HPSS) \parencite{musicsepgood}. Harmonic (or steady-state, or tonal) sounds are narrowband and steady in time, while percussive (or transient) sounds are broadband and have a fast decay. \textcite{fitzgerald1} noted that they appear as horizontal and vertical lines, respectively, in the STFT, and applied a median filter in the vertical and horizontal directions to estimate the harmonic and percussive components. HPSS is in the category of KAM (kernel additive modeling) \parencite{musicsepgood}.

Median filtering is a technique for image processing where a pixel is replaced by the median value of its neighbors in a window, or filter, which slides across all pixels. Applying a median filter shaped like a horizontal rectangle (i.e., stretching in time) to the STFT causes vertical (or percussive) features to be diminished since their neighboring pixels are empty, which preserves horizontal (or harmonic) features. Similarly, applying a median filter shaped like a vertical rectangle (i.e., stretching in frequency) causes horizontal (or harmonic) features to be diminished, which preserves vertical (or percussive) features. From these estimates, soft masks are computed, which are applied to the original STFT and inverted to create the estimated harmonic and percussive signals. \textcite{driedger} replaced the soft mask with a binary/hard mask, which allows for estimation of a third component, the residual, which is neither harmonic nor percussive, and is described in further detail in Section \ref{sec:masksandoracles}.

The entire process of median-filtering HPSS with binary masks applied to the spectrogram of the glockenspiel signal is shown in Figure \ref{fig:fitz12}.

\begin{figure}[ht]
	\centering
	\subfloat[Median filters applied to the STFT spectrogram.]{{\includegraphics[width=0.4688\textwidth]{./images-hpss/mix_stft_medianfilters.png}} }
	\subfloat[Percussive estimate.]{{\includegraphics[width=0.4688\textwidth]{./images-hpss/perc_stft.png} }}\\
	\subfloat[Harmonic estimate.]{{\includegraphics[width=0.4688\textwidth]{./images-hpss/harm_stft.png} }}
	\subfloat[Residual estimate.]{{\includegraphics[width=0.4688\textwidth]{./images-hpss/resi_stft.png} }}
	\caption{Outputs median filtering HPSS with binary masking applied to the glockenspiel signal.}
	\label{fig:fitz12}
\end{figure}

\textcite{driedger} also introduced a two-pass variant. The first pass separates the harmonic component using an STFT with a large window size for high frequency resolution, followed by the second pass, which separates the percussive component using an STFT with a small window size for high time resolution. \textcite{fitzgerald2} created a similar iterative variant using soft masks, a larger window size for the harmonic separation, and a smaller window size for the percussive separation. Additionally, they noted that when they replaced the small-window STFT with the CQT in the second pass, they obtained a separation of the human singing voice. These modifications of HPSS related to the time-frequency tradeoff of the STFT and CQT directly inspired the hypothesis of this thesis, which is that using the sliCQT instead of the STFT in a neural network for music source separation might improve the results due to the better time-frequency resolution characteristics of the sliCQT.

\subsubsection{Source separation with wavelets}
\label{sec:wavelets}

In this section, I will describe the use of wavelets and wavelet transforms in source separation. Wavelet transforms were designed to overcome the fixed time-frequency resolution of the STFT \parencite{wavelets1, wavelets2}. This relates to the hypothesis of this thesis, which is that the varying time-frequency resolution of the sliCQT might lead to an advantage over the STFT in a music source separation application.

Like the CQT, NSGT, and sliCQT, the wavelet transform uses short windows in the high-frequency regions and long windows in the low-frequency regions, making it more suitable for musical and auditory applications from its nonuniform frequency resolution. As noted in Section \ref{sec:theorynsgt} when introducing the Bark scale, \textcite{barkjust1} used a Bark-scale wavelet transform to improve an audio source separation algorithm that was previously based on the STFT:

\begin{quote}
	Traditionally, short time Fourier transform (STFT) is used in many audio and speech processing applications. Bark-Scaled Wavelet Packet Decomposition (BS-WPD) is a time-frequency signal transformation with non uniform frequency resolution. This transformation reflects the critical bands structure of the human auditory system \parencite[1]{barkjust1}.
\end{quote}

Wavelet transforms relate to the Fourier transform as follows. The Fourier transform, described previously in Section \ref{sec:freqanal}, represents a signal as a sum of sinusoids which are infinite in duration. The infinite duration is, in fact, what leads to the time-frequency uncertainty principle, since it transforms a time-domain waveform into the mutually exclusive frequency domain without any temporal information. Wavelet transforms, instead of using infinite sinusoids, represent signals as a sum of \textit{wavelets}, which are finite in duration:

\begin{quote}
       Many signals and images of interest exhibit piecewise smooth behavior punctuated by transients. Speech signals are characterized by short bursts encoding consonants followed by steady-state oscillations indicative of vowels[...] Unlike the Fourier basis, wavelet bases are adept at sparsely representing piecewise regular signals and images, which include transient behavior. Compare wavelets with sine waves, which are the basis of Fourier analysis. Sinusoids do not have limited duration -- they extend from minus to plus infinity. While sinusoids are smooth and predictable, wavelets tend to be irregular and asymmetric.\footnote{\url{https://www.mathworks.com/help/wavelet/gs/what-is-a-wavelet.html}}
\end{quote}

The wavelet and infinite sinusoid are compared in Figure \ref{fig:waveletinf}.

\begin{figure}[ht]
       \centering
        \begin{minipage}{1.\textwidth}
               \renewcommand\footnoterule{} % optional removing footnote bar
               \renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
               \includegraphics[width=0.65635\textwidth]{./images-wavelets/wavelet.png}
               \caption[Infinite sine wave of the Fourier transform vs. finite wavelet.]{Infinite sine wave of the Fourier transform vs. finite wavelet.\footnote[1]{\url{https://www.mathworks.com/help/wavelet/gs/what-is-a-wavelet.html}}}
               \label{fig:waveletinf}
       \end{minipage}
\end{figure}

\textcite{wavelets} use a wavelet transform to perform source separation. They create a \textit{scalogram}, which is the wavelet equivalent of a spectrogram, using a wavelet transform similar to the constant-Q transform \parencite[2--3]{wavelets}:

\begin{quote}
       The time-frequency structures of audio signals are well revealed by Q-constant filter banks, which model the signal cochlea transformation. This can also be written as a multiscale wavelet transform, whose modulus defines a time-frequency representation that is called a scalogram image[...] We show that a wavelet transform can partly separate the time-frequency supports of two different harmonic template models, and characterize the overlap of these time-frequency supports.
\end{quote}

Figure \ref{fig:waveletsep} shows the wavelet scalogram-based source separation.

\begin{figure}[ht]
       \centering
       \includegraphics[width=0.65635\textwidth]{./images-wavelets/wavelet_sep.png}
       \caption{Source separation using wavelets. Cyan shows the first harmonic source, yellow shows the second harmonic source, and red shows the overlap \parencite[5]{wavelets}.}
       \label{fig:waveletsep}
\end{figure}

\subsubsection{Open-Unmix (UMX)}
\label{sec:umx}

In this thesis, my goal is to address the fixed time-frequency resolution limitation of the STFT in music source separation applications. Open-Unmix (UMX) was created by \textcite{umx} as an open-source, reference implementation of a deep neural network for music source separation based on the STFT. Due to the code being open-source and freely available, I chose to begin with UMX as the initial STFT-based model in this thesis. My goal is to replace the STFT with the sliCQT shown in Section \ref{sec:theoryslicqt}.

UMX is based on time-frequency masking of the STFT, which was described previously in Section \ref{sec:masksandoracles}. UMX also only estimates the spectrogram of the target, and uses the phase of the mixed audio, which is a common strategy for spectrogram-based music demixing described previously in Section \ref{sec:noisyphaseoracle}.

UMX uses the openly available MUSDB18 and MUSB18-HQ datasets for its training data \parencite{musdb18, musdb18hq} to encourage reproducible research \parencite{umx}. The Python source code of the reference implementation of UMX is openly available,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} which uses PyTorch \parencite{pytorch} for GPU-accelerated numerical computation and deep learning.

In UMX, a deep neural network (DNN) is used to estimate the magnitude spectrograms of the sources from an input mixed song. The architecture of the DNN is a variant of a sequence2sequence model. UMX uses a bidirectional LSTM or Bi-LSTM architecture, which is based on two predecessor networks by \textcite{umxorig1} and \textcite{umxorig2}. Figure \ref{fig:umxes1} shows the architecture of UMX, and Figure \ref{fig:umxes1point5} shows one of its early predecessors.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}

		\centering
		\includegraphics[width=0.9375\textwidth]{./images-neural/umx.png}
		\caption[UMX Bi-LSTM architecture.]{UMX Bi-LSTM architecture.\footnote[1]{\url{https://github.com/sigsep/open-unmix-pytorch\#-the-model-for-one-source}}}
		\label{fig:umxes1}
	\end{minipage}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-neural/typical_simple_mss_dnn.png}
	\caption{Simple DNN architecture for music source separation {\parencite[262]{umxorig1}}.}
	\label{fig:umxes1point5}
\end{figure}

The MUSDB18-HQ data loader is the starting point of the neural network, and it returns a single target at a time, or an $(x, y)$ pair where $x$ is the mixed waveform and $y$ is the desired target. The final output of UMX is $\hat{y}$, the estimated target waveform. In between the input and output time-domain waveforms, the magnitude STFT domain is used by UMX. The important training and inference steps are as follows.

Data whitening refers to a process in machine learning where the mean $\mu$ and standard deviation $\sigma$ of a training dataset $X$ is used to normalize $X$, or to transform $X$ into a normal Gaussian distribution $X'$ with a mean of zero and a standard deviation of one \parencite{Kessy_2018}. Data whitening is important because it ``greatly simplifies multivariate data analysis both from a computational and a statistical standpoint,'' and ``whitening is a critically important tool, most often employed in preprocessing'' \parencite[309]{Kessy_2018}.

In the training loop, the $(x, y)$ pairs of mixed audio and the ground-truth of the target source waveform are converted to the magnitude STFT domain $(|X|, |Y|)$. To apply data whitening, the mean and standard deviation of $|X|$ are computed, or $\mu_{|X|} = \mathit{mean}(\{|X|\})$ and $\sigma_{|X|} = \mathit{std}(\{|X|\})$. The training input to the Bi-LSTM neural network is $(|X|+\mu_{|X|})\times\sigma_{|X|}$. The Bi-LSTM neural network outputs $|\hat{Y}|$, the estimated magnitude STFT of the target sources.

The loss function is the mean-squared error (MSE) function. In the loss function, the estimated magnitude STFT of the target source is compared to the ground-truth, shown in equation \eqref{equation:mselossstft}.
\begin{align}\tag{29}\label{equation:mselossstft}
	\nonumber & \mathit{MSE}(Y, \hat{Y}) = \frac{1}{n} \sum_{i = 1}^{n}{(Y_{i}-\hat{Y}_{i})^{2}}
\end{align}

where $n$ is the number of elements in the magnitude STFT matrix.

There are four independent Bi-LSTM neural networks, shown in Figure \ref{fig:umxes2}, for each of the four sources: vocals, bass, drums, and other. The four Bi-LSTM models are trained until the loss stops improving. At this point, UMX is ready to perform inference.

First, the four independently-trained Bi-LSTM models are used to obtain the estimated magnitude STFTs of the four target sources. The mix phase inversion is used to create a first estimate of the waveforms for the four targets. Equations \eqref{equation:mpirepeat} show how the estimated magnitude STFT of UMX Bi-LSTM is converted to the time-domain waveform estimate:

\begin{align}\tag{30}\label{equation:mpirepeat}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \mathit{UMX}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}} = \mathit{iSTFT}(\hat{X}_{\text{source, est}})
\end{align}

Similar equations were previously described for the mix-phase inversion (MPI) oracle in Section \ref{sec:noisyphaseoracle}.

Finally, the four estimated magnitude STFTs are considered together, along with the phase of the STFT of the original mixed audio, to perform a post-processing step called iterative Wiener filtering and expectation-maximization (EM), jointly referred to as ``Wiener-EM'' \parencite{umxorig1, wiener2, wiener3, wiener4}. The output of the Wiener-EM step is a refined estimate of the waveforms of the four target sources, which is the final output of UMX.

Figure \ref{fig:umxes2}(a) shows the training of a single UMX Bi-LSTM network, and Figure \ref{fig:umxes2}(b) shows the four independent networks performing inference for the four targets (vocals, drums, bass, other) of MUSDB18-HQ with the Wiener-EM post-processing step.

\begin{figure}[ht]
	\centering
	\subfloat[UMX, single target training.]{\includegraphics[width=0.7500\textwidth]{./images-blockdiagrams/umx_new_single.png}}\\
	\subfloat[UMX, four targets inference.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/umx_new_multiple.png}}
	\caption{UMX training for a single target and inference for four targets.}
	\label{fig:umxes2}
\end{figure}

\subsubsection{CrossNet-Open-Unmix (X-UMX)}
\label{sec:xumx}

I will now introduce a closely-related variant of Open-Unmix (UMX), called CrossNet-Open-Unmix (X-UMX) \parencite{xumx}. X-UMX improves on the music demixing performance of UMX without introducing any additional parameters in the neural network. Since X-UMX is mostly identical to UMX with better performance, I will be using it as the starting point for the new model that will be proposed by this thesis in Chapter \ref{ch:methodology}.

\textcite{xumx} combined the four separate target networks of UMX into a single model to apply the loss functions and optimizations jointly across all four targets, rather than optimizing each one separately, shown in Figure \ref{fig:umxandxumx}(b). X-UMX also includes loss computed from the time-domain waveforms, in addition to the loss measured from the magnitude spectral coefficients from the original UMX.

The loss function of X-UMX is first modified to include a multi-domain loss (MDL), which includes the existing frequency-domain loss (spectrogram MSE) in addition to a new time-domain loss. The time-domain loss is the same SDR score from the BSS metrics which was shown in Section \ref{sec:evalbss}.

I showed in equation \eqref{equation:mpirepeat} how to convert from the estimated magnitude spectrogram of the neural network to the waveform estimate. In X-UMX, these equations are used to compute the time-domain waveforms and apply the SDR loss function.

The next addition to the loss function of X-UMX is the combination loss (CL), which considers 14 possible combinations of the four target sources (vocals, drums, bass, other) in both the spectral MSE loss and time-domain SDR loss. The 14 combinations are shown in Table \ref{table:14targets}.

\begin{table}[ht]
	\centering
	\caption{14 combinations of the four targets in X-UMX.}
	\label{table:14targets}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Combination size & Combination number & Targets \\
	 \hline
	 \hline
	 	1 & 1 & bass \\
	 \hline
	 	1 & 2 & vocals \\
	 \hline
	 	1 & 3 & other \\
	 \hline
	 	1 & 4 & drums \\
	 \hline
	 	2 & 5 & bass + vocals \\
	 \hline
	 	2 & 6 & bass + other \\
	 \hline
	 	2 & 7 & bass + drums \\
	 \hline
	 	2 & 8 & vocals + other \\
	 \hline
	 	2 & 9 & vocals + drums \\
	 \hline
	 	2 & 10 & other + drums \\
	 \hline
	 	3 & 11 & bass + vocals + other \\
	 \hline
	 	3 & 12 & bass + vocals + drums \\
	 \hline
	 	3 & 13 & bass + other + drums \\
	 \hline
	 	3 & 14 & vocals + other + drums \\
	 \hline
\end{tabular}
\end{table}

Equations \eqref{equation:cl} show the computation of the total frequency-domain loss from the 14 combinations, where $|Y|$ represents the magnitude spectrogram of the ground-truth target waveform, and $\hat{|Y|}$ represents the estimated magnitude spectrogram of the neural network. The subscript of $|Y|$ and $|\hat{Y}|$ represents the target (1--4 for vocals, drums, bass, and other):
\begin{align}\tag{31}\label{equation:cl}
	\nonumber & \text{mse\_loss}_{1} = \mathit{MSE}(\hat{|Y|}_{1}, |Y|_{1})\\
	\nonumber & \text{(... repeat for size-1 combinations)}\\
	\nonumber & \text{mse\_loss}_{5} = \mathit{MSE}(\hat{|Y|}_{1} + \hat{|Y|}_{2}, |Y|_{1} + |Y|_{2})\\
	\nonumber & \text{(... repeat for size-2 combinations)}\\
	\nonumber & \text{mse\_loss}_{11} = \mathit{MSE}(\hat{|Y|}_{1} + \hat{|Y|}_{2} + \hat{|Y|}_{3}, |Y|_{1} + |Y|_{2} + |Y|_{3})\\
	\nonumber & \text{(... repeat for size-3 combinations)}\\
	\nonumber & \text{mse\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{mse\_loss}_{n}}
\end{align}

For the time-domain loss, the same 14 combinations are considered in the time domain, using the ground-truth time-domain waveforms $y$ and the estimated time-domain waveform $\hat{y}$ from the mix-phase inversion, shown in the equations \eqref{equation:mdl}:
\begin{align}\tag{32}\label{equation:mdl}
	\nonumber & \text{sdr\_loss}_{1} = \mathit{SDR}(\hat{y}_{1}, y_{1})\\
	\nonumber & \text{(... repeat for size-1 combinations)}\\
	\nonumber & \text{sdr\_loss}_{5} = \mathit{SDR}(\hat{y}_{1} + \hat{y}_{2}, y_{1} + y_{2})\\
	\nonumber & \text{(... repeat for size-2 combinations)}\\
	\nonumber & \text{sdr\_loss}_{11} = \mathit{SDR}(\hat{y}_{1} + \hat{y}_{2} + \hat{y}_{3}, y_{1} + y_{2} + y_{3})\\
	\nonumber & \text{(... repeat for size-3 combinations)}\\
	\nonumber & \text{sdr\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{sdr\_loss}_{n}}
\end{align}

As a last step, the time-domain loss is multiplied by a mixing coefficient before adding it to the frequency-domain loss. The coefficient is set to 10 to ``approximately equalize the ranges'' of the two losses \parencite[4]{xumx}. The total loss function of X-UMX is calculated by equation \eqref{equation:totalloss}:
\begin{align}\tag{33}\label{equation:totalloss}
	\nonumber & \text{loss}_{X-UMX} = \text{mse\_loss}_{\text{total}} + 10 \cdot \text{sdr\_loss}_{\text{total}}
\end{align}

Figure \ref{fig:umxandxumx} shows how independent UMX networks for the desired four targets (vocals, drums, bass, other) are combined together in X-UMX, illustrating the key concepts of the multi-domain loss functions (MDL), which computes loss for both the magnitude spectrograms and time-domain waveforms, and the combination losses across the four targets (CL). A visualization of the multi-domain loss (MDL) and combination loss (CL) for four targets is shown in Figure \ref{fig:xumxlosses}.

\begin{figure}[ht]
	\centering
	\subfloat[UMX system for a single target with loss function.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/umx_single_target.png}}\\
	\subfloat[X-UMX network for all four targets with MDL and CL loss functions.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/xumx_multiple_targets.png}}
	\caption{UMX and X-UMX loss functions compared. In UMX, a copy of the network is trained independently for each target, using single-target spectrogram loss. In X-UMX, four copies of the network are trained simultaneously for all four targets, using multi-domain and combination loss functions as shown in Figure \ref{fig:xumxlosses}.}
	\label{fig:umxandxumx}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[Multi-domain loss (MDL), showing the frequency-domain MSE loss from the spectrograms and time-domain SDR loss from the waveforms summed with a mixing coefficient from equation \eqref{equation:totalloss}.]{\includegraphics[width=0.9375\textwidth]{./images-neural/xumx1_2.png}}\\
	\subfloat[Combination loss (CL), showing the 14 combinations of the four targets from Table \ref{table:14targets}.]{\includegraphics[width=0.9375\textwidth]{./images-neural/xumx2.png}}
	\caption{Diagrams of the loss functions of X-UMX \parencite[2]{xumx}.}
	\label{fig:xumxlosses}
\end{figure}

Figure \ref{fig:umxandxumx} shows how X-UMX encompasses UMX entirely, and adds parameter-free enhancements with the new MDL and CL loss functions. For simplicity, having a single training loop and single trained model to perform the separation is easier than training four independent models. For this reason, and that the music demixing performance of X-UMX is higher than UMX, I chose to base this thesis on the X-UMX variant of UMX.

The available implementation of X-UMX\footnote{\url{https://github.com/sony/ai-research-code/tree/master/x-umx}} uses NNabla,\footnote{\url{https://nnabla.org/}} which is a different Python deep learning framework from PyTorch. The X-UMX additions are easily copied from the NNabla implementation of X-UMX into the PyTorch implementation of UMX.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} In the upcoming sections, I will use X-UMX to refer to the PyTorch implementation of UMX with the changes of X-UMX included, since PyTorch is the deep learning framework chosen for this thesis.

\subsubsection{Convolutional denoising autoencoder (CDAE)}
\label{sec:cdae}

While Open-Unmix is based on an RNN architecture, a different type of neural network architecture based on CNN, the convolutional denoising autoencoder (CDAE), has been successfully used in music demixing \parencite{plumbley1, plumbley2}. CDAEs have fewer learnable parameters than fully-connected network (FCN) or recurrent neural network (RNN) architectures with comparable music source separation performance \parencite{plumbley1}, resulting in a smaller model size. For this reason, I chose the CDAE as one of the models to base my work on in this thesis.

The architecture of a simple CDAE is shown in Figure \ref{fig:cdaes}, where 2D convolutions (in the time and frequency dimensions) are applied on the magnitude STFT. In the encoder layer, max-pooling layers are used after the convolutional layers to reduce the dimensionality of the input spectrogram and ``extract robust low-dimension features from the input data'' \parencite[1]{plumbley1}. In the decoder layer, up-sampling layers are used after the convolutional layers to increase the low-dimension encoded representation back to the original dimensions of the input spectrogram. In Section \ref{sec:cnn}, I provided an overview on convolutional, deconvolutional, pooling, and up-sampling layers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-neural/cdae_1.png}
	\caption{Diagram of a CDAE {\parencite[2]{plumbley1}}, where 2D convolutional layers are applied to the input spectrogram. Max-pooling layers are used in the encoder to reduce the dimensionality, and up-sampling layers are used in the decoder to increase the dimensionality.}
	\label{fig:cdaes}
\end{figure}

\end{document}
