\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{sec:background}

\subsection{Acoustic signals and the time-domain waveform}
\label{sec:timedomain}

\citeauthor{discretebook} define a signal as a function of one or more variables which ``conveys information about the state or behavior of a physical system'' \parencite[8]{discretebook}. Some examples of signals are a 2-dimensional image, which is a brightness function of two spatial variables, and a speech signal, which is a function of time. \citeauthor{moore} describes the more specific case of a sound or audio signal as being the description of the vibration of an object and how it impresses this vibration upon the ``surrounding medium (usually air) as a pattern of changes in pressure'' \parencite[2]{moore}.

According to \textcite{melbook}, the waveform is a natural and mathematically convenient representation of a signal. For acoustic signals, the waveform represents the continuously varying pattern of the signal as a function of the continuous variable $t$, which represents time. A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers \parencite{discretebook}. The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$.  A continuous-time signal and its discrete representation are shown in Figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform]{\includegraphics[width=0.9\textwidth]{./images-tftheory/continuoustime.png}}\\
	\subfloat[Discrete-time waveform]{\includegraphics[width=0.9\textwidth]{./images-tftheory/discretetime.png}}
	\caption{A continuous-time signal and its discrete-time representation sampled with $T = 125\mu s$ \parencite[10]{discretebook}}
	\label{fig:discretecontinuous}
\end{figure}

Continuous-time signals are converted into discrete-time representations by two processes: sampling and quantization. Points on the continuous time axis of the waveform are \textit{sampled} periodically, and the amplitude values of the waveform at these sampled points are \textit{quantized} to find the closest digital number \parencite{melbook}. Time sampling is controlled by the sampling period $T \text{ seconds}$, or the sampling rate $F_{s} = \sfrac{1}{T} \text{ Hz}$, which define the periodicity of the sampling. Amplitude quantization is controlled by the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x_{a}(t)$ is defined by $x[n] = x_{a}(nT)$, where $n$ is an integer and $T = \sfrac{1}{F_{s}}$ is the sampling period. The Nyquist-Shannon sampling theorem \parencite{discretebook}, described independently by \textcite{nyquist1928} and \textcite{shannon1948}, states that the maximum frequency of a signal that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \sfrac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency.

Aliasing is one of the pitfalls of choosing an inappropriate sampling rate for the frequencies present in the signal under observation \parencite{dspfirst}. The diagram in Figure \ref{fig:aliasing} shows the phenomenon of aliasing due to undersampling, which occurs when a time-domain signal is undersampled, i.e. $F_{\text{sig}} > F_{\text{nyq}}$, and the continuous-time signal cannot be reconstructed accurately.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-tftheory/aliasing_undersampling.png}
	\caption{An undersampled cosine wave (red) and the resulting incorrect reconstruction (black) \parencite[82]{dspfirst}}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \citeauthor{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values'' \parencite[190]{discretebook}. The second variable which relates to the quantization process is the number of quantization levels. The quantization operation is represented as $ \hat{x}[n] = Q(x[n])$, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation \parencite{discretebook}. An A/D or analog-to-digital converter (ADC) circuit and its operation on a waveform is shown in Figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC)]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\vspace{0.1em}
	\subfloat[Waveform results of the ADC process]{\includegraphics[width=0.85\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations \parencite[188, 190, 192]{discretebook}}
	\label{fig:adccircuit}
\end{figure}

\subsection{Transforms of acoustic signals}
\label{sec:freqdomain}

\citeauthor{moore} states that

\begin{quote}
	[a]lthough all sounds can be specified by their variation in pressure with time, it is often more convenient, and more meaningful, to specify them in a different way when the sounds are complex. This method is based on a theorem by Fourier, who proved that almost any complex waveform can be analyzed, or broken down, into a series of sinusoids with specific frequencies, amplitudes, and phases. This is done using a mathematical procedure called the Fourier Transform \parencite[4]{moore}.
\end{quote}

Throughout this section, the description of the Fourier Transform of acoustic signals and the evolution of further transforms that build on it will be covered in detail.

The Large Time-Frequency Analysis toolbox (LTFAT) is ``a Matlab/Octave toolbox for working with time-frequency analysis and synthesis.''\footnote{\url{http://ltfat.org/}} It contains a test signal from the glockenspiel instrument, loaded by the \Verb#gspi# function.\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} This signal can be seen in audio signal processing papers on the topic of time-frequency \parencite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt, wmdct}. This is because the glockenspiel contains both tonal and transient properties, which have conflicting needs for time and frequency resolution in their analysis. Figure \ref{fig:glockwaveform} shows the discrete-time waveform of the glockenspiel signal. For the rest of this chapter, demonstrations of each transform will use this same glockenspiel signal as the input $x[n]$. The signal has a total duration of 5.94 seconds, and is sampled with a rate of 44,100 Hz.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.725\textwidth]{./images-gspi/glock_waveform.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

\subsubsection{Frequency analysis and Fourier Transforms (CTFT, DFT, FFT)}
\label{sec:freqanal}

The Fourier Transform originated as an integral transform in mathematics, which are a class of ``useful tools for solving problems involving certain types of partial differential equations (PDE), mainly when their solutions on the corresponding domains of definition are difficult to deal with'' \parencite[54]{fourierhistory}. The Fourier Transform was originally introduced by Joseph Fourier in his earlier papers \parencite{fourierhist1, fourierhist2}, and fully expanded and collected in his seminal work on heat \parencite{fourierheat}. The connection of the Fourier Transform to music is described by \citeauthor{fouriermusic}, who state that

\begin{quote}
	[b]eyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contribution represents some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string \parencite[461]{fouriermusic}.
\end{quote}

The continuous-time Fourier Transform (CTFT) of a time-domain acoustic waveform is defined by the pair of equations \eqref{equation:ctft} and \eqref{equation:ictft} \parencite[308]{dspfirst}:
\begin{align}
	X(j\omega) = \int_{-\infty}^{\infty}{x(t)e^{-j\omega t}\mathit{dt}} \tag{1}\label{equation:ctft} \\
	x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{X(j\omega)e^{j\omega t}\mathit{d\omega}} \tag{2}\label{equation:ictft}
\end{align}

\textcite{dspfirst} calls $X(j\omega)$ the frequency-domain representation of the signal $x(t)$. Equation \eqref{equation:ictft} defines the signal $x(t)$ in terms of a sum of infinitely many complex-exponential signals with $X(j\omega)$ controlling the amplitude and phases of these signals. The continuous-time Fourier Transform provides a one-to-one mapping of the time domain to the frequency domain.

Signals needs to be transformed from the continuous to the discrete domain via sampling to be processed digitally or computationally, which was covered previously in Section \ref{sec:timedomain}. The discrete-time Fourier Transform (DTFT), also called the discrete Fourier Transform (DFT), is derived from a sampled version of the continuous-time Fourier Transform shown previously in equations \eqref{equation:ctft} and \eqref{equation:ictft}, and is defined by the pair of equations \eqref{equation:dtft} and \eqref{equation:idtft} \parencite[289]{melbook}:
\begin{align}
	X(e^{j\omega}) = \sum_{n = -\infty}^{\infty}{x[n]e^{-j\omega n}} \tag{3}\label{equation:dtft} \\
	x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}{X(e^{j\omega})e^{j\omega n}\mathit{d\omega}} \tag{4}\label{equation:idtft}
\end{align}

The Fourier Transform is a complex-valued function of $\omega$, which is the variable that represents the angular frequency in radians. The Fourier Transform can be expressed in the rectangular form in equation \eqref{equation:rect} or polar form in equation \eqref{equation:polar} \parencite[49]{discretebook}:
\begin{align}
	X(e^{j\omega}) = X_{\text{real}}(e^{j\omega}) + j X_{\text{imag}}(e^{j\omega}) \tag{5}\label{equation:rect} \\
	X(e^{j\omega}) = |X(e^{j\omega})|e^{j\angle X(e^{j\omega})} \tag{6}\label{equation:polar}
\end{align}

The quantities $|X(e^{j\omega})|$ and $\angle X(e^{j\omega})$ are referred to as the magnitude and phase respectively. The Fourier Transform is also referred to as the spectrum, while its magnitude and phase are the magnitude and phase spectra respectively \parencite{discretebook}.

According to \textcite{skiena}, algorithms in computer science are often described by their time complexity in a hypothetical computer where simple operations take one time step. The ``Big-O'' notation, or $O(N)$, provides the upper bound of algorithm running time in relation to number of input elements. In the case of the DFT, the number of input elements is the number of samples in the discrete signal. In \textcite[Chapter~9]{discretebook} it is described that in its original formulation, the algorithmic complexity of the DFT is $O(N^{2})$, i.e. the running time of the algorithm grows proportionally the size of the input signal squared \parencite{skiena}. 

Starting from legendary mathematician Carl Friedrich Gauss in 1805 \parencite{gausshist}, and reaching its most famous formulation published by \textcite{cooleytukey}, a family of efficient algorithms for the computation of the DFT by computing a series of smaller DFTs, known collectively as the Fast Fourier Transform (FFT), reduced this computation time to $O(N \log{N})$. This resulted in the FFT becoming one of the most important algorithms of the 20th century \parencite{ffttopten}.

The number of points, or samples, of the DFT, determines the frequency resolution, also called $\mathit{df}$ or $\mathit{\Delta f}$, which is the frequency spacing between each point in the resulting spectrum \parencite{discretebook}. The frequency resolution of an $N$-point DFT is $\mathit{df} = \sfrac{F_{s}}{N}$, where $F_{s}$ is the sampling rate of the input signal. An illustration of the magnitude and phase spectra of the DFT are shown in Figure \ref{fig:glockdft}, using two different lengths of DFT to show the difference in the low and high frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[2,048-point DFT, $\mathit{df} = 21.533 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/glock_dft_2048.png}}\\
	\subfloat[262,144-point DFT, $\mathit{df} = 0.168 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/glock_dft_262144.png}}
	\caption{DFT of the Glockenspiel waveform. Note the richer frequency information in the higher frequency resolution transform in (b)}
	\label{fig:glockdft}
\end{figure}

For a real-valued input signal $x[n]$, the spectrum has the conjugate symmetry property, i.e., the DFT coefficients are mirrored symmetrically around the center frequency \parencite{dspfirst}. This center bin corresponds to the Nyquist frequency $\sfrac{F_{s}}{2}$ where $F_{s}$ is the sampling rate of the signal, which was described in Section \ref{sec:timedomain}. In practice, for a given DFT of $N$ points, the points of the output spectrum between 0--$\sfrac{N}{2}+1$ correspond to 0--$\sfrac{F_{s}}{2}$ Hz, and the points $\sfrac{N}{2}+1$--N correspond to the same frequency bins in reverse order, i.e., $\sfrac{F_{2}}{2}$--0 Hz. In practice, therefore, the useful output coefficients of an $N$-point DFT of a real signal $x[n]$ are the first $\sfrac{N}{2}+1$ points.

\newpagefill

\subsubsection{Joint time-frequency analysis with the Gabor Transform and Short-time Fourier Transform (STFT)}
\label{sec:jointtfa}

Continuing from the discussion of the DFT in the previous Section \ref{sec:freqanal},\citeauthor{discretebook} state that ``often, in practical applications of sinusoidal signal models, the signal properties (amplitudes, frequencies, and phases) will change with time. For example, nonstationary signal models of this type are required to describe radar, sonar, speech, and data communication signals. A single DFT estimate is not sufficient to describe such signals...'' \parencite[714]{discretebook}.

Dennis Gabor's seminal signal processing paper, \textit{The Theory of Communication}, introduced significant and far-reaching concepts in the time and frequency analysis of acoustic signals \parencite{gabor1946}. \citeauthor{gabor1946} quotes famed American telecommunication engineer John Carson \parencite{carsonfamous} to describe the limitations of the Fourier Transform:

\begin{quote}
	[t]he foregoing solutions [of the Fourier Transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren \parencite[431]{gabor1946}
\end{quote}

According to \citeauthor{korpel}, ``Gabor came to the conclusion that the difficulty lay in our mutually exclusive formulations of time analysis and frequency analysis ... he suggested a new method of analyzing signals in which time and frequency play symmetrical parts'' \parencite[3,624]{korpel}.

Gabor derived the principal of time-frequency uncertainty from the Heisenberg uncertainty principle in quantum physics, which states that the more precisely the position of an electron is determined, the less precisely the momentum is known, and conversely \secondaryciteindirect{heisenberg1927}{hallm}. \citeauthor{gabor1946} states that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit'' \parencite[432]{gabor1946}. This is referred to as the time-frequency uncertainty principle or the Gabor limit. Gabor defines the unit of time-frequency information $\Delta t \Delta f$ as the \textit{logon}, where $\Delta t$ and $\Delta f$ are defined as ``the uncertainties inherent in the definition of the epoch t and frequency f of an oscillation'' \parencite[432]{gabor1946}.

Let us start with a description of the time-domain unit impulse signal or sequence \parencite[20]{melbook} in equation \eqref{equation:delta}:
\begin{flalign}\label{equation:delta}
\delta[n] = \begin{cases}
	1 \text{\hspace{1em}} n = 0\\
	0 \text{\hspace{1em}} \text{otherwise}
\end{cases}
\end{flalign}

The impulse is a useful signal, as it is the ``simplest [time-domain] sequence because it has only one nonzero value, which occurs at n = 0. The mathematical notation is that of the Kronecker delta function'' \parencite[107]{dspfirst}. Note the Kronecker delta function is the discrete equivalent of the Dirac delta function \parencite[937]{melbook}. Contrast this with the DFT spectrum of a signal that is only nonzero at the 0 Hz (direct current, or DC) frequency component, which is a ``sinusoid of zero frequency'' \parencite[13]{dspfirst}.

Figure \ref{fig:gaborfirst}(a) shows the unit impulse contrasted with the DC-component DFT, and these two in fact demonstrate the mutually exclusive formulations of time and frequency. The unit impulse, which has a single nonzero value in the time domain, has an infinite extent in the frequency domain. Conversely, the DC-component DFT has a single nonzero value in the frequency domain, but has an infinite extent in the time domain. Figure \ref{fig:gaborfirst}(b) shows the intuition of the time-frequency tradeoff as a choice of signal length. The periodicity of the sine wave is more apparent over longer periods of time $\Delta t$, but the detail of the individual sample values become lost.

\begin{figure}[ht]
	\centering
	\subfloat[Mutually exclusive formulations of time and frequency by two extremes, the unit impulse (bottom left) and the 0 Hz cosine DFT spectrum (top right)]{\includegraphics[width=0.6\textwidth]{./images-tftheory/gabor13.png}}\\
	\subfloat[The periodicity of the sine wave is more apparent with a longer $\Delta t$, at the expense of temporally localized samples]{\includegraphics[width=0.6\textwidth]{./images-tftheory/gabor2.png}}
	\caption{Time-frequency tradeoff intuitions \parencite[103, 106]{gabor2}}
	\label{fig:gaborfirst}
\end{figure}

The result of the time-frequency uncertainty principle is a consequence of how the Fourier Transform is used to swap between the mutually exclusive domains of time and frequency. To further elaborate that this is a property of the Fourier Transform and not a fundamental physical limitation, several psychacoustic studies have shown that humans can exhibit better time-frequency resolution than the Gabor limit. \citeauthor{psycho2} describes one of these experiments:

\begin{quote}
	It is concluded that models based on a place (spectral) analysis should be subject to a limitation of the type $\Delta f \cdot d \ge \text{constant}$, where $\Delta f$ is the frequency difference limen for a tone pulse of duration d. [...]  It was found that at short durations the product of $\Delta f$ and d was about one order of magnitude smaller than the minimum predicted [...] \parencite[610]{psycho2}
\end{quote}

More recently, according to \citeauthor{psycho1}:

\begin{quote}
	[w]e have conducted the first direct psychoacoustical test of the Fourier uncertainty principle in human hearing, by measuring simultaneous temporal and frequency discrimination. Our data indicate that human subjects often beat the bound prescribed by the uncertainty theorem, by factors in excess of 10 \parencite[4]{psycho1}.
\end{quote}

\citeauthor{psycho1} goes on to state that ``most sound analysis and processing tools today continue to use models based on spectral theories... [w]e believe it is time to revisit this issue'' \parencite[4]{psycho1}. Nevertheless, if we choose to proceed with time-frequency analysis despite this limitation, it is preferable to minimize time-frequency uncertainty, or to set the \textit{logon} ($\Delta t \Delta f$) to its lowest possible value. \citeauthor{gabor1946} asks:

\begin{quote}
	What is the shape of the signal for which the product $\Delta t \Delta f$ actually assumes the smallest possible value? [... it is] the modulation product of a harmonic oscillation of any frequency with a pulse of the form of the probability function \parencite[435]{gabor1946}
\end{quote}

Gabor performed joint time-frequency analysis by multiplying overlapping, temporally consecutive portions of the input signal with shifted copies of the Gaussian window function (i.e. the probability function), and by taking the Fourier Transform of the windowed segments of the signal. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described by equations \eqref{equation:gabort}:
\begin{flalign}\tag{7}\label{equation:gabort}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)]\\
	\nonumber G_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n}
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform \parencite{dictionary}.

The STFT, or Short-Time Fourier Transform, has been described independently from Gabor's work \parencite{stftindie}, but additional research in the 1980s \parencite{dictionary} led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described by equations \eqref{equation:stft}:
\begin{flalign}\tag{8}\label{equation:stft}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)]\\
	\nonumber X_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n}
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window \parencite{dictionary}. Note how similar equations \eqref{equation:gabort} and \eqref{equation:stft} are, which is expected since the original Gabor transform is the STFT with a Gaussian window. In practice, the STFT allows the use of different windows and overlap sizes \parencite{stftinvertible}, as long as the constant overlap-add (COLA) constraint is respected \parencite{cola}. Figure \ref{fig:stftdiagram} shows how a windowed Fourier Transform (i.e. Gabor transform or STFT) is performed on a waveform.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\includegraphics[width=0.8\textwidth]{./images-tftheory/stft_diagram.png}
		\caption[Forward and inverse STFT or Gabor transform]{Forward and inverse STFT or Gabor transform\footnote[1]{\url{https://www.mathworks.com/help/signal/ref/iscola.html}}}
		\label{fig:stftdiagram}
	\end{minipage}
\end{figure}

Figure \ref{fig:gabortf} shows different sizes of \textit{logon} in the time-frequency plane, and how the Gabor transform and the STFT with higher frequency or higher time resolution appear on the time-frequency plane.

\begin{figure}[ht]
	\centering
	\subfloat[Pure time domain, pure frequency domain, and Gabor's time-frequency tiles of $\Delta t \Delta f = 1$]{\includegraphics[height=3.4cm]{./images-tftheory/gabor3.png}}\\
	\subfloat[High frequency resolution vs. high time resolution]{\includegraphics[height=3.2cm]{./images-tftheory/gabor4.png}}
	\caption{Different tiling of the time-frequency plane \parencite[326, 327]{gabordiagrams}}
	\label{fig:gabortf}
\end{figure}

The number of output frequency bins of the STFT is the same as the DFT, since the output of the STFT is a matrix of columns which each column contains the DFT coefficients from one window of the input signal. The rows represent the frequency bins and the columns represent the time windows or frames. Section \ref{sec:freqanal} described that the output coefficients of an N-point DFT for a real-valued input signal $x[n]$ were symmetrically mirrored around the center bin, such that the bins 0--$\sfrac{N}{2}+1$ corresponded to the frequencies 0--$\sfrac{F_{s}}{2}$ Hz, and bins $\sfrac{N}{2}+2$--0 are redundant. The STFT has the same behavior, such that an STFT with a window size of N, which is the same as the N points of the DFT for each window of the signal, outputs $\sfrac{N}{2}+1$ non-redundant frequency bins. For an $N$-point DFT, the frequency spacing between each bin $\mathit{df} = \sfrac{F_{s}}{N}$, which is that of the DFT shown in Section \ref{sec:freqanal}. The frequency in Hz corresponding to each bin is $f \text{ (Hz)} = \text{bin}\times\sfrac{F_{s}}{N}$. The frequency bins 0--$\sfrac{N}{2}+1$ therefore correspond to 0--$\sfrac{F_{s}}{2}$ Hz. Figure \ref{fig:stftdiag} shows the rectangular matrix of coefficients which is the output of the STFT, and the frequency bins and frequencies in Hz corresponding to the rows of the matrix.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-blockdiagrams/stft_diag.png}
	\caption{Rectangular matrix output of the STFT}
	\label{fig:stftdiag}
\end{figure}

A key characteristic of the STFT is its fixed time-frequency resolution across the entire frequency spectrum. To change the time-frequency resolution, the window size must be changed.  Figure \ref{fig:stfts1} shows the STFT with the Hamming window and Figure \ref{fig:stfts2} shows the Gabor transform, which is the STFT with the Gaussian window. Three different window sizes are shown for each: 128 samples, 2,048 samples, and 16,384 samples, which at the sample rate of the glockenspiel signal (44,100 Hz) represent $\sfrac{128}{44,100} =$ 2.9 ms, 46.44 ms, and 371.52 ms respectively. The Hamming window was chosen because it is the default window in the MATLAB \Verb#spectrogram# function.\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} The Gaussian and Hamming windows are shown together in Figure \ref{fig:gaussvshamm}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/hamming_vs_gauss.png}
	\caption{2,048-sample Hamming and Gaussian windows}
	\label{fig:gaussvshamm}
\end{figure}

\newpagefill

\begin{figure}[ht]
	\centering
	\subfloat[STFT, 128-sample Hamming]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_hamm_128.png}}\\
	\subfloat[STFT, 2,048-sample Hamming]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_hamm_2048.png}}\\
	\subfloat[STFT, 16,384-sample Hamming]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_hamm_16384.png}}
	\caption{Magnitude spectrograms with the Hamming window STFT}
	\label{fig:stfts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[Gabor transform, 128-sample Gaussian]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_gauss_128.png}}\\
	\subfloat[Gabor transform, 2,048-sample Gaussian]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_gauss_2048.png}}\\
	\subfloat[Gabor transform, 16,384-sample Gaussian]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_gauss_16384.png}}
	\caption{Magnitude spectrograms with the Gabor transform, i.e., the Gaussian window STFT}
	\label{fig:stfts2}
\end{figure}

\newpagefill

\subsubsection{Constant-Q Transform (CQT)}
\label{sec:cqt}

\citeauthor{cqtransient} stated that the problem with the STFT is its ``rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies'' \parencite[1]{cqtransient}. As described in Section \ref{sec:motivation}, music should be analyzed with a high frequency resolution in the low-frequency region, and with a high time resolution in the high-frequency region \parencite{doerflerphd, cqtransient}. 

The Constant-Q Transform was proposed by \textcite{jbrown, msp} to analyze musical signals with a logarithmic frequency scale to show the relationship between the fundamental frequency of a musical instrument and its harmonics better than the Fourier Transform's uniform frequency spacing. A demonstration of a violin analyzed with the DFT and CQT was shown in Figure \ref{fig:violin}.

The name ``Constant-Q'' refers to the constant ratio of the frequency being analyzed to the frequency resolution of analysis, or $\sfrac{f}{\delta f} = Q$, such that the frequency resolution increases with the center frequency to maintain the $Q$ factor. The transform uses long-duration windows in the low frequency regions and short-duration windows in the high frequency regions, resulting in good time resolution for transients \parencite{cqtransient}. Figure \ref{fig:jbrowncqt} shows some properties of the linearly-spaced DFT spectrum compared to the Constant-Q Transform, and the different window sizes used for each frequency bin of interest. Equations \eqref{equation:jbrowncqt} describe how the different windows $N[k]$ are applied to the signal:
\begin{align}\tag{9}\label{equation:jbrowncqt}
	\nonumber & \text{window length} = N[k] = \frac{f_{s}}{f_{k}}Q\\
	\nonumber & W[k, n] = \alpha + (1 - \alpha)\cos\big(\frac{2\pi n}{N[k]}\big)
\end{align}

\begin{figure}[ht]
	\centering
	\subfloat[Properties of DFT, CQT]{\includegraphics[width=0.65\textwidth]{./images-tftheory/dftvcqt.png}}\\
	\subfloat[Window sizes for computing the CQT]{\includegraphics[width=0.65\textwidth]{./images-tftheory/qwindowchanges.png}}
	\caption{Various aspects of the original CQT \parencite[427, 428]{jbrown}}
	\label{fig:jbrowncqt}
\end{figure}

This first CQT implementation was not designed to be invertible, until \textcite{klapuricqt} introduced an algorithm for an approximate inverse of the CQT back to the time-domain waveform, with an inversion error of $\approx 10^{-3}$. The approximate inverse for the CQT was also approached differently by \textcite{fitzgeraldcqt}.

\citeauthor{klapuricqt} describe the CQT as ``a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal'' \parencite[1]{klapuricqt}. Frequency bins that are geometrically spaced are also logarithmically spaced with a base of two \parencite{geometriclog}. The bins-per-octave setting of the CQT is related to the Constant-Q ratio by the formula $Q = 2^{\sfrac{1}{\text{bins}}}$. For example, for 12 bins-per-octave, this results in a $Q$ factor of 1.059, which is the well-known ``12th root of two'' based on the Western chromatic scale with equal temperament \parencite{westernpitch1, westernpitch2}.

\textcite{invertiblecqt} implemented the Constant-Q Transform with the Nonstationary Gabor Transform (NSGT) \parencite{balazs}, which allowed for perfect reconstruction for the first time. This is also called the CQ-NSGT, or Constant-Q Nonstationary Gabor Transform.

\textcite{slicq} followed up with a realtime algorithm, the \textit{sliCQ} or ``sliced Constant-Q'' transform (sliCQT), which can process the input signal in fixed-size slices, as opposed to operating on the entire input signal like the NSGT. Finally, \textcite{variableq1} introduced the Variable-Q scale and improved the phase of the transform. From \textcite{invertiblecqt}, the total frequency bins of the CQ-NSGT or sliCQT can be derived from the bins-per-octave, and the minimum and maximum frequencies of the desired frequency scale, shown in equation \ref{equation:bpo}:

\begin{align}
	K = [B \log_{2}\big(\frac{\xi_{\text{max}}}{\xi_{\text{min}}}\big) + 1]\tag{10}\label{equation:bpo}
\end{align}

, where $K$ is the total bins of the CQT, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. For example, for $B = 12 \text{ bins-per-octave}$, $\xi_{\text{min}} = 83 \text{ Hz}$ and $\xi_{\text{max}} = 22{,}050 \text{ Hz}$, the result is $K \approx 97 \text{ total frequency bins}$.

In the landscape of music analysis libraries, Essentia,\footnote{\url{https://essentia.upf.edu/}} LTFAT,\footnote{\url{https://ltfat.org/}} and the MATLAB Wavelet Toolbox\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} have converged on the CQ-NSGT \parencite{invertiblecqt, slicq, variableq1}. However, librosa\footnote{\url{https://librosa.org/}} still uses the older implementation with the approximate reconstruction \parencite{klapuricqt}. Finally, \textcite{invertiblecqt} have released an open-source reference Python implementation\footnote{\url{https://github.com/grrrr/nsgt}} of the CQ-NSGT and the sliCQT.

Several examples of magnitude spectrograms of the glockenspiel signal are generated using the STFT, shown in Figure \ref{fig:cqtvsstft1}, and the CQT, shown in Figure \ref{fig:cqtvsstft2}. The spectrograms generated from the standard STFT spectrogram\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} and the CQ-NSGT implementation of the CQT from the MATLAB Wavelet Toolbox. Note how the CQT spectrograms contain more musical information than the STFT spectrograms at all frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[STFT spectrogram, window size 256]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_stft_256.png}}\\
	\subfloat[STFT spectrogram, window size 1,024]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_stft_1024.png}}\\
	\subfloat[STFT spectrogram, window size 4,096]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_stft_4096.png}}
	\caption{STFT magnitude spectrograms of the glockenspiel signal}
	\label{fig:cqtvsstft1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins-per-octave]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_cqt12.png}}\\
	\subfloat[CQT spectrogram, 24 bins-per-octave]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_cqt24.png}}\\
	\subfloat[CQT spectrogram, 48 bins-per-octave]{\includegraphics[width=0.675\textwidth]{./images-gspi/glock_cqt48.png}}
	\caption{CQT magnitude spectrograms of the glockenspiel signal}
	\label{fig:cqtvsstft2}
\end{figure}

\newpagefill

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

\textcite{doerflerphd} in their dissertation analyzed music with multiple Gabor dictionaries (i.e. STFTs). Figure \ref{fig:dorflertradeoff} shows the time-frequency tradeoff of the short and long window STFT analyses of the glockenspiel signal for its transient and tonal characteristics, and how two Gabor dictionaries combined gives us the ability to analyze either the transient or tonal aspects with a more appropriate resolution.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal \parencite[20]{doerflerphd}}
	\label{fig:dorflertradeoff}
\end{figure}

As summarized by \citeauthor{adaptivecqt}, ``[t]he definition of multiple Gabor frames, which is comprehensively treated in [D{\"o}rfler 2002], provides Gabor frames with analysis techniques with multiple resolutions... The nonstationary Gabor frames [...] are a further development; [...] they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas'' \parencite[2]{adaptivecqt}. In other words, a single Nonstationary Gabor transform can replace the need for multiple distinct Gabor transforms (or STFTs).

\citeauthor{dictionary} describe the shift from the term \textit{transform}, e.g., the Gabor transform or STFT, to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began a ``fundamental move from transforms to dictionaries for [...] signal representation'' \parencite[1,049]{dictionary}. Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task'' \parencite[1,049]{dictionary}.

Using multiple transforms, such as using two Gabor transforms (or STFTs) with a small and large window for the transient and tonal properties of music, is also called an \textit{overcomplete} dictionary \parencite{dictionary}, where there is a lot of redundancy in the transform domain.

The advantage of the CQT over such approaches is that it contains these desirable transform properties in one single transform. The NSGT is a time-frequency transform with varying time-frequency resolution, whose motivating application is the CQT \parencite{jaillet, balazs}. It is constructed from frame theory \parencite{frametheory}, a mathematical technique for computing redundant, stable ways of representing a signal \parencite{framesintro}. The NSGT can use nonuniform time and frequency spacing in its time-frequency analysis of a signal, and it has a perfect inverse operation with practically no reconstruction error.

The construction of nonstationary Gabor frames relies on three properties of the windows and time-frequency shift parameters used:
\begin{tight_itemize}
	\item
		``The signal $f$ of interest is localized at time- (or frequency-) positions $n$ by means of multiplication with a compactly supported (or limited bandwidth, respectively) window function $g_{n}$'' \parencite[2]{balazs}
	\item
		``The Fourier Transform is applied on the localized pieces $f \cdot g_{n}$. The resulting spectra are sampled densely enough in order to perfectly re-construct $f \cdot g_{n}$ from these samples'' \parencite[2]{balazs}
	\item
		``Adjacent windows overlap to avoid loss of information. At the same time, unnecessary overlap is undesirable. We assume that $0 < A \le \sum_{n \in \mathbb{Z}}|g_{n}(t)|^{2} \le B < \infty$, a.e. (almost everywhere), for some positive A and B'' \parencite[2]{balazs}
\end{tight_itemize}

These requirements lead to invertibility of the frame operator and therefore to perfect reconstruction. \citeauthor{balazs} continues on to say that

\begin{quote}
	[m]oreover, the frame operator is diagonal and its inversion is straightforward. Further, the canonical dual frame has the same structure as the original one. Because of these pleasant consequences following from the three above-mentioned requirements, the frames satisfying all of them will be called painless nonstationary Gabor frames and we refer to this situation as the painless case \parencite[1,482]{balazs}
\end{quote}

To derive the NSGT, let us recall the previously-seen definition of the Gabor transform from Section \ref{sec:jointtfa}. In the standard Gabor transform, the same window function (also called the Gabor atom or Gabor function) is shifted in time to cover entire signal \parencite{adaptivecqt}, described by equation \eqref{equation:stationarygab}:
\begin{align} \tag{11}\label{equation:stationarygab}
g_{m, n}(t) = g(t - na)e^{2\pi i m b t}
\end{align}

As stated by \citeauthor{adaptivecqt}, ``[w]e will indicate such a frame as \textit{stationary}, since the window used for time-frequency shifts does not change and the time-frequency shifts form a lattice of $a \times b$'' \parencite[3]{adaptivecqt}. Figure \ref{fig:uniformtflattice} shows the resulting uniform $a \times b$ tiling of the time-frequency plane.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-tftheory/stationarygabor.png}
	\caption{Uniform time-frequency resolution of the stationary Gabor transform \parencite[3]{adaptivecqt}}
	\label{fig:uniformtflattice}
\end{figure}

The definition of the multiple Gabor systems of \textcite{doerflerphd} continues from the stationary Gabor transform. Given the stationary Gabor atom, where $a$ and $b$ are the time-frequency shift parameters and $f(t)$ is the reconstructed input signal, the Gabor transform is described by equations \eqref{equation:onegabor}:
\begin{align}
	\nonumber g_{m,n}(t) &= g(t - na)e^{j2\pi m b t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{m,n \in \mathbb{Z}}c_{m,n}g_{m,n}(t) \tag{12}\label{equation:onegabor}
\end{align}

An overcomplete system that uses $R$ distinct STFTs (or Gabor transforms) with different window sizes, where $a_{r}$ and $b_{r}$ are the time-frequency shift parameters for each window size, can be described with equations \eqref{equation:multigabor}:
\begin{align}
	\nonumber g_{m,n}^{r}(t) &= g(t - na_{r})e^{j2\pi m b_{r} t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{r=0}^{R-1}\sum_{m,n \in \mathbb{Z}}c^{r}_{m,n}g^{r}_{m,n}(t) \tag{13}\label{equation:multigabor}
\end{align}

For a resolution that changes with time, the Nonstationary Gabor atom is chosen from a set of functions $\{g_{n}\}$ and a fixed frequency sampling step $b_{n}$, shown in equation \eqref{equation:irregulartime}:
\begin{align}\tag{14}\label{equation:irregulartime}
	g_{m,n}(t) &= g_{n}(t)e^{j2\pi m b_{n}t}, m,n \in \mathbb{Z}
\end{align}

\citeauthor{balazs} describe the system in equation \eqref{equation:irregulartime} further:

\begin{quote}
	[...] the functions $\{g_{n}\}$ are well-localized and centered around time-points $a_{n}$. This is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{n}$ for each position $a_{n}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position \parencite[1,485]{balazs}.
\end{quote}

For a resolution that changes with frequency, the Nonstationary Gabor atom is chosen from a family of functions $\{h_{m}\}$, which are `well-localized band-pass functions with center frequency $b_{n}$'' and a fixed time sampling step $a_{m}$, shown in equation \eqref{equation:irregularfrequency} \parencite[1,486]{balazs}:
\begin{align}\tag{15}\label{equation:irregularfrequency}
	h_{m,n}(t) = h_{m}(t - na_{m}), m,n \in \mathbb{Z}
\end{align}

Figure \ref{fig:nonuniformtflattices} show both the cases of the varying resolution by time and by frequency in terms of sampled points on the time-frequency grid.

\begin{figure}[ht]
	\centering
	\subfloat[Gabor atoms that change with time]{\includegraphics[width=6.25cm]{./images-tftheory/irregulartime.png}}
	\hspace{1em}
	\subfloat[Gabor atoms that change with frequency]{\includegraphics[width=6.25cm]{./images-tftheory/irregularfrequency.png}}
	\caption{Varying time-frequency sampling of the Nonstationary Gabor transform \parencite[1,485, 1,487]{balazs}}
	\label{fig:nonuniformtflattices}
\end{figure}

The NSGT coefficients for a signal of length $L$ are given by an FFT of length $M_{n}$ for each window $g_{n}$. For each window, there are $L$ window operations and $O(M_{n} \cdot \log(M_{n}))$ FFT operations. The overall algorithmic complexity of the NSGT is therefore $O(N \cdot (M \log(M)))$ for $N$ windows.

I believe that the general NSGT is more interesting to study than the CQ-NSGT (Constant-Q NSGT), as it can be constructed with nonuniform frequency scales besides the Constant-Q scale, such as the psychoacoustic mel and Bark scales \parencite{melbook}, or the Variable-Q scale which combines the Constant-Q scale with the psychoacoustic concept of the equivalent rectangular bandwidth (ERB) \parencite{variableq1, variableq2}. In essence the NSGT can be thought of as a filterbank \parencite{variableq1}.

Figures \ref{fig:bunchansgts1} and \ref{fig:bunchansgts2} show different configurations of the NSGT of the glockenspiel signal, to demonstrate the diversity of the transform. A high and low frequency resolution NSGT, using 100 and 500 total frequency bins respectively, is generated for the Constant-Q/logarithmic scale for music, and the mel psychoacoustic scale, using a frequency range of 20--22,050 Hz. Details of the frequency scales used to generate the NSGT spectrograms are shown in Table \ref{table:nsgtfreqsandqs}.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, Constant-Q scale, 100 bins]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_cqlog_100.png}}\\
	\subfloat[NSGT, Constant-Q scale, 500 bins]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_cqlog_500.png}}
	\caption{Constant-Q NSGT spectrograms of the glockenspiel signal}
	\label{fig:bunchansgts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, mel scale, 100 bins]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_mel_100.png}}\\
	\subfloat[NSGT, mel scale, 500 bins]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_mel_500.png}}
	\caption{mel-scale NSGT spectrograms of the glockenspiel signal}
	\label{fig:bunchansgts2}
\end{figure}

\begin{table}[ht]
	\centering
	\caption{Different frequencies and Q factors for various NSGT scales}
	\label{table:nsgtfreqsandqs}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & First five frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 20.00, 21.47, 23.04, 24.73, 26.54 \\
	 \hline
	 mel & 100 & 20.00, 45.56, 72.02, 99.42, 127.80 \\
	 \hline
	 Constant-Q & 500 & 20.00, 20.28, 20.57, 20.86, 21.16 \\
	 \hline
	 mel & 500 & 20.00, 25.00, 30.03, 35.10, 40.21 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & Last five frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 16,614.38, 17,832.63, 19,140.20, 20,543.64, 22,050.00 \\
	 \hline
	 mel & 100 & 19,087.44, 19,789.79, 20,517.07, 21,270.17, 22,050.00 \\
	 \hline
	 Constant-Q & 500 & 20,845.91, 21,140.62, 21,439.50, 21,742.61, 22,050.00 \\
	 \hline
	 mel & 500 & 21,428.92, 21,582.58, 21,737.31, 21,893.11, 22,050.00 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & First five Q-factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 mel & 100 & 0.40, 0.88, 1.34, 1.78, 2.21 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 mel & 500 & 2.01, 2.49, 2.97, 3.45, 3.92 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{10cm}| }
	 \hline
	 Scale & Bins & Last five Q-factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 mel & 100 & 13.83, 13.85, 13.86, 13.88, 13.89 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 mel & 500 & 69.97, 69.98, 70.00, 70.02, 70.03 \\
	 \hline
\end{tabular}
\end{table}

\newpagefill

\subsubsection{sliCQ Transform (sliCQT)}
\label{sec:theoryslicqt}

The NSGT processes the entire input signal at once. In cases where the input signal must be processed in fixed-size chunks, such as realtime streaming, the sliCQ Transform (sliCQT, or sliced Constant-Q Transform) was created \parencite{invertiblecqt, slicq}. The slicing operation is shown in Figure \ref{fig:slicqtukeys}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.55\textwidth]{./images-misc/slicq_windows.png}
	\caption{Slicing the input signal with 50\% overlapping Tukey windows. N is the slice length and M is the transition area \parencite{slicq}}
	\label{fig:slicqtukeys}
\end{figure}

Additionally, the ``slicing windows are symmetrically zero-padded to length 2N , reducing time-aliasing significantly'' \parencite[10]{slicq}, with the effect that adjacent slices need to be 50\% overlap-added with each other to create the final spectrogram. There is no inverse operation for the 50\% overlap-add provided in the paper. Figure \ref{fig:slicqoverlaps} demonstrates this issue.\todo{fix flatten spectrogram}

Note the terminology of the sliCQT, which uses slice length instead of the window length of the STFT, and transition length or transition area instead of the overlap or hop length of the STFT. Slice and transition are used to distinguish the larger slice-wise operation of the sliCQT from the local windowing and overlapping done \textit{within} each slice to achieve the desired time-frequency resolution. The real overlap between slices can vary up to a maximum of the transition area.

\begin{figure}[ht]
	\centering
	\subfloat[Adjacent slices placed side-by-side]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_overlap_flatten.png}}\\
	\subfloat[Adjacent slices with 50\%-overlap-add]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_overlap_proper.png}}
	\caption{sliCQT spectrograms demonstrating the slice half-overlap}
	\label{fig:slicqoverlaps}
\end{figure}

\subsubsection{Ragged time-frequency transforms}
\label{sec:raggedtf}

\citeauthor{klapuricqt} state that the consequence of nonuniform frequency analysis, such as that seen in the CQT or NSGT, leads to
\begin{quote}
	... a data structure that is more difficult to work with than the time-frequency matrix (spectrogram) obtained by using Short-Time Fourier transform in successive time frames. The last problem is due to the fact that in CQT, the time resolution varies for different frequency bins, in effect meaning that the ``sampling'' of different frequency bins is not synchronized \parencite[1]{klapuricqt}.
\end{quote}

This same statement also applies to the NSGT, where different frequency bins have a different time resolution. An illustration is shown in Figure \ref{fig:raggedslicqt}. The shape of the transform can be referred to as an irregular or ragged matrix.\footnote{\url{https://xlinux.nist.gov/dads/HTML/raggedmatrix.html}}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{./images-misc/slicq_shape.png}
	\caption{Illustration of the ragged NSGT, where frequency bins are grouped by their time resolution}
	\label{fig:raggedslicqt}
\end{figure}

Note that to produce a meaningful spectrogram like the NSGT in Figure \ref{fig:raggedslicqt}, the slices returned by the sliCQT must be overlap-added as described in Section \ref{sec:theoryslicqt} and shown in \ref{fig:slicqoverlaps}. The NSGT outputs the meaningful spectrogram directly, without any further modifications needed.

\newpagefill

\subsubsection{Interesting frequency scales for musical applications}
\label{sec:fscales}

Let us consider frequency scales that may be interesting for music or audio analysis, in addition to the Constant-Q/logarithmic and mel scales already defined in the reference NSGT library and shown in the previous section.

The Variable-Q scale \parencite{variableq1, variableq2} is the same as the log or octave scales, except with a small fixed frequency offset, denoted by gamma or $\gamma \text{ (Hz)}$, added to each frequency bin. \citeauthor{variableq1} provide the motivation for the Variable-Q scale:
\begin{quote}
	... [The] CQT has several advantages over STFT when analysing music signals. However, one considerable practical drawback is the fact that the analysis/synthesis atoms get very long towards lower frequencies. This is unreasonable both from a perceptual viewpoint and from a musical viewpoint. Auditory filters in the human auditory system are approximately Constant-Q only for frequencies above 500 Hz and smoothly approach a constant bandwidth towards lower frequencies. Accordingly, music signals generally do not contain closely spaced pitches at low frequencies, thus the Q-factors (relative frequency resolution) can safely be reduced towards lower frequencies, which in turn improves the time resolution \parencite[5]{variableq1}.
\end{quote}

Both \textcite{variableq1} and \textcite{variableq2} cite the ERBlet transform \parencite{erblet} as a psychoacoustic transform which in turn motivates the Variable-Q Transform. \textcite{variableq1, variableq2} provide the following equations \eqref{equation:variablebw} for computing the bandwidth $B_{k}$ of the frequency bin (or filter channel) $k$ where $b$ is the bins-per-octave or bpo:
\begin{align}\tag{25}\label{equation:variablebw}
	\nonumber & B_{k} = \alpha f_{k} + \gamma, \alpha = 2^{\frac{1}{b}} - 2^{\frac{1}{b}}
\end{align}

The effect is to widen the bandwidths of the windows at the low frequency bins significantly, since the offset is comparable in its order of magnitude to the lower frequency bandwidths. \textcite{variableq1} show some examples with $\gamma = [0, 3, 6.6, 10, 30] \text{ Hz}$. As the center frequency increases, the effect of the small offset becomes negligible. This results in a widening of the Q-factors in the low frequency bins, but it becomes Constant-Q in the high frequency bins. 

The Bark psychoacoustic scale to complement the included mel scale, because the Bark scale has been used with success in music source separation \parencite{barkjust1} and in percussion instrument classification \parencite{barkjust2}. In this thesis, I use the formula shown in equation \eqref{equation:barktan} to convert between Bark and Hz frequencies \parencite{barktan}:
\begin{align}\tag{26}\label{equation:barktan}
	\nonumber & f_{\text{Bark}} = 6 \cdot arcsinh \Big(\frac{f_{\text{Hz}}}{600}\Big), f_{\text{Hz}} = 600 \cdot sinh \Big(\frac{f_{\text{Bark}}}{6}\Big)
\end{align}

\newpagefill

\subsection{Machine learning and deep learning for music signals}
\label{sec:ml}

Machine learning (ML) is a technique for modeling complex, unknown systems from data \parencite{introtoml}:

\begin{quote}
	In many scientific disciplines, the primary objective is to model the relationship between a set of observable quantities (inputs) and another set of variables that are related to these (outputs). [...] Machine learning provides techniques that can automatically build a computational model of these complex relationships by processing the available data and maximizing a problem dependent performance criterion \parencite[105]{introtoml}.
\end{quote}

According to \citeauthor{introtodl}, deep learning (DL) ``is the subfield of machine learning that is devoted to building algorithms that explain and learn a high and low level of abstractions of data that traditional machine learning algorithms often cannot'' \parencite[1]{introtodl}. A characteristic of deep networks is that they have many hidden layers, named so ``because we do not necessarily see what the inputs and outputs of these neurons are explicitly beyond knowing they are the output of the preceding layer,'' \parencite[2]{introtodl} to model more complex relationships between the input and output. Figure \ref{fig:fcdn} shows a deep network with hidden layers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65\textwidth]{./images-neural/dnn.png}
	\caption{Deep neural network with hidden layers \parencite[2]{introtodl}}
	\label{fig:fcdn}
\end{figure}

\citeauthor{introtodl} states that ``most machine learning algorithms as they exist now focus on function optimization, and the solutions yielded do not always explain the underlying trends within the data nor give the inferential power that artificial intelligence was trying to get close to'' \parencite[1]{introtodl}. However, machine learning methods have achieved success in many fields including natural language processing \parencite{nlpml}, computer vision \parencite{cvml}, and audio \parencite{audiodeeplearning}, indicating that data-driven approaches are useful, despite falling short of general artificial intelligence \parencite{generalai}.

Optimization techniques have been used in statistical, approximate, or stochastic signal processing \parencite{stochasticsp, statisticalsp} for decades \parencite{optsp}. Some of the classic examples include basis pursuit and matching pursuit \parencite{dictionary1, dictionary2}. Signal processing techniques and machine and deep learning are compatible with one another and can be used together \parencite{mlsp1, mlsp2}. According to \citeauthor{mldspmix}, signal processing and machine learning should both be considered in the study of acoustic signals, shown in Figure \ref{fig:dspmlmix}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65\textwidth]{./images-neural/dspmlmix.jpg}
	\caption{``Whereas physical models are reliant on rules, which are updated by physical evidence (data), machine learning (ML) is purely data-driven. By augmenting ML methods with physical models to obtain hybrid models, a synergy of the strengths of physical intuition and data-driven insights can be obtained \parencite[3,591]{mldspmix}''}
	\label{fig:dspmlmix}
\end{figure}

In machine learning, the input data is typically split into a training set and a test set \parencite{introtoml}. The model makes predictions with its initial parameters using the training input. It measures how correct its prediction was by using a loss function to compare its predicted output to the training (or ground truth) output. Contemporary machine learning is dependent on the field of optimization \parencite{boyd2004convex, mlopt1, mlopt2} for the loss function and parameter updates \parencite{sgd}. The performance of the network is measured by its ability to generalize on the test set, which was data not seen during training. There is sometimes a third set called the validation set, which is also unseen during training and typically used to perform hyperparameter tuning \parencite{splitvaliddata}. Hyperparameters are the parameters of a machine learning model that are defined by the user \parencite{introtodl}. The user tries to maximize the performance of the model on the validation set by modifying the hyperparameters.

Music signals are a form of acoustic signal, and the domain of music signal processing is inseparable from the larger field of signal processing \parencite{musicsp}. It follows that ML and DL techniques can be extended to music signal processing applications. \textcite{audiodeeplearning} describe contemporary deep learning architectures that are used for audio applications. They state that models from the field of computer vision, originally designed for 2D images where pixels are related spatially, may not exactly fit audio waveforms where amplitude values are related temporally. However, convolutional neural networks, recurrent neural networks, and sequence-to-sequence models are three popular types of network architecture that have been adapted with varying levels of success to the audio and music domains \parencite{audiodeeplearning}.

\subsubsection{Convolutional neural networks (CNN)}
\label{sec:cnn}

Convolutional neural networks (CNN) ``are based on convolving their input with learnable kernels'' \parencite[3]{audiodeeplearning}. These are useful for both images and audio according because of their ability to exploit spatial or temporal correlation in data \parencite{cnns}. Figure \ref{fig:cnnbasic} shows how a learnable convolution kernel slides over the pixels of the input image.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\subfloat{\includegraphics[width=0.575\textwidth]{./images-neural/sliding_conv_1.png}}\\
		\subfloat{\includegraphics[width=0.575\textwidth]{./images-neural/sliding_conv_2.png}}
		\caption[$3 \times 3$ convolution kernel sliding over patches of input pixels]{$3 \times 3$ convolution kernel sliding over patches of input pixels\footnote[1]{\url{https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks}}}
		\label{fig:cnnbasic}
	\end{minipage}
\end{figure}

Figure \ref{fig:convtranspose} shows how a convolutional and transpose convolutional or deconvolutional layer are the inverse operation of each other.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-neural/convtranspose.png}
	\caption{Deconvolution operation \parencite[11]{convtranspose}}
	\label{fig:convtranspose}
\end{figure}

The network architecture and feature map for an audio CNN is shown in Figure \ref{fig:audiocnn}, where the extracted feature map is visualized in the spectral domain.

\begin{figure}[ht]
	\centering
	\subfloat[CNN architecture for audio waveforms {\parencite[7]{audiocnn2}}]{\includegraphics[width=0.9\textwidth]{./images-neural/audio_cnn.png}}\\
	\subfloat[Audio spectral features learned by each layer {\parencite[4]{audiocnn3}}]{\includegraphics[width=0.9\textwidth]{./images-neural/audio_cnn2.png}}
	\caption{Example of an audio CNN architecture and spectral feature maps}
	\label{fig:audiocnn}
\end{figure}

For audio applications, both 1D convolutions (also referred to as temporal convolutions) applied directly to the audio waveform and 2D convolutions applied to time-frequency transforms (most commonly the STFT) are both used \parencite{tcn, 2dconv}. To describe convolutional layers, \citeauthor{convguide} state that ``images, sound clips and many other similar kinds of data have an intrinsic structure,'' and share the following properties \parencite[6]{convguide}:

\begin{tight_enumerate}
	\item
		They are stored as multi-dimensional arrays (e.g. the STFT or sliCQT)
	\item
		They feature one or more axes for which ordering matters (e.g., time and frequency for a spectrogram)
	\item
		The channel axis is used to access different views of the data (e.g., the left and right channels of a stereo audio track)
\end{tight_enumerate}

The STFT and sliCQT of stereo (2-channel) music produce a 2-channel spectrogram, such that the total dimensions of the 2D time-frequency spectrogram are $\text{time} \times \text{frequency} \times \text{channel}$. The 2D convolution kernels slide across the spectrograms to learn a feature representation, and the number of output channels determines how many feature maps are created. 

The kernel parameters define the size and movement of the 2D convolution kernel in the time and frequency dimensions in each output channel. Besides the kernel size, the kernel has these additional parameters: stride, dilation, and padding \parencite{convguide}. Figure \ref{fig:convdiags} shows the behavior of these different kernel parameters. The stride is the amount by which the kernel moves; a stride larger than one implies \textit{subsampling}, because it dictates how much of the output is retained \parencite{convguide}. The dilation defines ``holes'' or gaps in the kernel to increase the receptive field \parencite{convguide}, which is a computationally cheap way to increase the amount of data points considered in a feature map. The padding defines zeros concatenated to the beginning and end of an axis.

\begin{figure}[ht]
	\centering
	\subfloat[2D convolution kernel]{\includegraphics[width=\textwidth]{./images-neural/conv1.png}}\\
	\subfloat[2D convolution kernel with padding > 0]{\includegraphics[width=\textwidth]{./images-neural/conv2.png}}\\
	\subfloat[2D convolution kernel with stride > 1]{\includegraphics[width=\textwidth]{./images-neural/conv3.png}}\\
	\subfloat[2D convolution kernel with dilation > 1]{\includegraphics[width=\textwidth]{./images-neural/conv4.png}}
	\caption{Different behaviors of kernel parameters \parencite[14, 29]{convguide}}
	\label{fig:convdiags}
\end{figure}

\subsubsection{Recurrent neural networks (RNN)}

For time series data, for which the audio waveform is an exemplar given the temporal evolution of its amplitude, recurrent neural networks (RNNs) are useful since the input sequence of data is introduced back into the network with a cyclic or recurrent connection, and outputs are predicted based on past (or future, if the network is bi-directional) values of the sequence \parencite{rnns}. RNNs are also used commonly in sequence-to-sequence, or seq2seq, models \parencite{seq2seqs}.

In the audio noise suppression model RNNoise,\footnote{\url{https://jmvalin.ca/demo/rnnoise/}} the usefulness of the RNN and its Long Short-Term Memory (LSTM) \parencite{lstm1} and Gated Recurrent Unit (GRU) \parencite{gru1} variants for audio are described:

\begin{quote}
	Recurrent neural networks (RNN) are very important [...] because they make it possible to model time sequences instead of just considering input and output frames independently. [...] RNNs were heavily limited in their ability because they could not hold information for a long period of time [...] [These] problems were solved by the invention of gated units, such as the Long Short-Term Memory (LSTM), the Gated Recurrent Unit (GRU), and their many variants.
\end{quote}

Figure \ref{fig:rnndiags} shows some example RNN architectures, as well as an illustration of the additional complexity in the gated LSTM and GRU units, which allow them to surpass the simple RNN.

\begin{figure}[ht]
	\centering
	\subfloat[Regular RNN with cyclic connections to past data {\parencite[3]{birnn}}]{\includegraphics[width=0.575\textwidth]{./images-neural/simple_rnn.png}}\\
	\subfloat[Bi-directional LSTM with cyclic connections to past and future data {\parencite[3]{birnn}}]{\includegraphics[width=0.625\textwidth]{./images-neural/birnn.png}}\\
	\subfloat[Different recurrent units {\parencite[7]{lstmrnngru}}]{\includegraphics[width=\textwidth]{./images-neural/gates.png}}
	\caption{RNN diagrams}
	\label{fig:rnndiags}
\end{figure}

\newpagefill

\subsection{Software and code concepts}
\label{sec:softcode}

\subsubsection{Python programming language}
\label{sec:pythonbasics}

Python\footnote{\url{https://www.python.org/}} is a general-purpose programming language. It is an interpreted language,\footnote{\url{https://www.python.org/doc/essays/blurb/}} which means that there is no compilation step required, and the Python code or script written by a user can be executed right away with the Python interpreter. The Python interpreter can also run statements directly for quick prototyping, without needing to write a script:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
Python 3.9.6 (default, Jul 16 2021, 00:00:00)
[GCC 11.1.1 20210531 (Red Hat 11.1.1-3)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print('this is python')
this is python
\end{BVerbatim}
\end{listing}

The Python interpreter is widely distributed on most modern operating systems (OS) like Linux, Windows, and OS-X. Python has support for various high-level and user-friendly data structures. Python has seen widespread adoption in academia, and especially in numerical contexts \parencite{pythonscience}.

One of the core features of the Python language is the list.\footnote{\url{https://docs.python.org/3/tutorial/data structures.html\#more-on-lists}} The list in Python corresponds to the array data structure in computer science \parencite{skiena}. It allows for the construction of an ordered list of objects. For example, a discrete-time speech signal may be described by a list of its amplitude values:

\hfill \Verb#speech_signal = [0.15, 0.28, 0.57, 0.98, -0.59]#

\subsubsection{Numerical and machine learning libraries for Python}

Python has a rich ecosystem of academic libraries. NumPy\footnote{\url{https://numpy.org/}} and SciPy\footnote{\url{https://scipy.org/}} are used for numerical computation and parallelized matrix operations that run on the CPU (central processing unit). Matplotlib\footnote{\url{https://matplotlib.org/}} is a plotting library. Tensorflow\footnote{\url{https://www.tensorflow.org/}} and PyTorch\footnote{\url{https://pytorch.org/}} are libraries for machine learning and deep learning, which also have numerical computation and parallelized matrix operations that are similar to NumPy and SciPy, except that they can run on the GPU (graphical processing unit). The use of the GPU allows for more efficient, faster, and larger parallelized matrix operations, which are essential for modern machine learning and deep learning techniques.

In NumPy, the ndarray\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html}} is the core data structure representing an n-dimensional array of objects, which can be numerical (such as int16 or float32) or general objects (such as strings). The same speech signal represented by the Python list above can be represented by a 1-dimensional ndarray of 64-bit floating-point values:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
>>> import numpy
>>> speech_signal = numpy.asarray([0.15, 0.28, 0.57, 0.98, -0.59])
>>> print(speech_signal.dtype)
float64
>>> print(speech_signal.shape)
(5,)
\end{BVerbatim}
\end{listing}

The underlying values are stored in formats that allow for efficient numerical computations, which is why using NumPy ndarrays for numerical computation is preferred to using regular Python data structures such as lists that are not designed for speed \parencite{ndarrayfast}.

In PyTorch and Tensorflow (and in general machine learning), a similar concept to the ndarray is the tensor. The tensor originates from the field of physics \parencite{whatistensor}. The tensor is also an n-dimensional numerical array, and NumPy ndarrays are interchangeable with both PyTorch and Tensorflow tensors.\footnote{\url{https://www.tensorflow.org/guide/tf_numpy}, \url{https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html}}

Tensors can be used for numerical computations using the GPU in both PyTorch and TensorFlow, much like the ndarray with NumPy and SciPy on the CPU. For example, \Verb#torch.cos()# computes the cosine of a tensor. Additionally and most importantly, the inputs and outputs to the machine learning and deep learning constructs supported by these libraries are represented as tensors.

\subsubsection{Package managers for Python}

To manage third-party libraries and packages for Python, a package manager is suggested. The built-in way of managing a sandboxed Python environment is the virtual environment.\footnote{\url{https://docs.python.org/3/tutorial/venv.html}} This creates an isolated copy of Python where the user can install packages without risking the stability of the operating system (OS). The Python dependency manager tool, pip,\footnote{\url{https://pip.pypa.io/en/stable/cli/pip_install/\#requirements-file-format}} can be used to install a list of third-party packages. Packages are published to the public Python Package Index (PyPi) by the authors.\footnote{\url{https://pypi.org/}} An example of a pip file, conventionally called \Verb#requirements.txt#, is shown in Code Listing \ref{lst:reqtxt}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
mir_eval==0.6
tabulate==0.8.7
numpy==1.19.4
\end{BVerbatim}
	\caption{Example pip requirements.txt file}
	\label{lst:reqtxt}
\end{listing}

There is another popular Python dependency manager called Conda.\footnote{\url{https://docs.conda.io/en/latest/}} Conda environments are also popular for creating reproducible Python environments for academic software. Conda supports channels for third-party package authors to publish and distribute their packages to users, similar to PyPi.\footnote{\url{https://conda.io/projects/conda/en/latest/user-guide/concepts/channels.html}} An example Conda environment file is shown in Code Listing \ref{lst:condayml}. Conda files can also support the pip syntax, since pip is a native Python tool. The user can combine the best of both worlds by using Conda and pip syntax simultaneously in their Conda environment file.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
name: test-conda-env

channels:
  - default

dependencies:
  - python=3.9
  - cudatoolkit=11
  - pip
  - pip:
    - norbert>=0.2.0
\end{BVerbatim}
	\caption{Example Conda environment.yml file}
	\label{lst:condayml}
\end{listing}

Pip is installed by default with Python, but Conda needs to be installed separately. For the code in this thesis, both pip and Conda files are used and provided to help the readers replicate the necessary Python environments.

\subsubsection{Version control systems and git}

Version control systems (VCS) are systems used for tracking and managing changes to code files, and storing the history of changes in a database \parencite{gitbook}. \citeauthor{gitbook} state that the benefits of using a VCS is that it ``allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more. Using a VCS also generally means that if you screw things up or lose files, you can easily recover'' \parencite[1]{gitbook}.

According to \textcite{gitbook}, version control systems are either centralized or distributed. In a distributed version control systems (DVCS), each person's copy of the code contains the full history of changes. This means that people can work on their own private copy of the code, and then sync changes with a central server, rather than depending on the central server for viewing the history like in a centralized version control system (CVCS). This allows people to work independently of the central server, until they are ready to sync changes back with the other collaborators.

Git is a popular tool for DVCS written by Linus Torvalds, who is also known for having created Linux. How Git works is that it ``thinks of its data more like a series of snapshots of a miniature filesystem... every time you commit, or save the state of your project, Git basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot'' \parencite[6]{gitbook}. To work on a code project that uses git, first the project, or \textit{repository}, must be cloned to your local system. You make local changes and then commit them. Finally, you can push your commits back to the central server to share with the other collaborators, often with a descriptive message describing what you changed. Figure \ref{fig:git} shows how git tracks a file and how a git workflow should look.

\begin{figure}[ht]
	\centering
	\subfloat[How git stores snapshots of files]{\includegraphics[width=0.8\textwidth]{./images-misc/git.png}}\\
	\subfloat[Recommended git workflow]{\includegraphics[width=0.6\textwidth]{./images-misc/git2.png}}
	\caption{How git works \parencite[6, 8]{gitbook}}
	\label{fig:git}
\end{figure}

All of the code for this thesis were stored in git repositories to keep track of historical changes. It conveniently allows the author to reference the git history to describe the evolution of the project and the incremental progress.

\subsubsection{Open-source software and GitHub}

\citeauthor{floss} state that ``free and open source software (FOSS) is any computer program released under a licence that grants users rights to run the program for any purpose, to study it, to modify it, and to redistribute it in original or modified form'' \parencite[1]{floss}. Free and open-source software is a merging of two distinct ideas: free software\footnote{\url{https://www.fsf.org/}} and open-source software.\footnote{\url{https://opensource.org/}} While these have nuanced differences when it comes to commercial licensing, code visibility, and availability, in simplistic terms FOSS implies code that is free and open for users to read and modify. Releasing code as open-source is becoming more popular in academia, with initiatives like Papers With Code\footnote{\url{https://paperswithcode.com/}} and the Journal of Open Source Software.\footnote{\url{https://joss.theoj.org/}} According to \textcite{floss}, FOSS software is a natural complement to academic publications, given concerns with reproducibility of results.

\citeauthor{gitbook} describe GitHub\footnote{\url{https://github.com/}} as ``the single largest host for Git repositories, and [... a] central point of collaboration for millions of developers and projects'' \parencite[131]{gitbook}. GitHub contains tools for working with git repositories, including social elements for personal profiles or organizations. Open-source projects can be made public on GitHub such that anybody can read and download the source code. For a project hosted on GitHub, there is a file browser built into the website which allows one to view the code and change history of a project. In this thesis, the GitHub code browser will be linked wherever appropriate. An example of the GitHub code browser showing a file from the PyTorch project's git repository\footnote{\url{https://github.com/pytorch/pytorch}} is shown in Figure \ref{fig:githubpytorch}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-misc/github.png}
	\caption{GitHub's code browser showing a file from the PyTorch source code}
	\label{fig:githubpytorch}
\end{figure}

\newpagefill

\subsection{Music source separation}
\label{sec:musicsep}

\subsubsection{Task motivation and definition}

Typical music recordings are mono or stereo mixtures, with multiple sound objects (drums, vocals, etc.) sharing the same track \parencite{musicsepintro1}. To manipulate the individual sound objects, the stereo audio mixture needs to be separated into a track for each different sound source, in a process called audio source separation.

There are many motivations for performing this separation. \citeauthor{musicsepgood} describe that ``we can remix the balance within the music [...] to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1-channel surround sound system... We might also want to change the spatial location of a musical instrument within the mix'' \parencite[31]{musicsepgood}.

The ISMIR 2021 Music Demixing Challenge\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} (MDX) \parencite{mdx21} describes audio source separation as consisting of different subtasks, including ``music source separation systems [that] take a song as input and output one track for each of the instruments,'' and ``speech enhancement systems [that] take noisy speech as input and separate the speech content from the noise.'' The competition description mentions important uses for music demixing systems, ranging from entertainment to hearing aids:

\begin{quote}
	For example, the original master of old movies contains all the material (dialogue, music and sound effects) mixed in mono or stereo: thanks to source separation we can retrieve the individual components and allow for up-mixing to surround systems. [...] Karaoke systems can benefit from the audio source separation technology as users can sing over any original song, where the vocals have been suppressed, instead of picking from a set of ``cover'' songs specifically produced for karaoke.
\end{quote}

A common thread in survey papers is that the domains of speech enhancement and music demixing are both mentioned as important subproblems of audio source separation of \parencite{musicsepintro1, musicmask}. Music demixing is the focus of this thesis, shown in Figure \ref{fig:mixingdiagrams}.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\subfloat{\includegraphics[height=4cm]{./images-mss/mixdemix.png}}
		\caption[Music mixing and demixing block diagrams]{Music mixing and demixing block diagrams\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}}}
		\label{fig:mixingdiagrams}
	\end{minipage}
\end{figure}

\subsubsection{Computational approaches}

Computational source separation has a history of at least 50 years \parencite{musicmask, musicsepintro1}, originating from the tasks of computational auditory scene analysis (CASA) and blind source separation (BSS). In CASA and BSS, the mixed audio contains unknown sources that must be separated. In music demixing, the sources are typically known; most commonly, four sources (vocals, drums, bass, other) are used as defined by the MUSDB18 dataset \parencite{musdb18}.

Among the earliest computational approaches for blind source separation is Independent Component Analysis (ICA)  \parencite{musicmask, musicsepgood, musicsepintro1}, which exploits spatial information of the sources, and assumes the sources to be independent. This technique can be used when there are as many channels in the mixture (corresponding to differently placed microphones) as the number of sources. \textcite{ica1, ica2} describe ICA algorithms, and \textcite{blind1, blind2} provide an in-depth review on the history of blind source separation. Figure \ref{fig:icaposition} shows an example of the positional considerations in a typical ICA system.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/positional.png}
	\caption{Position-based source separation \parencite[35]{musicsepgood}}
\label{fig:icaposition}
\end{figure}

According to \citeauthor{musicsepintro1}, ICA techniques arose more typically for speech denoising \parencite{speechsep}, and they make assumptions that cannot be generalized easily to music:

\begin{quote}
	[systems which aim] to recover clean speech from noisy recordings [...] can be seen as a particular instance of source separation. [...] many algorithms assume the audio background can be modeled as stationary. However, the musical sources are characterized by a very rich, nonstationary spectrotemporal structure. This prohibits the use of such methods. Musical sounds often exhibit highly synchronous evolution over both time and frequency, making overlap in both time and frequency very common. Furthermore, a typical commercial music mixture violates all the classical assumptions of ICA. Instruments are correlated (e.g., a chorus of singers), there are more instruments than channels in the mixture, and there are nonlinearities in the mixing process (e.g., dynamic range compression) \parencite[1]{musicsepintro1}.
\end{quote}

Techniques more specific to music were developed as a necessity to deal with these differences \parencite{musicseptechniques1, musicseptechniques2}. For cases where ICA cannot be applied, musical source models are more popular, which are ``model-based approaches that attempt to capture the spectral characteristics of the target source can be used'' \parencite[36]{musicsepgood}. Sources are assumed to be sufficiently different from each other, or sparse in their spectral representation \parencite{musicsepgood}, such that they can be extracted with time-frequency masks applied in the spectral domain. Figure \ref{fig:sepgood} shows how different sources have unique spectral patterns.

\begin{figure}[ht]
	\centering
	\subfloat[Mixed spectrogram]{\includegraphics[width=0.675\textwidth]{./images-mss/mss1.png}}\\
	\subfloat[Source spectrograms]{\includegraphics[width=0.675\textwidth]{./images-mss/mss2.png}}
	\caption{Sparsity of music sources in the spectral domain \parencite[32]{musicsepgood}}
\label{fig:sepgood}
\end{figure}

Kernel Additive Modeling (KAM) is the simplest form of music source modeling \parencite{musicsepgood}. To estimate a music source at a given time-frequency point, KAM ``select[s] a set of time-frequency bins, which, given the nature of the target source, e.g., percussive, harmonic or vocals, are likely to be similar in value. This set of time-frequency bins is termed a proximity kernel'' \parencite[36]{musicsepgood}. A well-known example of a KAM-based music separation algorithm is the median-filtering Harmonic/percussive Source Separation (HPSS) algorithm \parencite{fitzgerald1}.

Spectrogram factorization models are more sophisticated than KAM, and the most popular spectrogram factorization model is Nonnegative Matrix Factorization (NMF) \parencite{musicmask, musicsepgood}. According to \citeauthor{musicsepgood}, NMF ``attempts to factorize a given nonnegative matrix into two nonnegative matrices,'' and it can be applied to the magnitude spectrogram of the mix, $M$, to separate it into frequency weight matrices $W$ and time activation matrices $H$ \parencite[37]{musicsepgood}. A survey on NMF techniques by \textcite{nmfpaper} covers the algorithm in more detail. Most recently, data-driven approaches based on machine learning and deep learning have significantly surpassed past approaches \parencite{musicsepgood, sisec2018}. Figure \ref{fig:spectraldemix} shows different techniques for spectral demixing.

\begin{figure}[ht]
	\centering
	\subfloat[KAM vs. NMF for spectral music demixing]{\includegraphics[width=0.9\textwidth]{./images-mss/kamvnmf.png}}\\
	\subfloat[Deep neural networks for spectral music demixing]{\includegraphics[width=0.9\textwidth]{./images-mss/mssdnn.png}}
	\caption{Techniques for spectral music demixing \parencite[36, 38]{musicsepgood}}
	\label{fig:spectraldemix}
\end{figure}

\subsubsection{Harmonic/percussive source separation}
\label{sec:hpss}

A simple case of music source separation is harmonic/percussive source separation \parencite{musicsepgood}. Harmonic (or steady-state, or tonal) sounds are narrowband and steady in time, while percussive (or transient) sounds are broadband and have a fast decay. \textcite{fitzgerald1} noted that they appear as horizontal and vertical lines respectively in the STFT, and applied a median filter in the vertical and horizontal directions to estimate the harmonic and percussive components. This is a form of KAM (kernel additive modelling) algorithm.

Median filtering is a technique for image processing where a pixel is replaced by the median value of its neighbors in a window, or filter, which slides across all pixels. Applying a median filter shaped like a horizontal rectangle (i.e., stretching in time) to the STFT causes vertical (or percussive) features to be diminished since their neighboring pixels are empty, which preserves horizontal (or harmonic) features. Similarly, applying a median filter shaped like a vertical rectangle (i.e., stretching in frequency) causes horizontal (or harmonic) features to be diminished, which preserves vertical (or percussive) features. From these estimates, soft masks are computed which are applied to the original STFT and inverted to create the estimated harmonic and percussive signals. \textcite{driedger} replaced the soft mask with a binary/hard mask, which allows for estimation of a third component, the residual, which is neither harmonic nor percussive, described in further detail in Section \ref{sec:masksandoracles}.

The entire process of median-filtering harmonic/percussive source separation with binary masks applied to the spectrogram of the glockenspiel signal is shown in Figure \ref{fig:fitz12}.

\begin{figure}[ht]
	\centering
	\subfloat[Median filters applied to the STFT spectrogram]{{\includegraphics[width=0.5\textwidth]{./images-hpss/mix_stft_medianfilters.png}} }
	\subfloat[Percussive estimate]{{\includegraphics[width=0.5\textwidth]{./images-hpss/perc_stft.png} }}\\
	\subfloat[Harmonic estimate]{{\includegraphics[width=0.5\textwidth]{./images-hpss/harm_stft.png} }}
	\subfloat[Residual estimate]{{\includegraphics[width=0.5\textwidth]{./images-hpss/resi_stft.png} }}
	\caption{Outputs median filtering HPSS with binary masking applied to the glockenspiel signal}
	\label{fig:fitz12}
\end{figure}

\textcite{driedger} also introduced a two-pass variant. The first pass separates the harmonic component using an STFT with a large window size for high frequency resolution, followed by the second pass which separates the percussive component using an STFT with a small window size for high time resolution. \textcite{fitzgerald2} created a similar iterative variant using soft masks. Additionally, they noted that when they replaced the small-window STFT with the CQT in the second pass, they obtained a separation of the human singing voice.

\subsubsection{Tonal/transient separation}
\label{sec:custgabor}

The two-pass median filtering algorithms shown previously used a short-window STFT for a high time resolution percussive estimate, and a long-window STFT for a high frequency resolution harmonic estimate. The Gabor transform, which as described was the first published example of an STFT-like operation, eventually evolved into a system for time-frequency transforms, called Gabor expansions \parencite{gaborexpansion}. Alternate terms for harmonic and percussive are tonal and transient, and I will show two algorithms for tonal/transient source separation, which both follow the same idea of using two different time-frequency resolutions with custom Gabor expansions.

The Time-Frequency Jigsaw Puzzle (TFJigsaw) is an algorithm for tonal/transient separation with custom Gabor expansions \parencite[6--7]{tfjigsaw}:
\begin{quote}
	[...] the starting point is to define the tonal layer of the signal as the ``component'' which admits a sparse expansion with respect to a Gabor frame with high frequency resolution (i.e. with a wide window), and the transient layer as the ``component'' which admits a sparse expansion with respect to a Gabor frame with high time resolution (i.e. a narrow window).
\end{quote}

\begin{figure}[ht]
	\centering
	\subfloat[Two Gabor transforms, $G^{0}$ and $G^{1}$, with high time resolution and high frequency resolution respectively]{\includegraphics[width=0.55\textwidth]{./images-wavelets/tfjigsaw-supertiles.png}}\\
	\subfloat[$\mathcal{H}$ calculates the entropy of the $G^{0}$ and $G^{1}$ transforms of the input signal $x$ and decides which coefficients to subtract iteratively to create transient and tonal layers $\tilde{G}^{0}$ and $\tilde{G}^{1}$]{\includegraphics[width=0.55\textwidth]{./images-wavelets/tfjigsaw-entropycriterion.png}}
	\caption{Illustrations of the Time-Frequency Jigsaw Puzzle \parencite[3, 4]{tfjigsaw}}
	\label{fig:supertiles}
\end{figure}

After creating two representations of the signal, these are superimposed in a single time-frequency plane to create so-called ``super-tiles.'' Next, within each super-tile, a R{\'e}nyi entropy criterion selects coefficients which have more entropy than random white noise, and subtracts these from the original signal. This is performed iteratively until the tonal and transient layers emerge. The super-tiles and entropy criterion are shown in Figure \ref{fig:supertiles}.

The next tonal/transient separation algorithm considered is the WMDCTLasso algorithm \parencite{wmdct}. The Gabor expansion used is the WMDCT (Windowed Modified Discrete Cosine Transform) applied with two different time-frequency resolutions by using a wide and narrow window. The WMDCT is a time-frequency transform where the signal is decomposed into sum of real cosines instead of the complex exponentials of the DFT \parencite{mdct}.

Next, a method called Group-LASSO (Least Absolute Shrinkage and Selection Operator) is applied to separate the tonal and transient layers. In the wide-window MDCT analysis, the Group-LASSO shrinkage in the frequency group selects the most sparse coefficients in frequency, representing tonal parts of the signal. In the narrow-window MDCT analysis,  Group-LASSO shrinkage in the time group selects the most sparse coefficients in time, representing transient parts of the signal.

Examples of TFJigsaw and WMDCTLasso tonal/transient separation applied to the glockenspiel signal are shown in Figures \ref{fig:tfjigsawtonaltranssep} and \ref{fig:wmdcttonaltranssep}.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[width=0.8\textwidth,trim=0 0 1690 0, clip]{./images-wavelets/tfjigsaw-sep-example.png}}\\
	\subfloat{\includegraphics[width=0.8\textwidth,trim=1690 0 0 0, clip]{./images-wavelets/tfjigsaw-sep-example.png}}
	\caption{TFJigsaw tonal/transient separation}
	\label{fig:tfjigsawtonaltranssep}
\end{figure}

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\centering
		\subfloat{\includegraphics[width=0.575\textwidth]{./images-wavelets/Glock1.jpg}}\\
		\subfloat{\includegraphics[width=0.575\textwidth]{./images-wavelets/Glock_Ton.jpg}}\\
		\subfloat{\includegraphics[width=0.575\textwidth]{./images-wavelets/Glock_Trans.jpg}}
		\caption[WMDCTLasso tonal/transient separation]{WMDCTLasso tonal/transient separation\footnote[1]{\url{https://homepage.univie.ac.at/monika.doerfler/StrucAudio.html}}}
		\label{fig:wmdcttonaltranssep}
	\end{minipage}
\end{figure}

\newpagefill

\subsubsection{Wavelets}
\label{sec:wavelets}

The wavelet transform was designed to overcome the limitations of the fixed time-frequency resolution of the STFT \parencite{wavelets1, wavelets2}. Similar to the CQT, the wavelet transform uses short windows in the high-frequency regions and long windows in the low-frequency regions, making it more suitable for musical and auditory applications.

Wavelet transforms relate to the Fourier transform as follows. The Fourier transform, described previously in Section \ref{sec:freqanal}, represents a signal as a sum of sinusoids which are infinite in duration. The infinite duration is in fact what leads to the time-frequency uncertainty principle, since it transforms a time-domain waveform into the mutually exclusive frequency domain without any temporal information. The STFT and Gabor transform can be considered as a form of wavelet analysis \parencite{gaborwavelet1, gaborwavelet2}.

Wavelet transforms, instead of using infinite sinusoids, represent signals as a sum of \textit{wavelets}, which are finite in duration:

\begin{quote}
	Many signals and images of interest exhibit piecewise smooth behavior punctuated by transients. Speech signals are characterized by short bursts encoding consonants followed by steady-state oscillations indicative of vowels[...] Unlike the Fourier basis, wavelet bases are adept at sparsely representing piecewise regular signals and images, which include transient behavior. Compare wavelets with sine waves, which are the basis of Fourier analysis. Sinusoids do not have limited duration -- they extend from minus to plus infinity. While sinusoids are smooth and predictable, wavelets tend to be irregular and asymmetric.\footnote{\url{https://www.mathworks.com/help/wavelet/gs/what-is-a-wavelet.html}}
\end{quote}

The wavelet and infinite sinusoid are compared in Figure \ref{fig:waveletinf}.

\begin{figure}[ht]
	\centering
        \begin{minipage}{1.\textwidth}
		\renewcommand\footnoterule{} % optional removing footnote bar
		\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
		\includegraphics[width=0.75\textwidth]{./images-wavelets/wavelet.png}
		\caption[Infinite sine wave of the Fourier transform vs. finite wavelet]{Infinite sine wave of the Fourier transform vs. finite wavelet\footnote[1]{\url{https://www.mathworks.com/help/wavelet/gs/what-is-a-wavelet.html}}}
		\label{fig:waveletinf}
	\end{minipage}
\end{figure}

\textcite{wavelets} uses a wavelet transform to perform source separation. They create a \textit{scalogram}, which is the wavelet equivalent of a spectrogram, using a wavelet transform similar to the constant-Q transform \parencite[2--3]{wavelets}:

\begin{quote}
	The time-frequency structures of audio signals are well revealed by Q-constant filter banks, which model the signal cochlea transformation. This can also be written as a multiscale wavelet transform, whose modulus defines a time-frequency representation that is called a scalogram image[...] We show that a wavelet transform can partly separate the time-frequency supports of two different harmonic template models, and characterize the overlap of these time-frequency supports.
\end{quote}

Figure \ref{fig:waveletsep} shows the wavelet scalogram-based source separation.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{./images-wavelets/wavelet_sep.png}
	\caption{Source separation using wavelets. Cyan shows the first harmonic source, yellow shows the second harmonic source, and red shows the overlap \parencite[5]{wavelets}}
	\label{fig:waveletsep}
\end{figure}

\subsubsection{Time-frequency masking and oracle estimators}
\label{sec:masksandoracles}

Surveys on speech \parencite{speechmask} and music separation \parencite{musicmask} indicate that the majority of source separation algorithms use the technique of time-frequency masking to separate the sources.  \textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, or its real-valued magnitude, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins.

A soft mask, or ratio mask, has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only zero and one. To compute a binary mask, there must be an additional real-valued parameter, $\theta \in [0.0, 1.0]$, which is the separation factor -- values below $\theta$ are set to 0, and values above $\theta$ are set to 1. According to \textcite{masking}, soft masks are generally produce a higher quality of sound. An illustration of spectral masking is shown in Figure \ref{fig:masks}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-mss/maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising \parencite[71]{masking}}
	\label{fig:masks}
\end{figure}

An example of a soft or ratio mask, used by \textcite{fitzgerald1, fitzgerald2}, is given by equation \eqref{equation:softmask}:
\begin{align}
	M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|^{p}}{|\hat{S}_{\text{interference}}|^{p} + |\hat{S}_{\text{target}}|^{p}}\tag{16}\label{equation:softmask}
\end{align}

, where $\hat{S}$ represents the complex-valued spectrogram and $p$ represents the raised power ($p = 1$ is the magnitude spectrogram, and $p = 2$ is the power spectrogram). The binary or hard mask used by \textcite{driedger} is given by equation \eqref{equation:hardmask}:
\begin{align}
	M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|}{|\hat{S}_{\text{interference}}| + \epsilon} \le \beta\tag{17}\label{equation:hardmask}
\end{align}

, where $\beta$ is the separation factor. Note the inclusion of machine epsilon in the denominator, to avoid division by zero. One advantage of the hard mask's separation factor is that a third residual component can be extracted. This is shown for two arbitrary sources a and b in equations \eqref{equation:residual}:
\begin{align}\tag{18}\label{equation:residual}
	\nonumber & M_{\text{a}} = \frac{|\hat{S}_{\text{a}}|}{|\hat{S}_{\text{b}}| + \epsilon} > \beta, M_{\text{b}} = \frac{|\hat{S}_{\text{b}}|}{|\hat{S}_{\text{a}}| + \epsilon} \ge \beta\\
	\nonumber & M_{\text{residual}} = 1 - (M_{\text{a}} + M_{\text{b}})
\end{align}

The oracle mask, or oracle estimator, is a perfect time-frequency mask which is computed from ground-truth data. The use of the oracle estimator is to give an idea of the upper limit of audio quality from an algorithm or machine learning model for source separation or demixing. Typically, the mask is computed from and applied to the magnitude spectrograms \parencite{fitzgerald1, fitzgerald2, driedger, umx, plumbley1, plumbley2}. Discarding the phase of the complex STFT is a choice made for simplicity, although the phase may be important for source separation applications \parencite{ditchphase}.

To illustrate the calculation of the oracle mask, let us describe a simple case of a mixed song consisting of a vocal and drum track. To compute oracles, note that it is necessary to have access to the ground truth isolated source recordings of vocals and drums, in addition to the mix.

The waveforms $x_{v}[n]$ and $x_{d}[n]$ denote the isolated vocal and drum tracks respectively. The mixed song is defined by the waveform $x_{m}[n] = x_{v}[n] + x_{d}[n]$. To apply the music demixing task to this waveform, I want my algorithm or model to take the mixed waveform $x_{m}[n]$ as an input, and estimate the two source waveforms of vocals $\hat{x}_{v}[n]$ and drums $\hat{x}_{d}[n]$.

Several interesting oracle masks can be calculated from the STFTs of the mix and ground truths of two sources shown in equations \eqref{equation:targetstfts}:
\begin{align}\tag{19}\label{equation:targetstfts}
	\nonumber X_{m} &= \mathit{STFT}(x_{m}[n])\\
	\nonumber X_{v} &= \mathit{STFT}(x_{v}[n])\\
	\nonumber X_{d} &= \mathit{STFT}(x_{d}[n])
\end{align}
 
\textcite{sisec2018} report the performance of several oracles called IRM1, IRM2, IBM1, and IBM2. These acronyms can be understood as follows; the ``I'' stands for Ideal, ``R|B'' denotes a Ratio (soft) vs. Binary (hard) mask, ``M'' stands for Mask, and the trailing number is the $p$th power which the magnitude spectrogram is raised to. For example, the IRM1 is a soft mask between the magnitude spectrograms (since the magnitude spectrogram raised to the power of one is simply itself), and IBM2 is a hard mask between the power spectrograms (i.e., the magnitude spectrogram raised to the power of two).

The oracles for the vocal source (and the same equations apply to the drum track) are computed from equations \eqref{equation:vocaloracles}, noting that a typical default value for $\theta$ is 0.5:
\begin{align}\tag{20}\label{equation:vocaloracles}
	\nonumber \mathit{IRM1}_{v} &= \frac{|X_{v}|^{1}}{|X_{m}|^{1}}\text{,\qquad}
	\nonumber \mathit{IRM2}_{v} = \frac{|X_{v}|^{2}}{|X_{m}|^{2}}\\
	\nonumber \mathit{IBM1}_{v} &= \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} \ge \theta
	\end{cases},
	\nonumber \mathit{IBM2}_{v} = \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} \ge \theta
	\end{cases}
\end{align}

To estimate the time domain waveform from an oracle mask, the complex STFT of the mixed waveform, $X_{m}$, is multiplied by the mask, and the resultant complex STFT is inverted back to the time-domain waveform using equation \eqref{equation:cplxinvert}:
\begin{align}\tag{21}\label{equation:cplxinvert}
	\nonumber \hat{X}_{v\text{, IRM1}} &= X_{m} \cdot \text{IRM1}_{v}, \hat{x}_{v\text{, IRM1}} = \mathit{iSTFT}(\hat{X}_{v\text{, IRM1}})
\end{align}

\subsubsection{Noisy phase or mix-phase inversion (MPI)}
\label{sec:noisyphaseoracle}

As mentioned in Section \ref{sec:masksandoracles}, many music demixing systems prefer to only estimate magnitude spectrograms, and discard the phase. These systems use the phase of the mixed audio combined with the magnitude of the estimated source to create an audio waveform.

In demixing or source separation literature, this strategy is referred to as the ``noisy phase'' \parencite{noisyphase1, noisyphase2}. The name originates from speech separation, where a common task is to separate speech from noise. In music demixing, referring to interfering musical instruments as ``noise'' is inappropriate, and I will use the term ``mix-phase inversion'' (MPI) to refer to the same idea.

Equations \eqref{equation:mpineural} show how the phase of the mixture and magnitude of the estimated source are used to create a waveform by some arbitrary music demixing system:
\begin{align}\tag{29}\label{equation:mpineural}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}}[n])\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \mathit{MusicDemixingSystem}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \mathit{iSTFT}(\hat{X}_{\text{source, est}})
\end{align}

In other words, the magnitude of the isolated source is combined with the phase, or the angle, of the mixed waveform, to produce a complex time-frequency transform, which is then inverted with the backward transform to obtain the estimated isolated source waveform. 

Let us write out the equations for an MPI oracle, derived from equations \eqref{equation:mpineural}. The MPI oracle can be computed from ground-truth data, using equations \eqref{equation:mpioracle}:
\begin{align}\tag{30}\label{equation:mpioracle}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}}[n])\\
	\nonumber & X_{\text{source}} = \mathit{STFT}(x_{\text{source}}[n])\\
	\nonumber & \hat{X}_{\text{source, MPI}} = |X_{\text{source}}| \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, MPI}}[n] = \mathit{iSTFT}(\hat{X}_{\text{source, MPI}})
\end{align}

This is the first estimate of the mix-phase inversion strategy, and does not assume further post-processing of the estimates. It gives us the idea of the performance of a time-frequency transform of an isolated source when the phase is discarded and the magnitude is combined with the phase of the mixture.

\subsubsection{Open-source ecosystem}

There are several groups of researchers who created open-source ecosystems on the code sharing platform GitHub to provide tools, tutorials, and implementations for source separation. In particular, the SigSep organization\footnote{\url{https://github.com/sigsep}} contains many tools useful for music demixing research, including the reference implementation of Open-Unmix \parencite{umx}, MUSDB18 dataset loaders \parencite{musdb18, musdb18hq}, and BSS evaluation metrics \parencite{bss, bss2, sisec2018}.

\subsubsection{Public datasets}

The most popular music stem dataset used by the Signal Separation Evaluation Campaign (SiSEC) and SigSep is the MUSDB18 dataset \parencite{musdb18}, and more recently the HQ (high-quality) version \parencite{musdb18hq}. MUSDB18-HQ contains stereo wav files sampled at 44,100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets \parencite{sisec2016, otherdataset2}.

The MUSDB18-HQ dataset has fixed train, validation, and test splits. The dataset is organized into \Verb#train# and \Verb#test# folders, and a list of 14 tracks from the training folder are defined to be the validation set by the Python MUSDB18-HQ loader library.\footnote{https://github.com/sigsep/sigsep-mus-db}

Following the rules defined in the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} for a model to be considered trained only on MUSDB18-HQ, the predefined data splits must be used.

\subsubsection{Evaluation measures}
\label{sec:evalbss}

The SigSep\footnote{\url{https://sigsep.github.io/}} community, borrowing from the methodology of Signal Separation Evaluation Campaign (SISEC), uses the BSS (Blind Source Separation) Eval \parencite{bss, bss2} objective measure for separation quality. There are four distinct metrics that comprise BSS:

\begin{tight_itemize}
\item
	\textbf{SDR:} Signal to Distortion Ratio
\item
	\textbf{SIR:} Signal to Interference Ratio
\item
	\textbf{SAR:} Signal to Artifacts Ratio
\item
	\textbf{ISR:} source Image to Spatial distortion Ratio
\end{tight_itemize}

Out of these four scores, SDR is the single global score which is commonly used to summarize the overall performance of a music demixing system \parencite{sdruseful}. The SDR as it was defined in the ISMIR 2021 Music Demixing Challenge (and used to rank the participants) can be computed from equation \eqref{equation:sdrinstr}:
\begin{align}
	\nonumber & \mathit{SDR}_{\text{instr}} = \\
	&10 \log_{10}\frac{\sum_{n}\big(s_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n)\big)^{2}}{\sum_{n}\big(s_{\text{instr, left}}(n) - \hat{s}_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n) - \hat{s}_{\text{instr, right}}(n)\big)^{2}} \tag{22}\label{equation:sdrinstr}
\end{align}

The unit of the SDR score is in decibels or dB, $s_{\text{instr}}(n)$ denotes the ground truth waveform of the instrument, $\hat{s}_{\text{instr}}(n)$ is the estimate, and left and right refer to the two channels in the stereo dataset of MUSDB18-HQ. Given the four stems of MUSDB18-HQ -- vocals, drums, bass, other -- the SDR is computed for each stem from equation \eqref{equation:sdrinstr} above. Then, the four SDR scores are combined for a total song score in equation \eqref{equation:sdrsong}:
\begin{align}
	\mathit{SDR}_{\text{song}} = \frac{1}{4}(\mathit{SDR}_{\text{bass}} + \mathit{SDR}_{\text{drums}} + \mathit{SDR}_{\text{vocals}} + \mathit{SDR}_{\text{other}}) \tag{23}\label{equation:sdrsong}
\end{align}

In the 2018 Signal Source Separation Evaluation Compaign (SiSEC), \textcite{sisec2018} introduced an evolution of BSS metrics, called BSS v4. BSS v4 uses time-invariant distortion filters to reduce the computational cost over the original BSS metrics used in SiSec 2016 \parencite{sisec2016}. The new BSS v4 metrics were also made available in the Python libraries museval\footnote{\url{https://github.com/sigsep/museval}} and bsseval,\footnote{\url{https://github.com/sigsep/bsseval}} which were used to calculate the results in this thesis.

\subsubsection{Open-Unmix (UMX)}
\label{sec:umx}

Open-Unmix (UMX) was created by \textcite{umx} as an open-source, reference implementation of a state-of-the-art deep neural network for music source separation. UMX is based on STFT masking, and is intended to foster reproducible research by only using the open MUSDB18 dataset for training data \parencite{musdb18, musdb18hq}.

UMX has a reference implementation released as an open-source Python project,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} which uses PyTorch \parencite{pytorch} for GPU-accelerated numerical computation and deep learning, and includes a MUSDB18-HQ data loader \parencite{musdb18hq} and BSS metrics evaluator \parencite{bss}.

In UMX, a deep neural network (DNN) is used to estimate the magnitude spectrograms of the sources from an input mixed song. The architecture of the DNN is a variant of a sequence2sequence model. UMX uses a bidirectional LSTM or Bi-LSTM architecture, which is based on two predecessor networks by \textcite{umxorig1} and \textcite{umxorig2}. Figure \ref{fig:umxes} shows the architecture of UMX and one of its predecessors.

\begin{figure}[ht]
	\centering
	\subfloat[UMX Bi-LSTM architecture\protect\footnotemark]{\includegraphics[width=\textwidth]{./images-neural/umx.png}}\\
	\subfloat[Simple DNN architecture for music source separation {\parencite[262]{umxorig1}}]{\includegraphics[width=\textwidth]{./images-neural/typical_simple_mss_dnn.png}}
	\caption{Architecture of UMX and one of its simpler predecessors}
	\label{fig:umxes}
\end{figure}

\footnotetext{\url{https://github.com/sigsep/open-unmix-pytorch\#-the-model-for-one-source}}

The MUSDB18-HQ data loader is the starting point of the neural network, and it returns a single target at a time, or an $(x, y)$ pair where $x$ is the mixed waveform and $y$ is the desired target. The final output of UMX is $\hat{y}$, the estimated target waveform. In between the input and output time-domain waveforms, the magnitude STFT domain is used by UMX. The important training and inference steps are as follows.

Before the training of the neural network, the mixed waveforms of the training set $\{x\}_{\text{train}}$ have their magnitude STFTs taken, $\{|X|\}_{\text{train}}$. These are used to compute the mean and standard deviation, or $\mathit{mean}(\{|X|\}_{\text{train}})$ and $\mathit{std}(\{|X|\}_{\text{train}})$. This is to perform data whitening, which \citeauthor{Kessy_2018} describes as a process that ``greatly simplifies multivariate data analysis both from a computational and a statistical standpoint,'' and that ``whitening is a critically important tool, most often employed in preprocessing'' \parencite[309]{Kessy_2018}.

In the training loop, the $(x, y)$ pairs of mixed audio and the ground-truth of the target source waveform are converted to the magnitude STFT domain $(|X|, |Y|)$. The data whitening from the previous step is applied to the input mixed magnitude spectrogram $|X|$, to use $(|X|+\mathit{mean}(\{|X|\}_{\text{train}}))\times\mathit{std}(\{|X|\}_{\text{train}})$ as the input to the Bi-LSTM neural network. The Bi-LSTM neural network outputs $|\hat{Y}|$, the estimated magnitude STFT of the target sources.

The loss function is the mean-squared error (MSE) function. In the loss function, the estimated magnitude STFT of the target source is compared to the ground-truth, shown in equation \eqref{equation:mselossstft}.
\begin{align}\tag{31}\label{equation:mselossstft}
	\nonumber & \mathit{MSE}(Y, \hat{Y}) = \frac{1}{n} \sum_{i = 1}^{n}{(Y_{i}-\hat{Y}_{i})^{2}}
\end{align}

, where $n$ is the number of elements in the magnitude STFT matrix.

There are four independent Bi-LSTM neural networks for each of the four sources: vocals, bass, drums, and other. The four Bi-LSTM models are trained until the loss stops improving. At this point, UMX is ready to perform inference.

First, the four independently-trained Bi-LSTM models are used to obtain the estimated magnitude STFTs of the four target sources. The mix phase inversion is used to create a first estimate the waveforms for the four targets. Equations \eqref{equation:mpirepeat} show how the estimated magnitude STFT of UMX Bi-LSTM is converted to the time-domain waveform estimate:

\begin{align}\tag{32}\label{equation:mpirepeat}
	\nonumber & X_{\text{mix}} = \mathit{STFT}(x_{\text{mix}}[n])\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \mathit{UMX}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \angle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \mathit{iSTFT}(\hat{X}_{\text{source, est}})
\end{align}

Similar equations were previously described for the mix-phase inversion (MPI) oracle in Section \ref{sec:noisyphaseoracle}.

Finally, the four estimated magnitude STFTs are considered together, along with the phase of the STFT of the original mixed audio, to perform a post-processing step called iterative Wiener filtering and expectation-maximization (EM), jointly referred to as ``Wiener-EM'' \parencite{umxorig1, wiener2, wiener3, wiener4}. The output of the Wiener-EM step is a refined estimate of the waveforms of the four target sources, which is the final output of UMX.

\subsubsection{CrossNet-Open-Unmix (X-UMX)}
\label{sec:xumx}

\textcite{xumx} improved on UMX without introducing any additional parameters. They combined the four separate target networks into a single model to apply the loss functions and optimizations jointly across all four targets, rather than optimizing each one separately. The model is named CrossNet-Open-Unmix (X-UMX). It also includes loss computed from the time-domain waveforms, in addition to the loss measured from the magnitude spectral coefficients.

The loss function of X-UMX is first modified to include a multi-domain loss (MDL), which includes the existing frequency-domain loss (spectrogram MSE) in addition to a new time-domain loss. The time-domain loss is the same SDR score from the BSS metrics which was shown in Section \ref{sec:evalbss}.

I showed in equation \ref{equation:mpirepeat} how to convert from the estimated magnitude spectrogram of the neural network to the waveform estimate. In X-UMX, these equations are used to compute the time-domain waveforms and apply the SDR loss function.

The next addition to the loss function of X-UMX is the combination loss (CL), which considers 14 possible combinations of the four target sources (vocals, drums, bass, other) in both the spectral MSE loss and time-domain SDR loss. The 14 combinations are shown in Table \ref{table:14targets}.

\begin{table}[ht]
	\centering
	\caption{14 combinations of the four targets in X-UMX}
	\label{table:14targets}
	\begin{tabular}{ |l|l|l| }
	 \hline
		Combination size & Combination number & Targets \\
	 \hline
	 \hline
	 	1 & 1 & bass \\
	 \hline
	 	1 & 2 & vocals \\
	 \hline
	 	1 & 3 & other \\
	 \hline
	 	1 & 4 & drums \\
	 \hline
	 	2 & 5 & bass + vocals \\
	 \hline
	 	2 & 6 & bass + other \\
	 \hline
	 	2 & 7 & bass + drums \\
	 \hline
	 	2 & 8 & vocals + other \\
	 \hline
	 	2 & 9 & vocals + drums \\
	 \hline
	 	2 & 10 & other + drums \\
	 \hline
	 	3 & 11 & bass + vocals + other \\
	 \hline
	 	3 & 12 & bass + vocals + drums \\
	 \hline
	 	3 & 13 & bass + other + drums \\
	 \hline
	 	3 & 14 & vocals + other + drums \\
	 \hline
\end{tabular}
\end{table}

Equations \eqref{equation:cl} show the computation of the total frequency-domain loss from the 14 combinations, where $|Y|$ represents the magnitude spectrogram of the ground-truth target waveform, and $\hat{|Y|}$ represents the estimated magnitude spectrogram of the neural network. The subscript of $|Y|$ and $|\hat{Y}|$ represents the target (1--4 for vocals, drums, bass, and other):
\begin{align}\tag{34}\label{equation:cl}
	\nonumber & \text{mse\_loss}_{1} = \mathit{MSE}(\hat{|Y|}_{1}, |Y|_{1})\\
	\nonumber & \text{(... repeat for size-1 combinations)}\\
	\nonumber & \text{mse\_loss}_{5} = \mathit{MSE}(\hat{|Y|}_{1} + \hat{|Y|}_{2}, |Y|_{1} + |Y|_{2})\\
	\nonumber & \text{(... repeat for size-2 combinations)}\\
	\nonumber & \text{mse\_loss}_{11} = \mathit{MSE}(\hat{|Y|}_{1} + \hat{|Y|}_{2} + \hat{|Y|}_{3}, |Y|_{1} + |Y|_{2} + |Y|_{3})\\
	\nonumber & \text{(... repeat for size-3 combinations)}\\
	\nonumber & \text{mse\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{mse\_loss}_{n}}
\end{align}

For the time-domain loss, the same 14 combinations are considered in the time-domain, using the ground-truth time-domain waveforms $y$ and the estimated time-domain waveform $\hat{y}$ from the mix-phase inversion, shown in the equations \eqref{equation:mdl}:
\begin{align}\tag{35}\label{equation:mdl}
	\nonumber & \text{sdr\_loss}_{1} = \mathit{SDR}(\hat{y}_{1}, y_{1})\\
	\nonumber & \text{(... repeat for size-1 combinations)}\\
	\nonumber & \text{sdr\_loss}_{5} = \mathit{SDR}(\hat{y}_{1} + \hat{y}_{2}, y_{1} + y_{2})\\
	\nonumber & \text{(... repeat for size-2 combinations)}\\
	\nonumber & \text{sdr\_loss}_{11} = \mathit{SDR}(\hat{y}_{1} + \hat{y}_{2} + \hat{y}_{3}, y_{1} + y_{2} + y_{3})\\
	\nonumber & \text{(... repeat for size-3 combinations)}\\
	\nonumber & \text{sdr\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{sdr\_loss}_{n}}
\end{align}

As a last step, the time-domain loss is multiplied by a mixing coefficient, $\alpha$, before adding it to the frequency-domain loss to create the total loss function in equation \eqref{equation:totalloss}:
\begin{align}\tag{36}\label{equation:totalloss}
	\nonumber & \text{loss}_{X-UMX} = \text{mse\_loss}_{\text{total}} + \alpha \cdot \text{sdr\_loss}_{\text{total}}
\end{align}

A visualization of the multi-domain loss (MDL) and combination loss (CL) for four targets is shown in Figure \ref{fig:xumxlosses}.

\begin{figure}[ht]
	\centering
	\subfloat[Multi-domain loss (MDL)]{\includegraphics[width=\textwidth]{./images-neural/xumx1.png}}\\
	\subfloat[Combination loss (CL)]{\includegraphics[width=\textwidth]{./images-neural/xumx2.png}}
	\caption{Loss functions of X-UMX \parencite[2]{xumx}}
	\label{fig:xumxlosses}
\end{figure}

\newpagefill

\subsubsection{Convolutional denoising autoencoder (CDAE)}
\label{sec:cdae}

While the discussed Open-Unmix model is based on sequence2sequence ideas with a bidirectional LSTM architecture, a different class of neural network called convolutional autoencoders, or convolutional denoising autoencoders (CDAE), have been seeing increasing use in music demixing \parencite{plumbley1, plumbley2}. The architectures are shown in Figure \ref{fig:cdaes}, where 2D convolutions (in the time and frequency dimensions) are applied on the magnitude STFT. Section \ref{sec:cnn} provides more details on convolutional and deconvolutional layers and their various parameters.

\begin{figure}[ht]
	\centering
	\subfloat[Simple CDAE {\parencite[2]{plumbley1}}]{\includegraphics[width=\textwidth]{./images-neural/cdae_1.png}}\\
	\subfloat[CDAE with time and frequency filters that vary by frequency band {\parencite[2]{plumbley2}}]{\includegraphics[width=0.8\textwidth]{./images-neural/cdae_2.png}}
	\caption{Convolutional architectures for music source separation}
	\label{fig:cdaes}
\end{figure}

CDAEs are symmetric: a stack of convolutional layers in the encoder are applied to the input spectrogram to extract spectral audio features with sliding 2D (frequency filter, time filter) kernels. The corresponding reverse stack of transpose convolution (or deconvolution) layer in the decoder use the same size of filter (in reverse order) to grow the feature map back to the original spectrogram. The number of channels of each layer determines the feature maps of the hidden layers. For example, a 2-layer encoder/decoder for 2-channeled stereo spectrogram inputs might consist of the following layers:

\begin{itemize}
	\item
		$\text{Conv}(\text{in\_channels}=2, \text{out\_channels}=12, \text{filter}=(11,42))$
	\item
		$\text{Conv}(\text{in\_channels}=12, \text{out\_channels}=24, \text{filter}=(3,5))$
	\item
		$\text{ConvTranspose}(\text{in\_channels}=24, \text{out\_channels}=12, \text{filter}=(3,5))$
	\item
		$\text{ConvTranspose}(\text{in\_channels}=12, \text{out\_channels}=2, \text{filter}=(11,42))$
\end{itemize}

Each of the papers gives different ideas on how to adapt STFT-based convolutional models to fit the sliCQT. The model introduced in \textcite{plumbley1} shows a simple case of a 2D time and frequency filter applied to the STFT, which can be applied to any time-frequency transform. The model introduced in \textcite{plumbley2} varies the size of time and frequency filter to fit the Constant-Q Transform, which as discussed in Section \ref{sec:cqt} analyzes music with higher frequency resolution in the low frequency region, and higher time resolution in the high frequency region.

\end{document}
