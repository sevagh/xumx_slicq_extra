\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{sec:background}

\subsection{Acoustic signals and the time-domain waveform}
\label{sec:timedomain}

\textcite[8]{discretebook} define a signal as a function of one or more variables which ``conveys information about the state or behavior of a physical system.'' Some examples of signals are a 2-dimensional image, which is a brightness function of two spatial variables, and a speech signal, which is a function of time. \textcite[2]{moore} describes the more specific case of a sound or audio signal as being the description of the vibration of an object and how it impresses this vibration upon the ``surrounding medium (usually air) as a pattern of changes in pressure.''

The waveform is defined as the function of pressure variation plotted against time (\cite{moore, melbook}), and is real-valued and continuous in both time and amplitude (\cite{melbook}). A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers (\cite{discretebook}). The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$.  A continuous-time signal and its discrete representation are shown in figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/continuoustime.png}}\\
	\subfloat[Discrete-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/discretetime.png}}
	\caption{A continuous-time signal and its discrete-time representation sampled with $T = 125\mu s$ (\cite[10]{discretebook})}
	\label{fig:discretecontinuous}
\end{figure}

Continuous-time signals are converted into discrete-time representations by two processes: sampling and quantization. Points on the continuous time axis of the waveform are \textit{sampled} periodically, and the amplitude values of the waveform at these sampled points are \textit{quantized} to find the closest digital number (\cite{melbook}). Time sampling is controlled by the sampling period $T \text{ seconds}$, or the sampling rate $F_{s} = \sfrac{1}{T} \text{ Hz}$, which define the periodicity of the sampling. Amplitude quantization is controlled by the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x_{a}(t)$ is defined by $x[n] = x_{a}(nT)$, where $n$ is an integer and $T = \sfrac{1}{F_{s}}$ is the sampling period. The Nyquist-Shannon sampling theorem (\cite{discretebook}), described independently by \textcite{nyquist1928} and \textcite{shannon1948}, states that the maximum frequency of a signal that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \sfrac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency.

Aliasing is one of the pitfalls of choosing an inappropriate sampling rate for the frequencies present in the signal under observation (\cite{dspfirst}). The diagram in figure \ref{fig:aliasing} shows the phenomenon of aliasing due to undersampling, which occurs when a time-domain signal is undersampled, i.e. $F_{\text{sig}} > F_{\text{nyq}}$, and the continuous-time signal cannot be reconstructed accurately. We refer the readers to the full chapter in the textbook for a more full description of challenges in signal sampling.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-tftheory/aliasing_undersampling.png}
	\caption{An undersampled cosine wave (red) and the resulting incorrect reconstruction (black) (\cite[82]{dspfirst})}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \textcite[190]{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values.'' The second variable which relates to the quantization process is the number of quantization levels. The quantization operation is represented as $ \hat{x}[n] = Q(x[n])$, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation (\cite{discretebook}). An A/D or ADC (analog-to-digital converter) circuit and its operation on a waveform is shown in figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC)]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\vspace{0.1em}
	\subfloat[Waveform results of the ADC process]{\includegraphics[width=0.85\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations (\cite[188, 190, 192]{discretebook})}
	\label{fig:adccircuit}
\end{figure}

\subsection{Transforms of acoustic signals}
\label{sec:freqdomain}

\textcite[4]{moore} states that

\begin{quote}
	[a]lthough all sounds can be specified by their variation in pressure with time, it is often more convenient, and more meaningful, to specify them in a different way when the sounds are complex. This method is based on a theorem by Fourier, who proved that almost any complex waveform can be analyzed, or broken down, into a series of sinusoids with specific frequencies, amplitudes, and phases. This is done using a mathematical procedure called the Fourier Transform.
\end{quote}

Throughout this section, the description of the Fourier Transform of acoustic signals and the evolution of further transforms that build on it will be covered in detail.

The Large Time-Frequency Analysis toolbox (LTFAT) is ``a Matlab/Octave toolbox for working with time-frequency analysis and synthesis.''\footnote{\url{http://ltfat.org/}} It contains a test signal from the glockenspiel instrument, loaded by the \Verb#gspi# function.\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} This signal can be seen in audio signal processing papers on the topic of time-frequency (\cite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt, wmdct}). This is because the glockenspiel contains both tonal and transient properties, which have conflicting needs for time and frequency resolution in their analysis. Figure \ref{fig:glockwaveform} shows the discrete-time waveform of the glockenspiel signal. For the rest of this chapter, demonstrations of each transform will use this same glockenspiel signal as the input $x[n]$. The signal has a total duration of 5.94 seconds, and is sampled with a rate of 44100 Hz.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.725\textwidth]{./images-gspi/glock_waveform.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

\subsubsection{Frequency analysis and Fourier Transforms (CTFT, DFT, FFT)}
\label{sec:freqanal}

The Fourier Transform originated as an integral transform in mathematics, which are a class of ``useful tools for solving problems involving certain types of partial differential equations (PDEs), mainly when their solutions on the corresponding domains of definition are difficult to deal with'' (\cite[54]{fourierhistory}). The Fourier Transform was originally introduced by Joseph Fourier in his earlier papers (\cite{fourierhist1, fourierhist2}), and fully expanded and collected in his seminal work on heat (\cite{fourierheat}). The connection of the Fourier Transform to music is described by \textcite[461]{fouriermusic}, who state that

\begin{quote}
	[b]eyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contribution represents some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string.
\end{quote}

The continuous-time Fourier Transform (CTFT) of a time-domain acoustic waveform is defined by the pair of equations \eqref{equation:ctft} and \eqref{equation:ictft} (\cite[308]{dspfirst}):
\begin{align}
	X(j\omega) = \int_{-\infty}^{\infty}{x(t)e^{-j\omega t}\mathit{dt}} \tag{1}\label{equation:ctft} \\
	x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{X(j\omega)e^{j\omega t}\mathit{d\omega}} \tag{2}\label{equation:ictft}
\end{align}

\textcite{dspfirst} calls $X(j\omega)$ the frequency-domain representation of the signal $x(t)$. Equation \eqref{equation:ictft} defines the signal $x(t)$ in terms of a sum of infinitely many complex-exponential signals with $X(j\omega)$ controlling the amplitude and phases of these signals. The continuous-time Fourier Transform provides a one-to-one mapping of the time domain to the frequency domain.

Signals needs to be transformed from the continuous to the discrete domain via sampling to be processed digitally or computationally, which was covered previously in section \ref{sec:timedomain}. The discrete-time Fourier Transform (DTFT), also called the discrete Fourier Transform (DFT), is derived from a sampled version of the continuous-time Fourier Transform shown previously in equations \eqref{equation:ctft} and \eqref{equation:ictft}, and is defined by the pair of equations \eqref{equation:dtft} and \eqref{equation:idtft} (\cite[289]{melbook}):
\begin{align}
	X(e^{j\omega}) = \sum_{n = -\infty}^{\infty}{x[n]e^{-j\omega n}} \tag{3}\label{equation:dtft} \\
	x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}{X(e^{j\omega})e^{j\omega n}\mathit{d\omega}} \tag{4}\label{equation:idtft}
\end{align}

The Fourier Transform is a complex-valued function of $\omega$, which is the variable that represents the angular frequency in radians. The Fourier Transform can be expressed in the rectangular form in equation \eqref{equation:rect} or polar form in equation \eqref{equation:polar} (\cite[49]{discretebook}):
\begin{align}
	X(e^{j\omega}) = X_{\text{real}}(e^{j\omega}) + j X_{\text{imag}}(e^{j\omega}) \tag{5}\label{equation:rect} \\
	X(e^{j\omega}) = |X(e^{j\omega})|e^{j\measuredangle X(e^{j\omega})} \tag{6}\label{equation:polar}
\end{align}

The quantities $|X(e^{j\omega})|$ and $\measuredangle X(e^{j\omega})$ are referred to as the magnitude and phase respectively. The Fourier Transform is also referred to as the spectrum, while its magnitude and phase are the magnitude and phase spectra respectively (\cite{discretebook}).

According to \textcite{skiena}, algorithms in computer science are often described by their time complexity in a hypothetical computer where simple operations take 1 time step. The ``Big-O'' notation, or $O(N)$, provides the upper bound of algorithm running time in relation to number of input elements. In the case of the DFT, the number of input elements is the number of samples in the discrete signal. In \textcite[Chapter~9]{discretebook} it is described that in its original formulation, the algorithmic complexity of the DFT is $O(N^{2})$, i.e. the running time of the algorithm grows proportionally the size of the input signal squared (\cite{skiena}). 

Starting from legendary mathematician Carl Friedrich Gauss in 1805 (\cite{gausshist}), and reaching its most famous formulation published by \textcite{cooleytukey}, a family of efficient algorithms for the computation of the DFT by computing a series of smaller DFTs, known collectively as the Fast Fourier Transform (FFT), reduced this computation time to $O(N \log{N})$. This resulted in the FFT becoming one of the most important algorithms of the 20th century (\cite{ffttopten}).

The number of points, or samples, of the DFT, determines the frequency resolution, also called $\mathit{df}$ or $\mathit{\Delta f}$, which is the frequency spacing between each point in the resulting spectrum (\cite{discretebook}). The frequency resolution of an $N$-point DFT is $\mathit{df} = \sfrac{F_{s}}{N}$, where $F_{s}$ is the sampling rate of the input signal. An illustration of the magnitude and phase spectra of the DFT are shown in figure \ref{fig:glockdft}, using two different lengths of DFT to show the difference in the low and high frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[2048-point DFT, $\mathit{df} = 21.533 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/glock_dft_2048.png}}\\
	\subfloat[262144-point DFT, $\mathit{df} = 0.168 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/glock_dft_262144.png}}
	\caption{DFT of the Glockenspiel waveform. Note the richer frequency information in the higher frequency resolution transform in (b)}
	\label{fig:glockdft}
\end{figure}

\newpagefill

\subsubsection{Joint time-frequency analysis with the Gabor Transform and Short-time Fourier Transform (STFT)}
\label{sec:jointtfa}

Continuing from the discussion of the DFT in the previous section \ref{sec:freqanal}, \textcite[714]{discretebook} state that ``often, in practical applications of sinusoidal signal models, the signal properties (amplitudes, frequencies, and phases) will change with time. For example, nonstationary signal models of this type are required to describe radar, sonar, speech, and data communication signals. A single DFT estimate is not sufficient to describe such signals...''

Dennis Gabor's seminal signal processing paper, \textit{The Theory of Communication}, introduced significant and far-reaching concepts in the time and frequency analysis of acoustic signals (\cite{gabor1946}). \textcite[431]{gabor1946} quotes famed American telecommunication engineer John Carson (\cite{carsonfamous}) to describe the limitations of the Fourier Transform:

\begin{quote}
	[t]he foregoing solutions [of the Fourier Transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren
\end{quote}

According to \textcite[3624]{korpel}, ``Gabor came to the conclusion that the difficulty lay in our mutually exclusive formulations of time analysis and frequency analysis ... he suggested a new method of analyzing signals in which time and frequency play symmetrical parts.''

Gabor derived the principal of time-frequency uncertainty from the Heisenberg uncertainty principle in quantum physics, which states that the more precisely the position of an electron is determined, the less precisely the momentum is known, and conversely \secondaryciteindirect{heisenberg1927}{hallm}. \textcite[432]{gabor1946} states that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit.''. This is referred to as the time-frequency uncertainty principle or the Gabor limit. Gabor defines the unit of time-frequency information $\Delta t \Delta f$ as the \textit{logon}, where $\Delta t$ and $\Delta f$ are defined as ``the uncertainties inherent in the definition of the epoch t and frequency f of an oscillation'' (\cite[432]{gabor1946}).

Let us start with a description of the time-domain unit impulse signal or sequence (\cite[20]{melbook}) in equation \eqref{equation:delta}:
\begin{flalign}\label{equation:delta}
\delta[n] = \begin{cases}
	1 \text{\hspace{1em}} n = 0\\
	0 \text{\hspace{1em}} \text{otherwise}
\end{cases}
\end{flalign}

The impulse is a useful signal, as it is the ``simplest [time-domain] sequence because it has only one nonzero value, which occurs at n = 0. The mathematical notation is that of the Kronecker delta function'' (\cite[107]{dspfirst}). Note the Kronecker delta function is the discrete equivalent of the Dirac delta function (\cite[937]{melbook}). Contrast this with the DFT spectrum of a signal that is only nonzero at the 0 Hz (or DC) frequency component, which is a ``sinusoid of zero frequency'' (\cite[13]{dspfirst}).

Figure \ref{fig:gaborfirst}(a) shows the unit impulse contrasted with the DC-component DFT, and these two in fact demonstrate the mutually exclusive formulations of time and frequency. The unit impulse, which has a single nonzero value in the time domain, has an infinite extent in the frequency domain. Conversely, the DC-component DFT has a single nonzero value in the frequency domain, but has an infinite extent in the time domain. Figure \ref{fig:gaborfirst}(b) shows the intuition of the time-frequency tradeoff as a choice of signal length. The periodicity of the sine wave is more apparent over longer periods of time $\Delta t$, but the detail of the individual sample values become lost.

\begin{figure}[ht]
	\centering
	\subfloat[Mutually exclusive formulations of time and frequency by two extremes, the unit impulse (bottom left) and the 0 Hz cosine DFT spectrum (top right)]{\includegraphics[height=5cm]{./images-tftheory/gabor13.png}}
	\hspace{2em}
	\subfloat[The periodicity of the sine wave is more apparent with a longer $\Delta t$, at the expense of temporally localized samples]{\includegraphics[height=6.25cm]{./images-tftheory/gabor2.png}}
	\caption{Time-frequency tradeoff intuitions (\cite[103, 106]{gabor2})}
	\label{fig:gaborfirst}
\end{figure}

The result of the time-frequency uncertainty principle is a consequence of how the Fourier Transform is used to swap between the mutually exclusive domains of time and frequency. To further elaborate that this is a property of the Fourier Transform and not a fundamental physical limitation, several psychacoustic studies have shown that humans can exhibit better time-frequency resolution than the Gabor limit. \textcite[610]{psycho2} describes one of these experiments:

\begin{quote}
	It is concluded that models based on a place (spectral) analysis should be subject to a limitation of the type $\Delta f \cdot d \ge \text{constant}$, where $\Delta f$ is the frequency difference limen (DL) for a tone pulse of duration d. [...]  It was found that at short durations the product of $\Delta f$ and d was about one order of magnitude smaller than the minimum predicted [...]
\end{quote}

More recently, according to \textcite[4]{psycho1}:

\begin{quote}
	[w]e have conducted the first direct psychoacoustical test of the Fourier uncertainty principle in human hearing, by measuring simultaneous temporal and frequency discrimination. Our data indicate that human subjects often beat the bound prescribed by the uncertainty theorem, by factors in excess of 10.
\end{quote}

\textcite[4]{psycho1} go on to state that ``most sound analysis and processing tools today continue to use models based on spectral theories... [w]e believe it is time to revisit this issue.'' Nevertheless, if we choose to proceed with time-frequency analysis despite this limitation, it is preferable to minimize time-frequency uncertainty, or to set the \textit{logon} ($\Delta t \Delta f$) to its lowest possible value. \textcite[435]{gabor1946} asks:

\begin{quote}
What is the shape of the signal for which the product $\Delta t \Delta f$ actually assumes the smallest possible value? [... it is] the modulation product of a harmonic oscillation of any frequency with a pulse of the form of the probability function
\end{quote}

Gabor performed joint time-frequency analysis by multiplying overlapping, temporally consecutive portions of the input signal with shifted copies of the Gaussian window function (i.e. the probability function), and by taking the Fourier Transform of the windowed segments of the signal. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described by equation \eqref{equation:gabort}:
\begin{flalign}\tag{7}\label{equation:gabort}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)], G_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n}
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform (\cite{dictionary}).

The STFT, or Short-Time Fourier Transform, has been described independently from Gabor's work (\cite{stftindie}), but additional research in the 1980s (\cite{dictionary}) led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described by equation \eqref{equation:stft}:
\begin{flalign}\tag{8}\label{equation:stft}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)], X_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n}
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window (\cite{dictionary}). Note how similar equations \eqref{equation:gabort} and \eqref{equation:stft} are, which is expected since the original Gabor transform is the STFT with a Gaussian window. In practice, the STFT allows the use of different windows and overlap sizes (\cite{stftinvertible}), as long as the COLA (constant overlap-add) constraint is respected (\cite{cola}). Figure \ref{fig:stftdiagram} shows how a windowed Fourier Transform (i.e. Gabor transform or STFT) is performed on a waveform.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/stft_diagram.png}
	\caption{Diagram showing the forward and inverse STFT or Gabor transform\protect\footnotemark}
	\label{fig:stftdiagram}
\end{figure}

\footnotetext{\url{https://www.mathworks.com/help/signal/ref/iscola.html}}

Figure \ref{fig:gabortf} shows different sizes of \textit{logon} in the time-frequency plane, and how the Gabor transform and the STFT with higher frequency or higher time resolution appear on the time-frequency plane.

\begin{figure}[ht]
	\centering
	\subfloat[Pure time domain, pure frequency domain, and Gabor's time-frequency tiles of $\Delta t \Delta f = 1$]{\includegraphics[height=3.4cm]{./images-tftheory/gabor3.png}}\\
	\subfloat[High frequency resolution vs. high time resolution]{\includegraphics[height=3.2cm]{./images-tftheory/gabor4.png}}
	\caption{Different tiling of the time-frequency plane (\cite[326, 327]{gabordiagrams})}
	\label{fig:gabortf}
\end{figure}

Figure \ref{fig:stfts} shows the Gabor transform alongside the STFT using a popular choice of window, the Hamming window, for three different window sizes: 128 samples, 2048 samples, and 16384 samples, which at the sample rate of the glockenspiel signal (44100 Hz) represent $\sfrac{128}{44100}\cdot 1000 = $ 2.9 ms, 46.44 ms, and 371.52 ms respectively. The Hamming window was chosen because it is the default window in the MATLAB \Verb#spectrogram# function.\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} The Gaussian and Hamming windows are shown together in figure \ref{fig:gaussvshamm}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/hamming_vs_gauss.png}
	\caption{2048-sample Hamming and Gaussian windows}
	\label{fig:gaussvshamm}
\end{figure}

\newpagefill

\begin{figure}[ht]
	\centering
	\subfloat[STFT, 128-sample Hamming]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_hamm_128.png}}
	\hspace{0.1em}
	\subfloat[Gabor transform, 128-sample Gaussian]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_gauss_128.png}}\\
	\subfloat[STFT, 2048-sample Hamming]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_hamm_2048.png}}
	\hspace{0.1em}
	\subfloat[Gabor transform, 2048-sample Gaussian]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_gauss_2048.png}}\\
	\subfloat[STFT, 16384-sample Hamming]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_hamm_16384.png}}
	\hspace{0.1em}
	\subfloat[Gabor transform, 16384-sample Gaussian]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_gauss_16384.png}}
	\caption{Visual comparison of the magnitude STFT spectrograms with the Hamming and Gaussian window}
	\label{fig:stfts}
\end{figure}

We close this section with a quote from \textcite[96]{doerflersouls} on the powerful properties of the STFT:

\begin{quote}
       ... the STFT has at least three souls: it is the Fourier Transform of windowed portions of the signal, it is the convolution of the signal with modulated versions of the window and it is the scalar product of the signal with time-shifted modulated versions of the window. These three souls can be exploited in the applications, for the computation of the STFT and for its sampling ...
\end{quote}

\newpagefill

\subsubsection{Constant-Q Transform (CQT)}
\label{sec:cqt}

\textcite[1]{cqtransient} stated that the problem with the STFT is its ``rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies.'' As described in section \ref{sec:motivation}, music should be analyzed with a high frequency resolution in the low-frequency region, and with a high time resolution in the high-frequency region (\cite{doerflerphd, cqtransient}). 

The Constant-Q Transform was proposed by \textcite{jbrown, msp} to analyze musical signals with a logarithmic frequency scale to show the relationship between the fundamental frequency of a musical instrument and its harmonics better than the Fourier Transform's uniform frequency spacing. A demonstration of a violin analyzed with the DFT and CQT was shown in figure \ref{fig:violin}.

The name ``Constant-Q'' refers to the constant ratio of the frequency being analyzed to the frequency resolution of analysis, or $\sfrac{f}{\delta f} = Q$, such that the frequency resolution increases with the center frequency to maintain the $Q$ factor. The transform uses long-duration windows in the low frequency regions and short-duration windows in the high frequency regions, resulting in good time resolution for transients (\cite{cqtransient}). Figure \ref{fig:jbrowncqt} shows some properties of the linearly-spaced DFT spectrum compared to the Constant-Q Transform, and the different window sizes used for each frequency bin of interest. Equation \eqref{equation:jbrowncqt} describes how the different windows $N[k]$ are applied to the signal:
\begin{align}\tag{9}\label{equation:jbrowncqt}
	\nonumber N[k] \text{(window length)} &= \frac{f_{s}}{f_{k}}Q, W[k, n] = \alpha + (1 - \alpha)\cos\big(\frac{2\pi n}{N[k]}\big)
\end{align}

\begin{figure}[ht]
	\centering
	\subfloat[Properties of DFT, CQT]{\includegraphics[height=5.05cm]{./images-tftheory/dftvcqt.png}}
	\hspace{0.5em}
	\subfloat[First 12 window sizes for the CQT]{\includegraphics[height=5cm]{./images-tftheory/qwindowchanges.png}}
	\caption{Various aspects of the original CQT (\cite[427, 428]{jbrown})}
	\label{fig:jbrowncqt}
\end{figure}

This first CQT implementation was not designed to be invertible, until \textcite{klapuricqt} introduced an algorithm for an approximate inverse of the CQT back to the time-domain waveform, with an inversion error of $\approx 10^{-3}$. The approximate inverse for the CQT was also approached differently by \textcite{fitzgeraldcqt}.

\textcite[1]{klapuricqt} describe the CQT as ``a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal.'' Frequency bins that are geometrically spaced are also logarithmically spaced with a base of two (\cite{geometriclog}). The bins-per-octave setting of the CQT is related to the Constant-Q ratio by the formula $Q = 2^{\sfrac{1}{\text{bins}}}$. For example, for 12 bins-per-octave, this results in a $Q$ factor of 1.059, which is the well-known ``12th root of two'' based on the Western chromatic scale with equal temperament (\cite{westernpitch1, westernpitch2}).

\textcite{invertiblecqt} implemented the Constant-Q Transform with Nonstationary Gabor frames (\cite{balazs}), which allowed for perfect reconstruction for the first time. This is also called the CQ-NSGT, or Constant-Q Nonstationary Gabor Transform. The next section, \ref{sec:theorynsgt}, will cover the NSGT in detail, but for now we shall continue with how the NSGT led to improved implementations of the CQT.

\textcite{slicq} followed up with a realtime algorithm, the \textit{sliCQ} or ``sliced Constant-Q'' transform (sliCQT), which can process the input signal in fixed-size slices, as opposed to operating on the entire input signal like the NSGT. Finally, \textcite{variableq1} introduced the Variable-Q scale and improved the phase of the transform. From \textcite{invertiblecqt}, the total frequency bins of the CQ-NSGT or sliCQT can be derived from the bins-per-octave, and the minimum and maximum frequencies of the desired frequency scale. The formula given is $K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}}) + 1]$, where $K$ is the total bins of the CQT, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. For example, for $B = 12 \text{ bins-per-octave}$, $\xi_{\text{min}} = 83 \text{ Hz}$ and $\xi_{\text{max}} = 22050 \text{ Hz}$, the result is $K \approx 97 \text{ total frequency bins}$.

In the landscape of music analysis libraries, Essentia,\footnote{\url{https://essentia.upf.edu/}} LTFAT,\footnote{\url{https://ltfat.org/}} and the MATLAB Wavelet Toolbox\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} have converged on the CQ-NSGT (\cite{invertiblecqt, slicq, variableq1}). However, librosa\footnote{\url{https://librosa.org/}} still uses the older implementation with the approximate reconstruction (\cite{klapuricqt}). Finally, \textcite{invertiblecqt} have released an open-source reference Python implementation\footnote{\url{https://github.com/grrrr/nsgt}} of the CQ-NSGT and the sliCQT.

Various plots of STFT and CQT spectrograms are shown in figure \ref{fig:cqtvstft}, generated from the standard STFT spectrogram\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} and the CQ-NSGT implementation of the CQT from the MATLAB Wavelet Toolbox.

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt12.png}}
	\hspace{0.1em}
	\subfloat[STFT spectrogram, window size 256]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft_256.png}}\\
	\subfloat[CQT spectrogram, 24 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt24.png}}
	\hspace{0.1em}
	\subfloat[STFT spectrogram, window size 1024]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft_1024.png}}\\
	\subfloat[CQT spectrogram, 48 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt48.png}}
	\hspace{0.1em}
	\subfloat[STFT spectrogram, window size 4096]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft_4096.png}}
	\caption{Spectrograms of the glockenspiel signal, showing how the CQT displays more musical information than the STFT for all frequency resolutions}
	\label{fig:cqtvstft}
\end{figure}

\newpagefill

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

\textcite{doerflerphd} in their dissertation analyzed music with multiple Gabor dictionaries (i.e. STFTs). Figure \ref{fig:dorflertradeoff} shows the time-frequency tradeoff of the short and long window STFT analyses of the glockenspiel signal for its transient and tonal characteristics, and how two Gabor dictionaries combined gives us the ability to analyze either the transient or tonal aspects with a more appropriate resolution.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal (\cite[20]{doerflerphd})}
	\label{fig:dorflertradeoff}
\end{figure}

As summarized by \textcite[2]{adaptivecqt}, ``[t]he definition of multiple Gabor frames, which is comprehensively treated in [D{\"o}rfler 2002], provides Gabor frames with analysis techniques with multiple resolutions... The nonstationary Gabor frames [...] are a further development; [...] they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas.'' In other words, a single Nonstationary Gabor transform can replace the need for multiple distinct Gabor transforms (or STFTs).

\textcite[1049]{dictionary} describe the shift from the term \textit{transform}, e.g., the Gabor transform or STFT, to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began a ``fundamental move from transforms to dictionaries for [...] signal representation.'' Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task.''

Using multiple transforms, such as using two Gabor transforms (or STFTs) with a small and large window for the transient and tonal properties of music, is also called an \textit{overcomplete} dictionary (\cite{dictionary}), where there is a lot of redundancy in the transform domain.

The advantage of the CQT over such approaches is that it contains these desirable transform properties in one single transform. The NSGT is a time-frequency transform with varying time-frequency resolution, whose motivating application is the CQT (\cite{jaillet, balazs}). It is constructed from frame theory (\cite{frametheory}), which is a mathematical technique for computing redundant, stable ways of representing a signal (\cite{framesintro}). The NSGT can use nonuniform time and frequency spacing in its time-frequency analysis of a signal, and it has a perfect inverse operation, such that there is practically no reconstruction error.

The construction of nonstationary Gabor frames relies on three properties of the windows and time-frequency shift parameters used (\cite{balazs}):
\begin{tight_itemize}
	\item
		The signal $f$ of interest is localized at time- (or frequency-) positions $n$ by means of multiplication with a compactly supported (or limited bandwidth, respectively) window function $g_{n}$
	\item
		The Fourier Transform is applied on the localized pieces $f \cdot g_{n}$. The resulting spectra are sampled densely enough in order to perfectly re-construct $f \cdot g_{n}$ from these samples
	\item
		Adjacent windows overlap to avoid loss of information. At the same time, unnecessary overlap is undesirable. We assume that $0 < A \le \sum_{n \in \mathbb{Z}}|g_{n}(t)|^{2} \le B < \infty$, a.e. (almost everywhere), for some positive A and B
\end{tight_itemize}

These requirements lead to invertibility of the frame operator and therefore to perfect reconstruction. \textcite[1482]{balazs} continues on to say that

\begin{quote}
	[m]oreover, the frame operator is diagonal and its inversion is straightforward. Further, the canonical dual frame has the same structure as the original one. Because of these pleasant consequences following from the three above-mentioned requirements, the frames satisfying all of them will be called painless nonstationary Gabor frames and we refer to this situation as the painless case
\end{quote}

To derive the NSGT, let us recall the previously-seen definition of the Gabor transform from section \ref{sec:jointtfa}. In the standard Gabor transform, the same window function (also called the Gabor atom or Gabor function) is shifted in time to cover entire signal (\cite{adaptivecqt}), described by equation \eqref{equation:stationarygab}:
\begin{align} \tag{10}\label{equation:stationarygab}
g_{m, n}(t) = g(t - na)e^{2\pi i m b t}
\end{align}

As stated by \textcite[3]{adaptivecqt}, ``[w]e will indicate such a frame as \textit{stationary}, since the window used for time-frequency shifts does not change and the time-frequency shifts form a lattice of $a \times b$.'' Figure \ref{fig:uniformtflattice} shows the resulting uniform $a \times b$ tiling of the time-frequency plane.

\begin{figure}[ht]
	\centering
	\includegraphics[width=6.25cm]{./images-tftheory/stationarygabor.png}
	\caption{Uniform time-frequency resolution of the stationary Gabor transform (\cite[3]{adaptivecqt})}
	\label{fig:uniformtflattice}
\end{figure}

We can also describe the multiple Gabor systems of \textcite{doerflerphd} with the following set of equations, continuing from the stationary Gabor transform. Given the stationary Gabor atom, where $a$ and $b$ are the time-frequency shift parameters and $f(t)$ is the reconstructed input signal, the Gabor transform is described by the equations in \eqref{equation:onegabor}.
\begin{align}
	\nonumber g_{m,n}(t) &= g(t - na)e^{j2\pi m b t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{m,n \in \mathbb{Z}}c_{m,n}g_{m,n}(t) \tag{12}\label{equation:onegabor}
\end{align}

Similarly, we can describe an overcomplete system that uses $R$ distinct STFTs (or Gabor transforms) with different window sizes, where $a_{r}$ and $b_{r}$ are the time-frequency shift parameters for each window size, with the equations in \eqref{equation:multigabor}:
\begin{align}
	\nonumber g_{m,n}^{r}(t) &= g(t - na_{r})e^{j2\pi m b_{r} t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{r=0}^{R-1}\sum_{m,n \in \mathbb{Z}}c^{r}_{m,n}g^{r}_{m,n}(t) \tag{13}\label{equation:multigabor}
\end{align}

For a resolution that changes with time, the Nonstationary Gabor atom is chosen from a set of functions $\{g_{n}\}$ and a fixed frequency sampling step $b_{n}$:
\begin{align}\tag{14}\label{equation:irregulartime}
	g_{m,n}(t) &= g_{n}(t)e^{j2\pi m b_{n}t}, m,n \in \mathbb{Z}
\end{align}

\textcite[1485]{balazs} describe the system in equation \eqref{equation:irregulartime} further:

\begin{quote}
		[...] the functions $\{g_{n}\}$ are well-localized and centered around time-points $a_{n}$. This is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{n}$ for each position $a_{n}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position.
\end{quote}

For a resolution that changes with frequency, the Nonstationary Gabor atom is chosen from a family of functions $\{h_{m}\}$, which are `well-localized band-pass functions with center frequency $b_{n}$'' and a fixed time sampling step $a_{m}$ (\cite[1486]{balazs}):
\begin{align}\tag{15}\label{equation:irregularfrequency}
	h_{m,n}(t) = h_{m}(t - na_{m}), m,n \in \mathbb{Z}
\end{align}

Figure \ref{fig:nonuniformtflattices} show both the cases of the varying resolution by time and by frequency in terms of sampled points on the time-frequency grid.

\begin{figure}[ht]
	\centering
	\subfloat[Gabor atoms that change with time]{\includegraphics[width=6.25cm]{./images-tftheory/irregulartime.png}}
	\hspace{1em}
	\subfloat[Gabor atoms that change with frequency]{\includegraphics[width=6.25cm]{./images-tftheory/irregularfrequency.png}}
	\caption{Varying time-frequency sampling of the Nonstationary Gabor transform (\cite[1485, 1487]{balazs})}
	\label{fig:nonuniformtflattices}
\end{figure}

The NSGT coefficients for a signal of length $L$ are given by an FFT of length $M_{n}$ for each window $g_{n}$. For each window, there are $L$ window operations and $O(M_{n} \cdot \log(M_{n}))$ FFT operations. The overall algorithmic complexity of the NSGT is therefore $O(N \cdot (M \log(M)))$ for $N$ windows.

We believe that the general NSGT is more interesting to study than the CQ-NSGT (Constant-Q NSGT), as it can be constructed with nonuniform frequency scales besides the Constant-Q scale, such as the psychoacoustic mel and Bark scales (\cite{melbook}), or the Variable-Q scale based on the psychoacoustic ERBlet transform (\cite{variableq1, variableq2}).

Figure \ref{fig:bunchansgts} shows several different configurations of the NSGT of the glockenspiel signal, to demonstrate the diversity of the transform. The plots are generated from my fork\footnote{\url{https://github.com/sevagh/nsgt}} of the reference Python NSGT library. My modifications to the library will be described further in section \ref{sec:methodology}. A high and low frequency resolution NSGT, using 100 and 500 total frequency bins respectively, is generated for each of the following 4 frequency scales: mel, Bark, Constant-Q, and Variable-Q, using a frequency range of 20--22050 Hz. For the Variable-Q scale, the fixed frequency offset applied is 15 Hz.  Details of the frequency scales used to generate the NSGT spectrograms are shown in table \ref{table:nsgtfreqsandqs}.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, Constant-Q scale, 100 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_cqlog_100.png}}
	\hspace{0.1em}
	\subfloat[NSGT, Constant-Q scale, 500 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_cqlog_500.png}}\\
	\subfloat[NSGT, Variable-Q scale, 100 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_vqlog_100.png}}
	\hspace{0.1em}
	\subfloat[NSGT, Variable-Q scale, 500 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_vqlog_500.png}}\\
	\subfloat[NSGT, mel scale, 100 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_mel_100.png}}
	\hspace{0.1em}
	\subfloat[NSGT, mel scale, 500 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_mel_500.png}}\\
	\subfloat[NSGT, Bark scale, 100 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_bark_100.png}}
	\hspace{0.1em}
	\subfloat[NSGT, Bark scale, 500 bins]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_bark_500.png}}\\
	\caption{Different NSGT spectrograms of the glockenspiel signal}
	\label{fig:bunchansgts}
\end{figure}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & First 5 frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 20.00, 21.47, 23.04, 24.73, 26.54 \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 35.00, 36.47, 38.04, 39.73, 41.54 \\
	 \hline
	 mel & 100 & 20.00, 45.56, 72.02, 99.42, 127.80 \\
	 \hline
	 Bark & 100 & 20.00, 45.88, 71.85, 97.96, 124.24 \\
	 \hline
	 Constant-Q & 500 & 20.00, 20.28, 20.57, 20.86, 21.16 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 35.00, 35.28, 35.57, 35.86, 36.16 \\
	 \hline
	 mel & 500 & 20.00, 25.00, 30.03, 35.10, 40.21 \\
	 \hline
	 Bark & 500 & 20.00, 25.13, 30.26, 35.40, 40.54 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & Last 5 frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 16614.38, 17832.63, 19140.20, 20543.64, 22050.00 \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 16629.38, 17847.63, 19155.20, 20558.64, 22065.00 \\
	 \hline
	 mel & 100 & 19087.44, 19789.79, 20517.07, 21270.17, 22050.00 \\
	 \hline
	 Bark & 100 & 18558.87, 19376.12, 20229.33, 21120.07, 22050.00 \\
	 \hline
	 Constant-Q & 500 & 20845.91, 21140.62, 21439.50, 21742.61, 22050.00 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 20860.91, 21155.62, 21454.50, 21757.61, 22065.00 \\
	 \hline
	 mel & 500 & 21428.92, 21582.58, 21737.31, 21893.11, 22050.00 \\
	 \hline
	 Bark & 500 & 21308.75, 21491.70, 21676.21, 21862.31, 22050.00 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & First 5 Q factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 12.37, 12.00, 11.67, 11.35, 11.06 \\
	 \hline
	 mel & 100 & 0.40, 0.88, 1.34, 1.78, 2.21 \\
	 \hline
	 Bark & 100 & 0.39, 0.89, 1.38, 1.87, 2.35 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 62.33, 61.96, 61.59, 61.23, 60.87 \\
	 \hline
	 mel & 500 & 2.01, 2.49, 2.97, 3.45, 3.92 \\
	 \hline
	 Bark & 500 & 1.95, 2.45, 2.95, 3.45, 3.94 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & Last 5 Q factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 7.07, 7.07, 7.07, 7.07, 7.07 \\
	 \hline
	 mel & 100 & 13.83, 13.85, 13.86, 13.88, 13.89 \\
	 \hline
	 Bark & 100 & 11.60, 11.60, 11.60, 11.60, 11.60 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 35.64, 35.64, 35.64, 35.64, 35.64 \\
	 \hline
	 mel & 500 & 69.97, 69.98, 70.00, 70.02, 70.03 \\
	 \hline
	 Bark & 500 & 58.49, 58.49, 58.49, 58.49, 58.49 \\
	 \hline
\end{tabular}
	\caption{Different frequencies and Q factors for various NSGT scales, 20-22050 Hz}
	\label{table:nsgtfreqsandqs}
\end{table}

\newpagefill

\subsubsection{sliCQ Transform (sliCQT)}
\label{sec:theoryslicqt}

The NSGT processes the entire input signal at once. In cases where the input signal must be processed in fixed-size chunks, such as realtime streaming, the sliCQ Transform (sliCQT, or sliced Constant-Q Transform) was created (\cite{invertiblecqt, slicq}). The slicing operation is shown in figure \ref{fig:slicqtukeys}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.55\textwidth]{./images-misc/slicq_windows.png}
	\caption{Slicing the input signal with 50\% overlapping Tukey windows. N is the slice length and M is the transition area (\cite{slicq})}
	\label{fig:slicqtukeys}
\end{figure}

Slices have a 50\% overlap with adjacent slices. The noninvertibility of the half-overlap-add makes it only useful as a forward operation. Figure \ref{fig:slicqoverlaps} demonstrates this issue. This trait of the sliCQT has also been mentioned by in its implementation in the Essentia project:\footnote{\url{https://mtg.github.io/essentia-labs/news/2019/02/07/invertible-constant-q/}}

\begin{quote}
	[...] we have to overlap-add the spectrograms obtained for each frame. Note that it is not possible to synthesize the audio from this overlapped version as we cannot retrieve the analysis frames from it.
\end{quote}

\begin{figure}[ht]
	\centering
	\subfloat[Adjacent slices placed side-by-side]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_overlap_flatten.png}}
	\hspace{0.1em}
	\subfloat[Adjacent slices with 50\%-overlap-add]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_overlap_proper.png}}
	\caption{sliCQT spectrograms demonstrating the slice half-overlap}
	\label{fig:slicqoverlaps}
\end{figure}

\newpagefill

\subsection{Machine learning and deep learning for music signals}
\label{sec:ml}

Music signals are a form of acoustic signal, and the domain of music signal processing is inseparable from the larger field of signal processing (\cite{musicsp}). Signal processing encompasses the subfields of statistical, approximate, or stochastic signal processing (\cite{stochasticsp, statisticalsp}). The field of optimization has been used for approximate signal processing problems for decades (\cite{optsp}). Some of the classic examples include basis pursuit and matching pursuit (\cite{dictionary1, dictionary2}). To consider the problem of optimization outside of the signal processing domain, we refer to the description of machine learning by \textcite[105]{introtoml}:

\begin{quote}
	In many scientific disciplines, the primary objective is to model the relationship between a set of observable quantities (inputs) and another set of variables that are related to these (outputs). [...] Machine learning provides techniques that can automatically build a computational model of these complex relationships by processing the available data and maximizing a problem dependent performance criterion.
\end{quote}

According to \textcite[1]{introtodl}, deep learning ``is the subfield of machine learning that is devoted to building algorithms that explain and learn a high and low level of abstractions of data that traditional machine learning algorithms often cannot.'' A characteristic of deep networks is that they have many hidden layers, named so ``because we do not necessarily see what the inputs and outputs of these neurons are explicitly beyond knowing they are the output of the preceding layer,'' (\cite[2]{introtodl}) to model more complex relationships between the input and output. Figure \ref{fig:fcdn} shows a deep network with hidden layers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-neural/dnn.png}
	\caption{Deep neural network with hidden layers (\cite[2]{introtodl})}
	\label{fig:fcdn}
\end{figure}

\textcite[1]{introtodl} go on to state that ``most machine learning algorithms as they exist now focus on function optimization, and the solutions yielded do not always explain the underlying trends within the data nor give the inferential power that artificial intelligence was trying to get close to.'' However, machine learning methods have achieved success in many fields including natural language processing (\cite{nlpml}), computer vision (\cite{cvml}), and audio (\cite{audiodeeplearning}), indicating that data-driven approaches are useful, despite falling short of general artificial intelligence (\cite{generalai}).

We also note that statistical signal processing techniques and machine and deep learning are compatible with one another and can be used together (\cite{mlsp1, mlsp2}). In fact, according to \textcite[3591]{mldspmix}, analytic knowledge and machine learning should both be considered in the study of acoustic signals:

\begin{quote}
	Whereas physical models are reliant on rules, which are updated by physical evidence (data), ML is purely data-driven. By augmenting ML methods with physical models to obtain hybrid models, a synergy of the strengths of physical intuition and data-driven insights can be obtained.
\end{quote}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-neural/dspmlmix.jpg}
	\caption{Mixed analytic and data-driven approaches (\cite[3591]{mldspmix})}
	\label{fig:dspmlmix}
\end{figure}

In machine learning,  the input data is typically split into a training set and a test set (\cite{introtoml}). The model makes predictions with its initial parameters using the training input. It measures how correct its prediction was by using a loss function to compare its predicted output to the training (or ground truth) output. Contemporary machine learning is dependent on the field of optimization (\cite{boyd2004convex, mlopt1, mlopt2}) for the loss function and parameter updates (\cite{sgd}). The performance of the network is measured by its ability to generalize on the test set, which was data not seen during training. There is sometimes a third validation set, which is also unseen during training and used to perform hyperparameter tuning (\cite{splitvaliddata}) which are the parameters of a machine learning model set by the user (\cite{introtodl}). The user tries to maximize the performance of the model on the validation set by modifying the hyperparameters.

\textcite{audiodeeplearning} describe contemporary deep learning architectures that are used for audio applications. They state that models from the field of computer vision, originally designed for 2D images where pixels are related spatially, may not exactly fit audio waveforms where amplitude values are related temporally. However, convolutional neural networks, recurrent neural networks, and sequence-to-sequence models are three popular types of network architecture that have been adapted with varying levels of success to the audio and music domains (\cite{audiodeeplearning}).

\newpagefill

\subsubsection{Convolutional Neural Networks (CNN)}

Convolutional neural networks (CNN) ``are based on convolving their input with learnable kernels'' (\cite[3]{audiodeeplearning}). These are useful for both images and audio according because of their ability to exploit spatial or temporal correlation in data (\cite{cnns}). Figure \ref{fig:cnnbasic} shows how a learnable convolution kernel \textit{slides} over the pixels of the input image.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[width=0.48\textwidth]{./images-neural/sliding_conv_1.png}}
	\hspace{0.15em}
	\subfloat{\includegraphics[width=0.48\textwidth]{./images-neural/sliding_conv_2.png}}
	\caption{$3 \times 3$ convolution kernel sliding over patches of input pixels\protect\footnotemark}
	\label{fig:cnnbasic}
\end{figure}

\footnotetext{\url{https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks}}

The network architecture and feature map for an audio CNN is shown in figure \ref{fig:audiocnn}, where the extracted feature map is visualized in the spectral domain.

\begin{figure}[ht]
	\centering
	\subfloat[CNN architecture for audio waveforms ({\cite[7]{audiocnn2}})]{\includegraphics[width=0.65\textwidth]{./images-neural/audio_cnn.png}}\\
	\subfloat[Audio spectral features learned by each layer ({\cite[4]{audiocnn3}})]{\includegraphics[width=0.65\textwidth]{./images-neural/audio_cnn2.png}}
	\caption{Example of an audio CNN architecture and spectral feature maps}
	\label{fig:audiocnn}
\end{figure}

Finally, we note that for audio applications, both 1D convolutions (also referred to as temporal convolutions) applied directly to the audio waveform and 2D convolutions applied to time-frequency transforms (most commonly the STFT) are both used (\cite{tcn, 2dconv}).

\subsubsection{Recurrent Neural Networks (RNN)}

For time series data, for which the audio waveform is an exemplar given the temporal evolution of its amplitude, recurrent neural networks (RNNs) are useful since the input sequence of data is introduced back into the network with a cyclic or recurrent connection, and outputs are predicted based on past (or future, if the network is bi-directional) values of the sequence (\cite{rnns}). RNNs are also used commonly in sequence-to-sequence, or seq2seq, models (\cite{seq2seqs}).

In the audio noise suppression model RNNoise,\footnote{\url{https://jmvalin.ca/demo/rnnoise/}} the usefulness of the RNN and its LSTM (\cite{lstm1}) and GRU (\cite{gru1}) variants for audio are described:

\begin{quote}
	Recurrent neural networks (RNN) are very important [...] because they make it possible to model time sequences instead of just considering input and output frames independently. [...] RNNs were heavily limited in their ability because they could not hold information for a long period of time [...] [These] problems were solved by the invention of gated units, such as the Long Short-Term Memory (LSTM), the Gated Recurrent Unit (GRU), and their many variants.
\end{quote}

Figure \ref{fig:rnndiags} shows some example RNN architectures, as well as an illustration of the additional complexity in the gated LSTM and GRU units, which allow them to surpass the simple RNN.

\begin{figure}[ht]
	\centering
	\subfloat[Regular RNN with cyclic connections to past data ({\cite[3]{birnn}})]{\includegraphics[width=0.47\textwidth]{./images-neural/simple_rnn.png}}
	\hspace{0.1em}
	\subfloat[Bi-directional LSTM with cyclic connections to past and future data ({\cite[3]{birnn}})]{\includegraphics[width=0.47\textwidth]{./images-neural/birnn.png}}\\
	\subfloat[Different recurrent units ({\cite[7]{lstmrnngru}})]{\includegraphics[width=0.68\textwidth]{./images-neural/gates.png}}
	\caption{RNN diagrams}
	\label{fig:rnndiags}
\end{figure}

\newpagefill

\subsection{Software and code concepts}
\label{sec:softcode}

\subsubsection{Python programming language}

Python\footnote{\url{https://www.python.org/}} is a general-purpose programming language. It is an interpreted language,\footnote{\url{https://www.python.org/doc/essays/blurb/}} which means that there is no compilation step required, and the Python code or script written by a user can be executed right away with the Python interpreter. The Python interpreter can also run statements directly for quick prototyping, without needing to write a script:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
Python 3.9.6 (default, Jul 16 2021, 00:00:00)
[GCC 11.1.1 20210531 (Red Hat 11.1.1-3)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
>>> print('this is python')
this is python
\end{BVerbatim}
\end{listing}

The Python interpreter is widely distributed on most modern operating systems (OS) like Linux, Windows, and OS-X. Python has support for various high-level and user-friendly data structures. Python has seen widespread adoption in academia, and especially in numerical contexts (\cite{pythonscience}). Important concepts from the Python language that appear repeatedly in this thesis will be described in this section.

The first is the list,\footnote{\url{https://docs.python.org/3/tutorial/datastructures.html\#more-on-lists}} which is a core datastructure of the language. The list in Python corresponds to the array data structure in computer science (\cite{skiena}). It allows for the construction of an ordered list of objects. For example, a discrete-time speech signal may be described by a list of its amplitude values, where the position in the list is the time sample $n$:

\hfil \Verb#speech_signal = [0.15, 0.28, 0.57, 0.98, -0.59]#

The generator is a function in Python which generates the next value of a sequence whenever it is called with the \Verb#yield# statement.\footnote{\url{https://docs.python.org/3/reference/simple_stmts.html\#the-yield-statement}} The internal state of the sequence is managed by the generator function. Generators are used for efficiency, where instead of pre-computing the output of some expensive function for the entire set of values, the expensive function is only executed when a single value is demanded at a time. This is also called lazy evaluation.\footnote{\url{https://cvw.cac.cornell.edu/python/lazy}} An example of a generator is shown in listing \ref{lst:siggen}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
>>> def speech_signal():
...     samples = [0.15, 0.28, 0.57, 0.98, -0.59]
...     for s in samples:
...             yield expensive_function(s)
...
>>> s = speech_signal()
>>> print(s)
<generator object speech_signal at 0x7f642933fa50>
# N.B. expensive function has not been executed yet
>>>
>>> for sample in s:
...     print(sample) # expensive function is computed on demand
\end{BVerbatim}
	\caption{A generator in Python, where the expensive function is only called when a value is requested}
	\label{lst:siggen}
\end{listing}

The decorator in Python is a single line with a leading \Verb#@# symbol, added on top of a function, to wrap the execution of the function with another.\footnote{\url{https://docs.python.org/3/glossary.html\#term-decorator}} A simple example of a decorator is shown in listing \ref{lst:decorator}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
>>> def custom_decorator(func):
...     def wrapper():
...             print('before')
...             func()
...             print('after')
...     return wrapper
...
>>> @custom_decorator
... def print_hello():
...     print('hello')
...
>>>
>>> print_hello()
before
hello
after
\end{BVerbatim}
	\caption{A decorator in Python}
	\label{lst:decorator}
\end{listing}

Decorators are often used to simplify tasks like measuring a function's execution time, or CPU and memory footprint, which is intended to observe or measure but not modify the original function.\footnote{\url{https://pypi.org/project/line-profiler/, https://pypi.org/project/memory-profiler/}}

The \Verb#random# module of Python contains a random number generator (RNG).\footnote{\url{https://docs.python.org/3/library/random.html}} The \Verb#seed# function initializes the random number generator,\footnote{\url{https://docs.python.org/3/library/random.html\#bookkeeping-functions}} which is in fact pseudorandom (\cite{pseudorng}). Pseudorandom means that given the same starting seed, the exact same sequence of numbers are generated deterministically. The seed is usually a parameter to a script or Python program, such that the user can recreate the same RNG sequences for testing purposes by using the same seed.

\subsubsection{Numerical and machine learning libraries for Python}

Python has a rich ecosystem of academic libraries. NumPy\footnote{\url{https://numpy.org/}} and SciPy\footnote{\url{https://scipy.org/}} are used for numerical computation and parallelized matrix operations that run on the CPU (central processing unit). Matplotlib\footnote{\url{https://matplotlib.org/}} is a plotting library. Tensorflow\footnote{\url{https://www.tensorflow.org/}} and PyTorch\footnote{\url{https://pytorch.org/}} are libraries for machine learning and deep learning, which also have numerical computation and parallelized matrix operations that are similar to NumPy and SciPy, except that they can run on the GPU (graphical processing unit). The use of the GPU allows for more efficient, faster, and larger parallelized matrix operations, which are essential for modern machine learning and deep learning techniques.

In NumPy, the ndarray\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html}} is the core datastructure representing an n-dimensional array of objects, which can be numerical (such as int16 or float32) or general objects (such as strings). The same speech signal represented by the Python list above can be represented by a 1-dimensional ndarray of 64-bit floating-point values:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
>>> import numpy
>>> speech_signal = numpy.asarray([0.15, 0.28, 0.57, 0.98, -0.59])
>>> print(speech_signal.dtype)
float64
>>> print(speech_signal.shape)
(5,)
\end{BVerbatim}
\end{listing}

The underlying values are stored in formats that allow for efficient numerical computations, which is why using NumPy ndarrays for numerical computation is preferred to using regular Python datastructures such as lists that are not designed for speed (\cite{ndarrayfast}).

In PyTorch and Tensorflow (and in general machine learning), a similar concept to the ndarray is the tensor. The tensor originates from the field of physics (\cite{whatistensor}). The tensor is also an n-dimensional numerical array, and NumPy ndarrays are interchangeable with both PyTorch and Tensorflow tensors.\footnote{\url{https://www.tensorflow.org/guide/tf_numpy}, \url{https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html}}

Tensors can be used for numerical computations using the GPU in both PyTorch and TensorFlow, much like the ndarray with NumPy and SciPy on the CPU. For example \Verb#torch.cos()# computes the cosine of a tensor. Additionally and most importantly, the inputs and outputs to the machine learning and deep learning constructs supported by these libraries are represented as tensors.

\subsubsection{Package managers for Python}

To manage third-party libraries and packages for Python, a package manager is suggested. The built-in way of managing a sandboxed Python environment is the virtual environment.\footnote{\url{https://docs.python.org/3/tutorial/venv.html}} This creates a clean, isolated copy of Python where the user can install packages without risking the stability of the operating system (OS). The Python dependency manager tool, pip,\footnote{\url{https://pip.pypa.io/en/stable/cli/pip_install/\#requirements-file-format}} can be used to install a list of third-party packages. Packages must be published by the authors and maintainers on the public Python Package Index (PyPi).\footnote{\url{https://pypi.org/}} An example of a pip file, conventionally called \Verb#requirements.txt#, is shown in listing \ref{lst:reqtxt}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
mir_eval==0.6
tabulate==0.8.7
numpy==1.19.4
\end{BVerbatim}
	\caption{Example pip requirements.txt file}
	\label{lst:reqtxt}
\end{listing}

There is another popular Python dependency manager called Conda.\footnote{\url{https://docs.conda.io/en/latest/}} Conda environments are also popular for creating reproducible Python environments for academic software. Conda supports channels for third-party package authors to publish and distribute their packages to users, similar to PyPi.\footnote{\url{https://conda.io/projects/conda/en/latest/user-guide/concepts/channels.html}} An example Conda environment file is shown in listing \ref{lst:condayml}. Conda files can also support the pip syntax, since pip is a native Python tool. The user can combine the best of both worlds by using Conda and pip syntax simultaneously in their Conda environment file.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
name: test-conda-env

channels:
  - default

dependencies:
  - python=3.9
  - cudatoolkit=11
  - pip
  - pip:
    - norbert>=0.2.0
\end{BVerbatim}
	\caption{Example Conda environment.yml file}
	\label{lst:condayml}
\end{listing}

Pip is installed by default with Python, making it the simpler choice, while Conda is more powerful but needs to be installed separately in the user's OS. In different parts of the code accompanying this thesis, both pip and Conda files are used, and they will be shown to help the readers replicate the necessary Python environments.

\subsubsection{Version control systems and git}

Version control systems (VCS) are systems used for tracking and managing changes to code files, and storing the history of changes in a database (\cite{gitbook}). \textcite[1]{gitbook} state that the benefits of using a VCS is that it ``allows you to revert selected files back to a previous state, revert the entire project back to a previous state, compare changes over time, see who last modified something that might be causing a problem, who introduced an issue and when, and more. Using a VCS also generally means that if you screw things up or lose files, you can easily recover.''

According to \textcite{gitbook}, version control systems are either centralized or distributed. In a distributed version control systems (DVCS), each person's copy of the code contains the full history of changes. This means that people can work on their own private copy of the code, and then sync changes with a central server, rather than depending on the central server for viewing the history like in a centralized version control system (CVCS). This allows people to work independently of the central server, until they are ready to sync changes back with the other collaborators.

Git is a popular tool for DVCS written by Linus Torvalds, who is also known for having created Linux. How Git works is that it ``thinks of its data more like a series of snapshots of a miniature filesystem... every time you commit, or save the state of your project, Git basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot'' (\cite[6]{gitbook}). To work on a code project that uses git, first the project, or \textit{repository}, must be cloned to your local system. You make local changes and then commit them. Finally, you can push your commits back to the central server to share with the other collaborators, often with a descriptive message describing what you changed. Figure \ref{fig:git} shows how git tracks a file and how a git workflow should look.

\begin{figure}[ht]
	\centering
	\subfloat[How git stores snapshots of files]{\includegraphics[height=3.3cm]{./images-misc/git.png}}
	\hspace{0.1em}
	\subfloat[Recommended git workflow]{\includegraphics[height=3.3cm]{./images-misc/git2.png}}
	\caption{How git works (\cite[6, 8]{gitbook})}
	\label{fig:git}
\end{figure}

All of the code for this thesis were stored in git repositories to keep track of historical changes. It conveniently allows the author to reference the git history to describe the evolution of the project and the incremental progress.

\newpagefill

\subsubsection{Open-source software and GitHub}

\textcite[1]{floss} state that ``free and open source software (FOSS) is any computer program released under a licence that grants users rights to run the program for any purpose, to study it, to modify it, and to redistribute it in original or modified form.'' Free and open-source software is a merging of two distinct ideas: free software\footnote{\url{https://www.fsf.org/}} and open-source software.\footnote{\url{https://opensource.org/}} While these have nuanced differences when it comes to commercial licensing, code visibility, and availability, in simplistic terms FOSS implies code that is free and open for users to read and modify. Releasing code as open-source is becoming more popular in academia, with initiatives like Papers With Code\footnote{\url{https://paperswithcode.com/}} and the Journal of Open Source Software.\footnote{\url{https://joss.theoj.org/}} According to \textcite{floss}, FOSS software is a natural complement to academic publications, given concerns with reproducibility of results.

\textcite[131]{gitbook} describe GitHub\footnote{\url{https://github.com/}} as ``the single largest host for Git repositories, and [... a] central point of collaboration for millions of developers and projects.'' GitHub contains tools for working with git repositories, including social elements for creating personal profiles or organizations. Open-source projects can be made public on GitHub such that anybody can read and download the source code. For a project hosted on GitHub, there is a file browser built into the website which allows one to view the code and change history of a project. In this thesis, the GitHub code browser will be linked wherever appropriate to show code written for the thesis or code from third-party projects, such that the reader can view the true source code that is being described. An example of the GitHub code browser showing a file from the PyTorch project's git repository\footnote{\url{https://github.com/pytorch/pytorch}} is shown in figure \ref{fig:githubpytorch}.

\begin{figure}[ht]
	\centering
	\includegraphics[height=6cm]{./images-misc/github.png}
	\caption{GitHub's code browser showing a file from the PyTorch source code}
	\label{fig:githubpytorch}
\end{figure}

\newpagefill

\subsection{Music source separation}
\label{sec:musicsep}

\subsubsection{Task motivation and definition}

Typical music recordings are mono or stereo mixtures, with multiple sound objects (drums, vocals, etc.) sharing the same track (\cite{musicsepintro1}). To manipulate the individual sound objects, the stereo audio mixture needs to be separated into a track for each different sound source, in a process called audio source separation.

There are many motivations for performing this separation. \textcite[1]{musicsepgood} describe that once we perform audio source separation and have access to the individual source tracks, we can ``remix the balance within the music [...] to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1-
channel surround sound system... We might also want to change the spatial location of a musical instrument within the mix.''

The ISMIR 2021 Music Demixing Challenge (MDX)\footnotemark{} describes audio source separation as consisting of different subtasks, including ``music source separation systems [that] take a song as input and output one track for each of the instruments,'' and ``speech enhancement systems [that] take noisy speech as input and separate the speech content from the noise.'' The competition description mentions important uses for music demixing systems, ranging from entertainment to hearing aids:

\begin{quote}
	For example, the original master of old movies contains all the material (dialogue, music and sound effects) mixed in mono or stereo: thanks to source separation we can retrieve the individual components and allow for up-mixing to surround systems. [...] Karaoke systems can benefit from the audio source separation technology as users can sing over any original song, where the vocals have been suppressed, instead of picking from a set of ``cover'' songs specifically produced for karaoke.
\end{quote}

A common thread in survey papers is that the domains of speech enhancement and music demixing are both mentioned as important subproblems of audio source separation of (\cite{musicsepintro1, musicmask}). Music demixing is the focus of this thesis, shown in figure \ref{fig:mixingdiagrams}.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[height=4cm]{./images-mss/mixdemix.png}}
	\caption{Diagram for music mixing and demixing\protect\footnotemark[\value{footnote}]}
	\label{fig:mixingdiagrams}
\end{figure}

\footnotetext{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}}

\newpagefill

\subsubsection{Computational approaches}

Computational source separation has a history for at least 50 years (\cite{musicmask, musicsepintro1}), originating from the tasks of computational auditory scene analysis (CASA) and blind source separation (BSS). In CASA and BSS, the mixed audio contains unknown sources that must be separated. In music demixing, the sources are typically known; most commonly, 4 sources (vocals, drums, bass, other) are used as defined by the MUSDB18 dataset (\cite{musdb18}).

Among the earliest computational approaches for blind source separation is Independent Component Analysis (ICA)  (\cite{musicmask, musicsepgood, musicsepintro1}), which exploits spatial information of the sources, and assumes the sources to be independent. This technique can be used when there are as many channels in the mixture (corresponding to differently placed microphones) as the number of sources. We refer the readers to \textcite{ica1, ica2} for ICA algorithms, and to \textcite{blind1, blind2} for a more in-depth review on the history of blind source separation. Figure \ref{fig:icaposition} shows an example of the positional considerations in a typical ICA system.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/positional.png}
	\caption{Position-based source separation (\cite[9]{musicsepgood})}
\label{fig:icaposition}
\end{figure}

According to \textcite[1]{musicsepintro1}, ICA techniques arose more typically for speech denoising (\cite{speechsep}), and they make assumptions that cannot be generalized easily to music:

\begin{quote}
	[systems which aim] to recover clean speech from noisy recordings [...] can be seen as a particular instance of source separation. [...] many algorithms assume the audio background can be modeled as stationary. However, the musical sources are characterized by a very rich, nonstationary spectrotemporal structure. This prohibits the use of such methods. Musical sounds often exhibit highly synchronous evolution over both time and frequency, making overlap in both time and frequency very common. Furthermore, a typical commercial music mixture violates all the classical assumptions of ICA. Instruments are correlated (e.g., a chorus of singers), there are more instruments than channels in the mixture, and there are nonlinearities in the mixing process (e.g., dynamic range compression).
\end{quote}

Techniques more specific to music were developed as a necessity to deal with these differences (\cite{musicseptechniques1, musicseptechniques2}).

For cases where ICA cannot be applied, musical source models are more popular (\cite[11]{musicsepgood}), which are ``model-based approaches that attempt to capture the spectral characteristics of the target source can be used.'' Sources are assumed to be sufficiently different from each other, or sparse in their spectral representation (\cite{musicsepgood}), such that they can be extracted with time-frequency masks applied in the spectral domain. Figure \ref{fig:sepgood} shows how different sources have unique spectral patterns.

\begin{figure}[ht]
	\centering
	\subfloat[Mixed spectrogram]{\includegraphics[height=4.75cm]{./images-mss/mss1.png}}
	\subfloat[Source spectrograms]{\includegraphics[height=5cm]{./images-mss/mss2.png}}
	\caption{Sparsity of different music sources in the spectral domain (\cite[1, 3]{musicsepgood})}
\label{fig:sepgood}
\end{figure}

Kernel Additive Modeling (KAM) is the simplest form of music source modeling (\cite{musicsepgood}). To estimate a music source at a given time-frequency point, KAM ``select[s] a set of time-frequency bins, which, given the nature of the target source, e.g., percussive, harmonic or vocals, are likely to be similar in value. This set of time-frequency bins is termed a proximity kernel'' (\cite[11]{musicsepgood}). A well-known example of a KAM-based music separation algorithm is the median-filtering Harmonic/Percussive Source Separation (HPSS) algorithm (\cite{fitzgerald1}). Figure \ref{fig:fitzhpss} shows the result of HPSS.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65\textwidth]{./images-misc/hpss.png}
	\caption{Median-filtering HPSS\protect\footnotemark}
	\label{fig:fitzhpss}
\end{figure}

\footnotetext{\url{https://librosa.org/librosa_gallery/auto_examples/plot_hprss.html}}

Spectrogram factorization models are more sophisticated than KAM, and the most popular spectrogram factorization model is Nonnegative Matrix Factorization (NMF) (\cite{musicmask, musicsepgood}). According to \textcite[12]{musicsepgood}, NMF ``attempts to factorize a given nonnegative matrix into two nonnegative matrices,'' and it can be applied to the nonnegative magnitude spectrogram of the mix, $M$, to separate it into frequency weight matrices $W$ and time activation matrices $H$; a survey on NMF techniques by \textcite{nmfpaper} covers the algorithm in more detail. Most recently, data-driven approaches based on machine learning and deep learning have significantly surpassed past approaches (\cite{musicsepgood, sisec2018}). Figure \ref{fig:spectraldemix} shows different techniques for spectral demixing.

\begin{figure}[ht]
	\centering
	\subfloat[KAM vs. NMF for spectral music demixing]{\includegraphics[width=0.7\textwidth]{./images-mss/kamvnmf.png}}\\
	\subfloat[Deep neural networks for spectral music demixing]{\includegraphics[width=0.7\textwidth]{./images-mss/mssdnn.png}}
	\caption{Different techniques for spectral music demixing (\cite[12, 14]{musicsepgood})}
	\label{fig:spectraldemix}
\end{figure}

\newpagefill

\subsubsection{Time-frequency masking and oracle estimators}
\label{sec:masksandoracles}

Surveys on speech (\cite{speechmask}) and music separation (\cite{musicmask}) indicate that the majority of source separation algorithms use the technique of time-frequency masking (or spectral masking) to separate the sources.  \textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, or its real-valued magnitude, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins.

A soft mask, or ratio mask, has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only 0 and 1. To compute a binary mask, there must be an additional real-valued parameter, $\theta \in [0.0, 1.0]$, which is the separation factor -- values below $\theta$ are set to 0, and values above $\theta$ are set to 1. According to \textcite{masking}, soft masks are generally produce a higher quality of sound. An illustration of spectral masking is shown in figure \ref{fig:masks}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{./images-mss/maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising (\cite[71]{masking})}
	\label{fig:masks}
\end{figure}

The oracle mask, or oracle estimator, is a perfect time-frequency mask which is computed from ground-truth data. The use of the oracle estimator is to give an idea of the upper limit of audio quality from an algorithm or machine learning model for source separation or demixing. Typically, the mask is computed from and applied to the magnitude spectrograms (\cite{fitzgerald1, fitzgerald2, driedger, umx, plumbley1, plumbley2}). Discarding the phase of the complex STFT is a choice made for simplicity, but the phase may be important for source separation applications (\cite{ditchphase}).

To illustrate the calculation of the oracle mask, let us describe a simple case of a mixed song consisting of a vocal and drum track. We assume that we have access to the isolated source recordings of vocals and drums, as well as the mix.

We start by denoting the waveforms $x_{v}[n]$ and $x_{d}[n]$ for the isolated vocal and drum tracks respectively. The mixed song is defined by the waveform $x_{m}[n] = x_{v}[n] + x_{d}[n]$. To apply the music demixing task to this waveform, let us say that we want our algorithm or network to take the mixed waveform $x_{m}[n]$ as an input, and estimate the two source waveforms of vocals $\hat{x}_{v}[n]$ and drums $\hat{x}_{d}[n]$.

Given the following operations to get the STFTs of the mix and two sources, we seek to calculate various interesting oracle masks:
\begin{align}
	\nonumber X_{m} &= \text{STFT}(x_{m}[n]), |X_{m}| = \text{abs}(X_{m})\\
	\nonumber X_{v} &= \text{STFT}(x_{v}[n]), |X_{v}| = \text{abs}(X_{v})\\
	\nonumber X_{d} &= \text{STFT}(x_{d}[n]), |X_{d}| = \text{abs}(X_{d})
\end{align}
 
\textcite{sisec2018} report the performance of 5 oracles, called IRM1, IRM2, IBM1, IBM2, and the MWF (multichannel Wiener filter). The first 4 are more relevant to stereo music, which is the case considered in this thesis. The ``I'' is for Ideal, ``R|B'' denotes a Ratio vs. Binary mask, ``M'' is for Mask, and the trailing number is the $p$th power which the magnitude spectrogram is raised to. For example, the magnitude spectrogram raised to the 1st power remains unchanged, while one raised to the 2nd power is the power spectrogram.

The oracles for the vocal source (and the same equations apply to the drum track) are computed thus, noting that a typical default value for $\theta$ is 0.5:
\begin{align}
	\nonumber \text{IRM1}_{v} &= \frac{|X_{v}|^{1}}{|X_{m}|^{1}}\text{,\qquad}
	\nonumber \text{IRM2}_{v} = \frac{|X_{v}|^{2}}{|X_{m}|^{2}}\\
	\nonumber \text{IBM1}_{v} &= \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} \ge \theta
	\end{cases},
	\nonumber \text{IBM2}_{v} = \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} \ge \theta
	\end{cases}
\end{align}

To estimate the time domain waveform from an oracle mask, the complex STFT of the mixed waveform, $X_{m}$, is multiplied by the mask, and the resultant complex STFT is inverted back to the time-domain waveform:
\begin{align}
	\nonumber \hat{X}_{v\text{, IRM1}} &= X_{m} \cdot \text{IRM1}_{v}, \hat{x}_{v\text{, IRM1}} = \text{iSTFT}(\hat{X}_{v\text{, IRM1}})
\end{align}

\newpagefill

\subsubsection{Open-Unmix (UMX) and CrossNet-Open-Unmix (X-UMX)}
\label{sec:umx}

Open-Unmix (UMX) was created by \textcite{umx} as an open-source, reference implementation of a state-of-the-art deep neural network for music source separation. Open-Unmix is based on STFT masking, and is intended to foster reproducible research by only using the open MUSDB18 dataset for training data (\cite{musdb18, musdb18hq}).

In UMX, a deep neural network is used to estimate the magnitude spectrograms of the sources from an input mixed song. The architecture is a variant of a sequence2sequence model. Open-Unmix uses a bidirectional LSTM or Bi-LSTM architecture, which is based on two predecessor networks by \textcite{umxorig1} and \textcite{umxorig2}. Figure \ref{fig:umxes} shows the architecture of UMX and one of its predecessors.

\begin{figure}[ht]
	\centering
	\subfloat[Open-Unmix Bi-LSTM architecture\protect\footnotemark]{\includegraphics[width=0.8\textwidth]{./images-neural/umx.png}}\\
	\subfloat[Simple DNN architecture for MSS ({\cite[262]{umxorig1}})]{\includegraphics[width=0.8\textwidth]{./images-neural/typical_simple_mss_dnn.png}}
	\caption{Architecture of Open-Unmix and one of its simpler predecessors}
	\label{fig:umxes}
\end{figure}

\footnotetext{\url{https://github.com/sigsep/open-unmix-pytorch\#-the-model-for-one-source}}

\textcite{xumx} combined the 4 separate target networks into a single model to apply the loss functions and optimizations jointly across all 4 targets, rather than optimizing each one separately. The model is named CrossNet-Open-Unmix (X-UMX). It also includes loss computed from the time-domain waveforms, in addition to the loss measured from the magnitude spectral coefficients.

\subsubsection{Convolutional denoising autoencoders}

While the discussed Open-Unmix model is based on sequence2sequence ideas with a bidirectional LSTM architecture, a different class of neural network called convolutional autoencoders, or convolutional denoising autoencoders (CDAE), have been seeing increasing use in music demixing (\cite{plumbley1, plumbley2}). The architectures are shown in figure \ref{fig:cdaes}, where 2D convolutions (in the time and frequency dimensions) are applied on the magnitude STFT.

\begin{figure}[ht]
	\centering
	\subfloat[Simple CDAE ({\cite[2]{plumbley1}})]{\includegraphics[width=\textwidth]{./images-neural/cdae_1.png}}\\
	\subfloat[CDAE with time and frequency filters that vary by frequency band ({\cite[2]{plumbley2}})]{\includegraphics[width=0.7\textwidth]{./images-neural/cdae_2.png}}
	\caption{Convolutional architectures for music source separation}
	\label{fig:cdaes}
\end{figure}

We will note that each of the papers gives us different ideas with how to adapt STFT-based convolutional models to fit the sliCQT. The model introduced in \textcite{plumbley1} shows a simple case of a 2D time and frequency filter applied to the STFT, which can be applied to any time-frequency transform. The model introduced in \textcite{plumbley2} varies the size of time and frequency filter to fit the Constant-Q Transform, which as discussed in section \ref{sec:cqt} analyzes music with higher frequency resolution in the low frequency region, and higher time resolution in the high frequency region.

\subsubsection{Public datasets}

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset (\cite{musdb18}), and more recently the HQ (high-quality) version (\cite{musdb18hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets (\cite{sisec2016, otherdataset2}).

The songs in the MUSDB18-HQ dataset have a fixed train, validation, and test split. Following the rules defined in the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} for a network to be considered trained only on MUSDB18-HQ, the predefined data splits must be used.

\subsubsection{Evaluation measures}
\label{sec:evalbss}

The SigSep\footnote{\url{https://sigsep.github.io/}} community, borrowing from the methodology of Signal Separation Evaluation Campaign (SISEC), uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality. There are 4 distinct metrics that comprise BSS:

\begin{tight_itemize}
\item
	\textbf{ISR:} source Image to Spatial distortion Ratio
\item
	\textbf{SIR:} Signal to Interference Ratio
\item
	\textbf{SAR:} Signal to Artifacts Ratio
\item
	\textbf{SDR:} Signal to Distortion Ratio
\end{tight_itemize}

Out of these 4 scores, SDR is the single global score which is commonly used to summarize the overall performance of a music demixing system (\cite{sdruseful}). The SDR as it was defined in the ISMIR 2021 Music Demixing Challenge (and used to rank the participants) can be computed from the following equation \eqref{equation:sdrinstr}:
\begin{align}
	\nonumber & \text{SDR}_{\text{instr}} = \\
	&10 \log_{10}\frac{\sum_{n}\big(s_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n)\big)^{2}}{\sum_{n}\big(s_{\text{instr, left}}(n) - \hat{s}_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n) - \hat{s}_{\text{instr, right}}(n)\big)^{2}} \tag{10}\label{equation:sdrinstr}
\end{align}

The unit of the SDR score is in decibels or dB, $s_{\text{instr}}(n)$ denotes the ground truth waveform of the instrument, $\hat{s}_{\text{instr}}(n)$ is the estimate, and left and right refer to the two channels in the stereo dataset of MUSDB18-HQ. Given the four stems of MUSDB18-HQ -- vocals, drums, bass, other -- the SDR is computed for each stem from the equation \eqref{equation:sdrinstr} above. Then, the four SDR scores are combined for a total song score in equation \eqref{equation:sdrsong}:
\begin{align}
	\text{SDR}_{\text{song}} = \frac{1}{4}(\text{SDR}_{\text{bass}} + \text{SDR}_{\text{drums}} + \text{SDR}_{\text{vocals}} + \text{SDR}_{\text{other}}) \tag{11}\label{equation:sdrsong}
\end{align}

In the 2018 Signal Source Separation Evaluation Compaign (SiSEC), \textcite{sisec2018} introduced an evolution of BSS metrics, called BSS v4. BSS v4 uses time-invariant distortion filters to reduce the computational cost over the original BSS metrics used in SiSec 2016 (\cite{sisec2016}). The new BSS v4 metrics were also made available in the Python libraries museval\footnote{\url{https://github.com/sigsep/museval}} and bsseval,\footnote{\url{https://github.com/sigsep/bsseval}} which were used to calculate the results in this thesis.

\subsubsection{Open-source ecosystem}

There are several groups of researchers who created open-source ecosystems on the code sharing platform GitHub to provide tools, tutorials, and implementations for source separation. In particular, the SigSep organization\footnote{\url{https://github.com/sigsep}} contains many tools useful for music demixing research, including the reference implementation of Open-Unmix (\cite{umx}), MUSDB18 dataset loaders (\cite{musdb18, musdb18hq}), and BSS evaluation metrics (\cite{bss, sisec2018})

The author of this thesis owes a debt of gratitude to these projects. Without such community-focused efforts, the field of music demixing would be less approachable for beginners.

\end{document}
