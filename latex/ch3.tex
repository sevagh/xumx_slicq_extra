\documentclass[report.tex]{subfiles}
\begin{document}

\section{Methodology}
\label{sec:methodology}

\todo[inline]{apply ich corrections of ch3: section introductions, etc.: in this section..., will be seen in the next section..., etc., delete code snippets if they dont help}

The proposed adaptation of Open-Unmix to use the sliCQ Transform (sliCQT) is named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ}} and is the subject and main result of this thesis. xumx-sliCQ is pronounced like ``X-U-M-X-slice-Q.''

Decisions for the methodology were influenced by the limitations of the computer that xumx-sliCQ was developed on, e.g., the maximum GPU memory available. Refer to appendix \ref{appendix:computerspec} for the hardware and software specifications of the computer.

An overview of Open-Unmix was given in section \ref{sec:umx}. xumx-sliCQ will be based on the Open-Unmix PyTorch template, and as a result, all of the code developed will be in Python, using PyTorch for the deep learning network to run on the GPU. The rest of this chapter will be focused on the Python and PyTorch implementation details of xumx-sliCQ.

Figure \ref{fig:generalmdx} shows a block diagram for a general DNN music demixing system that uses the magnitude spectrogram as  its input and output representation, and uses the phase of the mixture to swap back to the time domain. This strategy was described in section \ref{sec:noisyphaseoracle}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-blockdiagrams/generic_mdx.png}
	\caption{General DNN music demixing system with magnitude spectrograms, single target}
	\label{fig:generalmdx}
\end{figure}

Figure \ref{fig:umxandxumxslicq} shows how UMX and xumx-sliCQ are variants of the above general system, and how they differ from each other.

\begin{figure}[ht]
	\centering
	\subfloat[UMX system, single target]{\includegraphics[width=\textwidth]{./images-blockdiagrams/umx_clean.png}}\\
	\subfloat[xumx-sliCQ, single target]{\includegraphics[width=\textwidth]{./images-blockdiagrams/xumx_slicq_clean.png}}
	\caption{UMX and xumx-sliCQ compared}
	\label{fig:umxandxumxslicq}
\end{figure}

Figure \ref{fig:xumxcl} shows how independent UMX networks for the desired four targets (vocals, drums, bass, other) are combined together in X-UMX, illustrating the key concepts of the multi-domain loss functions (MDL), which computes loss for both the magnitude spectrograms and time-domain waveforms, and the combination losses across the four targets (CL) as described in section \ref{sec:xumx}.

\begin{figure}[ht]
	\centering
	\subfloat[UMX network loss, single target]{\includegraphics[width=\textwidth]{./images-blockdiagrams/umx_single_target.png}}\\
	\subfloat[X-UMX network loss, all four targets]{\includegraphics[width=\textwidth]{./images-blockdiagrams/xumx_multiple_targets.png}}
	\caption{UMX and X-UMX compared}
	\label{fig:xumxcl}
\end{figure}

Section \ref{sec:inputrepresentation} will cover the replacement of the STFT with the sliCQT in the input representation. Next, section \ref{sec:neuralnet} will show why and how the Bi-LSTM RNN architecture of UMX is replaced with a CDAE CNN architecture in xumx-sliCQ, and the combining of the four independent networks per target into a single network like X-UMX. Finally, section \ref{sec:postprocessing} will show how the post-processing Wiener-EM step is adapted for the sliCQT.

\subsection{Input representation}
\label{sec:inputrepresentation}

In this section, we will describe the steps taken to use the sliCQT as the input representation in xumx-sliCQ.

First, section \ref{sec:improvelib} will show the addition of new frequency scales that are interesting for music applications that were described in section \ref{sec:fscales}. Recall from section \ref{sec:theorynsgt} that the NSGT/sliCQT can be thought of as filterbanks, and the chosen frequency scale is the most important parameter that defines the spectral representation of the musical signal, which influences the results of the downstream application.

Next, section \ref{sec:torchslicq} will show the modifications made to the library to compute the transform with PyTorch on the GPU, so that it can be used in the UMX neural network code. Following that, section \ref{sec:slicqtutil} will describe additional necessary utilities added to the PyTorch sliCQT, including an overlap-add utility as described in section \ref{sec:theoryslicqt}, as well as the related spectrogram plotting code.

Finally, in section \ref{sec:slicqparamsrch}, we describe the parameter search that was run to choose sliCQT parameters that may perform best in a music demixing application. Section \ref{sec:fasterbsscupy} describes modifications made to the BSS eval metrics library to make it run faster on the GPU, with the aim to speed up the parameter search process.

\subsubsection{New frequency scales in the sliCQT library}
\label{sec:improvelib}

In this section, we will discuss why we would like to add new frequency scales to the reference NSGT/sliCQT library.\footnote{\url{https://github.com/grrrr/nsgt/blob/master/nsgt/fscale.py}}

The frequency scale is the most important parameter of the NSGT and sliCQT. It defines the nonuniform frequency bands for audio analysis desired by the user. The choice of frequency scale affects the spectral representation of the music signal. The hypothesis of this thesis is that a spectrogram generated from a musical or auditory frequency scale may improve the results of the downstream music demixing system compared to the STFT, which suffers from the time-frequency tradeoff described in section \ref{sec:jointtfa}.

Section \ref{sec:theorynsgt} showed examples of NSGT spectrograms with the Constant-Q/logarithmic and mel scales, in Figures \ref{fig:bunchansgts1} and \ref{fig:bunchansgts2}. These are the scales provided in the reference NSGT/sliCQT library. There is also a provided octave scale, which is the same as the logarithmic scale, except that the octave scale takes a bins-per-octave (bpo) argument from which it computes the total number of frequency bins, instead of the logarithmic scale which takes the total number of frequency bins as a direct argument. Equation \eqref{equation:bpo1} describes how to compute the total bins from the bins-per-octave setting of the octave scale:

\begin{align}
	K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}} + 1]\tag{24}\label{equation:bpo1}
\end{align}

where $K$ is the total bins, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. This equation was shown previously in section \ref{sec:cqt}. The difference between the logarithmic and octave scales is shown in Code Listing \ref{code:octvlog}. The resulting frequency scale is plotted in Figure \ref{fig:octvlog}.

\begin{figure}[h]
  \centering
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{inputminted}[linenos,breaklines,frame=single,firstline=4,lastline=16,fontsize=\scriptsize]{text}{./scripts/fscale.py}
\end{inputminted}
 \subfloat{(a) Code snippet defining octave and log scales}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|\$ python scripts/fscale.py 3 # invoke script for bpo=3|
|bpo: 3, bins: 21, len(oct): 21, len(log): 21|
\end{minted}
 \subfloat{(b) Output printed by above}
 \end{minipage}
  \captionof{listing}{Octave and log scales for the NSGT}
  \label{code:octvlog}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/log_vs_oct.png}
	\caption{Frequency bins for the octave and log scales, annotated with their nearest musical note}
	\label{fig:octvlog}
\end{figure}

In section \ref{sec:fscales}, we discussed that the Variable-Q scale for music, and Bark psychoacoustic scales, are both of interest in music or auditory applications. Since we would like to find sliCQT parameters that perform best at music demixing, we would prefer a wider choice of frequency scales in our parameter search. For this reason, we are implementing, in addition to the Constant-Q and mel scales, the Variable-Q and Bark scales.

The time \todo{check this out} and frequency resolution of the NSGT and sliCQT in the reference library are defined from the frequency scale. The essential parameters to a frequency scale in the library are $f_{\text{min}}$ and $f_{\text{max}}$, which are the minimum and maximum frequency in Hz, and the total frequency bins. The type of scale determines how the frequency bins are distributed between $f_{min}$--$f_{\text{max}}$. A custom frequency scale may use additional parameters as needed, like the frequency offset parameter of the Variable-Q scale shown in section \ref{sec:fscales}.

To supplement the Constant-Q scales (octave and log), the Variable-Q scale was implemented.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L105}}

Figure \ref{fig:vq} shows the Constant-Q/log, and Variable-Q scales for different values of the frequency offset $\gamma$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/vqlog.png}
	\caption{Frequency bins and Q factors for the Constant-Q/log and Variable-Q scales}
	\label{fig:vq}
\end{figure}

The next frequency scale added was the Bark psychoacoustic scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L207}} to complement the included mel scale, as discussed in section \ref{sec:fscales}. Figure \ref{fig:melbarkfsandqs} shows the mel and Bark frequency scales and Q-factors from the NSGT library for 12 total mel or Bark bands.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/melbarkpitchesqs.png}
	\caption{Mel and Bark frequency curves and Q-factors from the NSGT library}
	\label{fig:melbarkfsandqs}
\end{figure}

In addition to the frequency scale, the user must specify the slice length and transition area for the sliCQT in samples, to support the desired time and frequency resolution dictated by the frequency scale. As mentioned in section \ref{sec:theoryslicqt}, these parameters of the sliCQT define how the input signal is processed in smaller-sized slices.

The NSGT library already contains code which warns the user if their chosen values for the slice length and transition area cannot support their desired frequency scale. The warning code was adapted to allow users to automatically choose an appropriate slice length and transition area for their frequency scale, so that users can use the sliCQT with fewer parameters.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L36}}

\subsubsection{Computing the sliCQT on the GPU with PyTorch}
\label{sec:torchslicq}

In this section, we will describe the procedure for porting the CPU-based code of the reference NSGT/sliCQT library to the GPU using PyTorch. This is needed to be able to use the sliCQT in the PyTorch GPU code of UMX. All the modifications to the library were made in my copy of the library,\footnote{\url{https://github.com/sevagh/nsgt}} and the same library was embedded inside the xumx-sliCQ code.\footnote{\url{https://github.com/sevagh/xumx-sliCQ}}.

The reference NSGT/sliCQT library uses NumPy, a CPU-based Python numerical computation library. PyTorch, in addition to being a deep learning framework, is also a numerical computation library that implements a large amount of NumPy's functions.

The line-profiler\footnote{\url{https://pypi.org/project/line-profiler/}} package for Python prints a line-by-line output of the execution times of Python functions that you annotate with \Verb#@profile#, and it was used to find performance hotspots and bottlenecks in the NSGT library. All NumPy functions and other non-GPU code was replaced with their PyTorch equivalents. To the user, the only difference was the addition of a \Verb#device# parameter to the \Verb#NSGT# and \Verb#NSGT_sliced# classes, which are the NSGT and sliCQT respectively. The choices of device are PyTorch device strings, and it defaults to the value \Verb#device="cpu"#. Using \Verb#device="cuda"# allows one to select their NVIDIA GPU. In practice, any device supported by PyTorch is also supported.

Many internal methods, functions, and utilities were modified during the port to PyTorch, and each in turn was also modified to take a \Verb#device# parameter. As these are internal to the library and not intended to be used directly by the end user, they won't be described here.

\subsubsection{Ragged and matrix forms of sliCQT}
\label{sec:slicqtshape}

After the port to PyTorch, the matrix form is a single tensor where each row represents a frequency bin, and the total number of columns represents the maximum time resolution from the highest frequency bin. Every lower frequency bin had zero-padding applied to have a row of time coefficients with the same length as the highest frequency bin. The non-matrix form could be referred to the ragged or jagged form, from its non-rectangular shape.

As was shown in Figure \ref{fig:raggedslicqt} in section \ref{sec:theorynsgt}, frequency bins that share the same time resolution are grouped together in a rectangular matrix. This results in a list of tensors or matrices, where each tensor contains a block of frequency bins that share the same time resolution, and thus have the same length of time coefficients.

The matrix form is created by zero-padding the tensors with a lower time resolution to fit the higher time resolution frequency bins. The ragged and zero-padded matrix forms are compared in Figure \ref{fig:raggedvszpad}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-blockdiagrams/slicq_zeropad.png}
	\caption{Ragged and zero-padded matrix forms of the sliCQT}
	\label{fig:raggedvszpad}
\end{figure}

In the upcoming section \ref{sec:slicqdim}, we will justify why the ragged form of the sliCQT was chosen instead of the zero-padded matrix form in the final neural network, xumx-sliCQ.

\subsubsection{Overlap-add and spectrogram utilities in the sliCQT library}
\label{sec:slicqtutil}

In section \ref{sec:theoryslicqt}, the sliCQT was described to have a 50\% overlap between adjacent slices, and that overlap-adding the slices was necessary to generate a good spectrogram. Figure \ref{fig:slicqtukeys} showed the difference between the overlap-added spectrogram and one where adjacent slices were simply joined without any overlap.

The next addition to the slice utilities was to include a utility function for the 50\% overlapping of adjacent slices,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/slicq.py\#L38}} where the overlap length is automatically inferred from the shape of the transform. The function is named \Verb#overlap_add_slicq# and is imported from the \Verb#nsgt.slicq# file. Figure \ref{fig:overlappedspectrograms} compares the non-sliced NSGT spectrogram with the overlap-added sliCQT spectrogram.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT (non-sliced), length is exact same as input signal. No overlap-add required]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_mel_nooverlap.png}}\\
	\subfloat[sliCQT with minimum automatic sllen (6,744 samples) and 50\% overlap-add]{\includegraphics[width=0.675\textwidth]{./images-gspi/gspi_nsgt_mel_perfect_slice.png}}
	\caption{NSGT spectrograms for the mel scale with 96 bins in 20--22,050 Hz}
	\label{fig:overlappedspectrograms}
\end{figure}

Due to necessity of the 50\% overlap to create a meaningful spectrogram, the overlap-add utility is accompanied with a spectrogram plotting implementation.  The spectrogram utility function\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/plot.py}} uses the matplotlib\footnote{\url{matplotlib.org/}} library functions pcolormesh\footnote{\url{https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pcolormesh.html}} and colorbar\footnote{\url{https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html}} in its implementation.

\newpagefill

\subsubsection{Choosing sliCQT parameters}
\label{sec:slicqparamsrch}

Mixed-phase inversion (MPI) is the music demixing strategy where the target magnitude spectrogram is combined with the phase of the mix spectrogram to create a time-domain target waveform estimate, as was described in section \ref{sec:noisyphaseoracle}. The MPI oracle represents the best possible result of the MPI strategy by using the ground truth target magnitude spectrogram. In this section, we will design a sliCQT parameter search to discover optimal sliCQT parameters that produce the best MPI oracle results. Our hypothesis is that using the optimal sliCQT parameters in xumx-sliCQ would produce the best final music demixing results.

As discussed in section \ref{sec:ml}, the validation split of a dataset is often used to tune hyperparameters, which are the parameters of a machine learning or deep learning network defined by the user. In UMX, the STFT window size is set to 4,096 samples by default (with a 1,024-sample overlap), and it is a hyperparameter of the network. Similarly in xumx-sliCQ, the sliCQT parameters are a hyperparameter. The MPI oracle testbench will use the 14 tracks of the validation set of MUSDB18-HQ.

According to \textcite{randomgrid}, 60 iterations of a random grid search will find a statistically good set of hyperparameters in a case where an exhaustive grid search is impossible. The strategy chosen was to choose random hyperparameters for 60 iterations and select the highest median SDR score of the MPI oracle waveforms for the four targets (drums, bass, vocals, other). We chose a random grid search because the space of all the possible NSGT/sliCQT parameters is too large for an exhaustive grid search.

The random search parameters are shown in Table \ref{table:slicqparams}. Several decisions were made to set limited ranges of the sliCQT parameters.

The maximum slice length was capped to 44,100 samples, which represents one second of music. The number was chosen because this maximum is often used in demixing papers \parencite{plumbley1, plumbley2, demucs}. The maximum bins were capped to 300. \todo{make this better} The maximum frequency was capped to 22,050 Hz, which is the the Nyquist rate of the 44,100 Hz sample rate of MUSDB18-HQ. The minimum frequency range (10.0--130.0 Hz) was inspired by one of the sliCQT papers \parencite{slicq} which demonstrated the sliCQT with those frequency ranges. The slice length and transition length were automatically picked for the randomly selected frequency scale, making use of the new feature whose addition was described in section \ref{sec:freqscales}.

\begin{table}[ht]
	\centering
	\caption{Parameter ranges for the sliCQT hyperparameter search}
	\label{table:slicqparams}
\begin{tabular}{ |l|l|l|l| }
	 \hline
	 Scale & Bins & Minimum frequency (Hz) & Additional params \\
	 \hline
	 \hline
	 Constant-Q/log & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Variable-Q & 10--300 & 10.0--130.0 & Offset = 10.0--130.0 Hz \\
	 \hline
	 mel & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Bark & 10--300 & 10.0--130.0 & n/a \\
	 \hline
\end{tabular}
\end{table}

The random parameter search selects the sliCQT parameters which achieve the median Signal-to-Distortion (SDR) score across the four targets of the 14 validation tracks of MUSDB18-HQ. Recall from section \ref{sec:evalbss} that the SDR is one of the scores of the BSS eval metrics, with a units of decibels (dB), which is commonly used as a single metric that summarizes the global performance of a music demixing system. The SDR was also the metric used to rank demixing systems in the ISMIR 2021 Music Demixing Challenge \parencite{mdx21}.\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}}

The performance of the MPI (mixed-phase inversion) and the IRM1 (soft magnitude mask) oracles will be reported for the best sliCQT parameters, alongside different STFT window sizes. The IRM1 oracle was described in section \ref{sec:masksandoracles}. The MPI and IRM1 oracles represent both possible configurations of UMX, which can use either the mixed-phase or ratio mask to compute the final waveform. The intent is to show how the window size and time-frequency resolution of the STFT and sliCQT affect music demixing results per target.

\subsubsection{Speeding up the parameter search using the GPU and CuPy}
\label{sec:fasterbsscupy}

Like in section \ref{sec:torchslicq}, the Python line-profiler library was used to discover any performance bottlenecks or slow parts of the MPI oracle parameter search script. The slowest step was the calculation of the SDR score from the BSS eval metrics library.\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} package. We would benefit from a speedup of the parameter search script to get results faster. For that reason we explored a speedup using the GPU.

The bottlenecks of the library use NumPy and SciPy operations, which are Python numerical computation libraries that use the CPU. The CuPy library\footnote{\url{https://cupy.dev/}} provides replacement functions of NumPy and SciPy which run on NVIDIA GPUs using the CUDA compute framework.\footnote{\url{https://developer.nvidia.com/cuda-toolkit}} This allows for acceleration of numerical operations through GPU parallelization. Note that although PyTorch was used to replace NumPy code in the earlier section \ref{sec:torchslicq}, CuPy is intended to be a direct replacement of NumPy and SciPy and thus represents an easier porting effort. Also, the MPI oracle testbench will not be used inside the xumx-sliCQ neural network, so it doesn't need to use PyTorch.

The replacement of the NumPy or SciPy function is as simple as using the CuPy equivalent, taking special care about GPU out-of-memory errors. The library is unchanged for end users, and the GPU is simply used whenever possible, falling back to the CPU in case of any errors, or if there is no GPU available on the user's computer. Similar to the NSGT/sliCQT library, the modifications made to the museval library are stored in my copy of the repository for future reference.\footnote{\url{https://github.com/sevagh/sigsep-mus-eval/commit/f3b7540faddc8d2b1bc91cecaf05fd20d96df7c6}}

\newpagefill

\subsection{Neural network}
\label{sec:neuralnet}

\todo[inline]{rewrite this}

First, we will look at incorporating CrossNet-Open-Unmix (X-UMX) ideas with Open-Unmix. Since X-UMX and UMX are both based on the STFT spectrogram, the general model will stay the same even if we replace the STFT with the sliCQT.

As mentioned, Open-Unmix has a reference PyTorch implementation\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} on which xumx-sliCQ, the neural network of this thesis, will be based. X-UMX has an open-source implementation available as well,\footnote{\url{https://github.com/sony/ai-research-code/tree/master/x-umx}} which uses NNabla,\footnote{\url{https://nnabla.org/}} a different Python deep learning framework. Therefore part of the methodology of this thesis involves incorporating the X-UMX ideas inside the PyTorch version of UMX.

After the X-UMX modifications are described, we will look at which parts of Open-Unmix need to incorporate the PyTorch implementation of the sliCQT that was described in section \ref{sec:torchslicq}.\footnote{\url{https://github.com/sevagh/nsgt}} Finally, we will show how we adapted the STFT-based convolutional denoising autoencoder (CDAE) model architectures that were described in section \ref{sec:cdae} to the sliCQT.

\subsubsection{sliCQT dimensionality analysis and neural network design choices}
\label{sec:slicqdim}

\todo[inline]{
Zero-pad: slicqt dimensionality analysis + matrixform fails inference on full tracks cause of its size, and MDX21 and UMX both use full tracks - important. print dims from script
use CDAE papers for low parameter
\url{https://vtechworks.lib.vt.edu/bitstream/handle/10919/98619/Messou_EJ_T_2020.pdf?sequence=1&isAllowed=y}
\url{https://arxiv.org/pdf/2010.02178.pdf}
\url{https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0}
\url{https://www.researchgate.net/publication/313732034_Monoaural_Audio_Source_Separation_Using_Deep_Convolutional_Neural_Networks}
}

In \ref{sec:slicqparamsrch}, it was

\subsubsection{Replacing the STFT with the sliCQT in UMX}
\label{sec:replacestft}

\todo[inline]{in section 2 whatever, we described umx}

These are are the places in the Open-Unmix PyTorch template where the STFT must be replaced with the sliCQT. Moreover, it was mentioned that the ragged sliCQT will be used, which is a list of time-frequency matrices for frequency bins that share the same time resolution, and not a single rectangular matrix like the STFT. This arises from the different temporal frame rates of each frequency bin which is the desire of the NSGT/sliCQT (or CQT). This means that all of the operations which expect a single rectangular real-valued matrix of the STFT returns need to be modified to work on a list of matrices instead.

Figure \ref{fig:umxandxumxslicq} showed the proposed modifications to the base UMX architecture to use the sliCQT in place of the STFT in the input representation.

In section \ref{sec:matrixvragged}, we saw that to use the ragged sliCQT one must simply loop over the list of matrices where each matrix (or block) contains at least one frequency bin that share a specific time-frequency resolution. A simple example of the STFT to sliCQT conversion is the custom MSE loss function\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/loss.py\#L8}} shown in Code Listing \ref{code:simplemse}.

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{python}
|def \_custom\_mse\_loss(pred\_magnitude, target\_magnitude):|
|    loss = 0|
|    for i in range(len(target\_magnitude)):|
|        loss += torch.mean((pred\_magnitude[i] - target\_magnitude[i])**2)|
|    return loss/len(target\_magnitude)|
\end{minted}
  \caption{Modified MSE loss for the ragged sliCQT}
  \label{code:simplemse}
\end{listing}

The MSE for the STFT is applied on the whole matrix. For the ragged sliCQT, we simply sum the MSE loss of each matrix in the list of matrices.

The \Verb#get_statistics# function\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/scripts/train.py\#L102}} for the data whitening step was modified to return a list of standard deviations and means, where each entry of the list corresponds to the standard deviation and mean of the respective matrix in the ragged sliCQT. Note that the sliCQT is overlap-added before computing its statistics. This change is shown in Code Listing \ref{code:fourthdiff}. Recall in sections \ref{sec:theoryslicqt} and \ref{sec:slicqola} that the 50\% overlap of adjacent slices was necessary for a meaningful spectrogram. The sliCQT is almost always overlap-added before being used in xumx-sliCQ.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{diff}{./diffs/diff4.txt}
\end{inputminted}
  \caption{Modifying the data whitening step of Open-Unmix}
  \label{code:fourthdiff}
\end{listing}

The neural network was modified to instantiate an inner network\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L150-L187}} for each matrix in the ragged sliCQT. The class name for the total neural network is called \Verb#OpenUnmix#. The inner network for each sub-matrix of the sliCQT is called \Verb#OpenUnmixTimeBucket#, referring to how each matrix is a ``bucket'' of frequency bins that share the same time resolution. Finally, the \Verb#DummyTimeBucket# class is a network that lets a single ragged sliCQT bucket pass through unchanged. This is to respect the \Verb#bandwidth# parameter of Open-Unmix (16,000 Hz by default), which lets the part of the STFT above 16,000 Hz to pass through the network unmodified. Similarly for the ragged sliCQT, all of the matrices whose first frequency bin is above 16,000 Hz passes through untouched. An excerpt of the neural network code is shown in Code Listing \ref{code:raggedumx}.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/openunmixbuckets.py}
\end{inputminted}
  \caption{OpenUnmix with a network per time bucket in the ragged sliCQT}
  \label{code:raggedumx}
\end{listing}

Within each sub-network, or \Verb#OpenUnmixTimeBucket#, the implementation is similar to the STFT-based Open-Unmix. The $i$th block from the list of magnitude sliCQT $|X|$ of the mixed-song waveform $x$ , or $|X_{i}|$, is the input to the network, and it is overlap-added to produce a 2D time-frequency matrix. Then, the 2D time-frequency matrix can be used just like an STFT, as an input to an LSTM or any other neural network layer. An excerpt of the neural network code is shown in Code Listing \ref{code:raggedumxoneblock}.  A necessary consideration in the network design is the non-invertibility of the overlap-add procedure, since the original transform shape is needed to synthesize an audio waveform. The neural network needs a strategy to compute a non-overlap-added estimate from the overlap-added mixed input spectrogram. This was achieved with an additional transpose convolutional layer, which will be covered in greater detail in the next section \ref{sec:convlayers}.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/openunmixbucket.py}
\end{inputminted}
  \caption{Single network for one time bucket from the ragged sliCQT}
  \label{code:raggedumxoneblock}
\end{listing}

\subsubsection{Defining the convolutional layers of the CDAE}
\label{sec:convlayers}

The convolutional denoising autoencoder (CDAE) models of \textcite{plumbley1, plumbley2} were used in xumx-sliCQ. In the early experiments, they showed more promising results than the bi-LSTM model. The bi-LSTM model was first adapted to work on each time bucket of the ragged list of the sliCQT, but the longest time resolution (highest frequency bin) was prohibitively slow in both training and inference. Convolutional layers are a simple way of defining 2D time-frequency filters that slide over the 2D time-frequency matrices that correspond to different time buckets of the ragged sliCQT. The diagram in Figure \ref{fig:cdaeslicqt} shows how the convolutional layers should be applied to the ragged sliCQT.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-blockdiagrams/xumx_slicq_pertarget_largefont.png}
	\caption{Block diagram of the individual CNNs of xumx-sliCQ}
	\label{fig:cdaeslicqt}
\end{figure}

Using the values from \textcite{plumbley2}, the two layers of convolution in the CDAE of xumx-sliCQ use 25 and 55 channels respectively. This means that the stereo or 2-channel sliCQT has 25 output feature maps (or channels) computed from the first encoder convolutional layer, and 55 output feature maps from the second encoder convolutional layer. These are reversed in the decoder layer i.e., $2 \rightarrow \text{encoder}(25 \rightarrow 55) \rightarrow \text{decoder}(25 \rightarrow 2)$.

Different buckets in the ragged transform may have different dimensions for time and frequency, so the 2D convolution filter sizes are different depending on the input size. The training input is split into one second sequences, which is the same sequence size used in both CDAE papers \parencite{plumbley1, plumbley2}. This means that the ragged sliCQT dimensions (frequency bins and time coefficients) for one second of audio dictate the maximum size of time and frequency kernel that can be applied in each convolution layer. After each layer, a 2D batch normalization (BN) and nonlinear activation function, specifically the rectified linear unit (ReLU) activation, are applied \parencite{plumbley2}.

Note that after the encoder-decoder layers for the sliCQT, we need to add a ``growth layer.'' This is the inverse of the slice overlap-add procedure with learnable parameters. It uses a transpose convolution layer with a kernel size of $(1, 3)$ and a stride of $(1, 2)$ where the first dimension is frequency and the second is time. This means that for each frequency bin, the kernel slides and considers three time coefficients to produce six time coefficients. The final activation is a Sigmoid, which is real-valued $\in [0.0, 1.0]$. This makes the network estimate a soft or ratio mask which can then be applied to the input mixed magnitude spectrogram.

The exact parameters and configuration of the convolutional kernels and layers are described in Tables \ref{table:convtable1}, \ref{table:convtable2}, \ref{table:convtable3}, \ref{table:convtable4}, and \ref{table:convtable5}. The parameters were initially chosen from the kernel values and layers in \textcite{plumbley1, plumbley2}. The parameters were repeatedly modified in an informal hyperparameter tuning process during my submissions to the ISMIR 2021 Music Demixing Challenge. The various tested networks can be viewed in the submission repository here.\footnote{\url{https://gitlab.aicrowd.com/sevagh/music-demixing-challenge-starter-kit}} The design was also constrained by the time limits of the network inference in the challenge. For example, dilations in the time dimension were chosen to efficiently increase the temporal receptive field on the sliCQT without slowing down the inference step.

\begin{table}[ht]
	\centering
	\caption{Kernel parameters, time dimension}
	\label{table:convtable1}
	\begin{tabular}{ |l|l|l| }
	 \hline
		\# time coefficients & CDAE layer & Growth layer \\
	 \hline
	 \hline
		$\le 100$ & \makecell[l]{size=7\\stride=1\\dilation=2} & \makecell[l]{size=3\\stride=2\\dilation=1} \\
	 \hline
		$> 100$ & \makecell[l]{size=13\\stride=1\\dilation=2} & \makecell[l]{size=3\\stride=2\\dilation=1} \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{Kernel parameters, frequency dimension}
	\label{table:convtable2}
\begin{tabular}{ |l|l|l| }
	 \hline
		\# frequency bins & CDAE layer & Growth layer \\
	 \hline
	 \hline
		$\le 10$ & \makecell[l]{size=1\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
		$\ge 10, < 20$ & \makecell[l]{size=3\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
		$\ge 20$ & \makecell[l]{size=5\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{CDAE encoder layers}
	\label{table:convtable3}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		Conv2d$\Big($\makecell[l]{in=2\\out=25}$\Big)$, BN, ReLU & Conv2d$\Big($\makecell[l]{in=25\\out=55}$\Big)$, BN, ReLU \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{CDAE decoder layers}
	\label{table:convtable4}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		ConvTranspose2d$\Big($\makecell[l]{in=55\\out=25}$\Big)$, BN, ReLU & ConvTranspose2d$\Big($\makecell[l]{in=25\\out=2}$\Big)$, BN, ReLU \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{Growth (inverse 50\% overlap-add) layer}
	\label{table:convtable5}
\begin{tabular}{ |l| }
	 \hline
		Growth layer \\
	 \hline
	 \hline
		ConvTranspose2d$\Big($\makecell[l]{in=2\\out=2}$\Big)$, Sigmoid \\
	 \hline
\end{tabular}
\end{table}

\subsubsection{Combining the target networks like X-UMX}
\label{sec:xumxinc}

This section will describe how the UMX models for the four targets are trained together to apply the X-UMX loss functions, as shown in Figure \ref{fig:xumxcl}.

For simplicity, having a single training loop and single trained model to perform the separation is easier to consider than four independent models. For this reason, and that the final music demixing performance of X-UMX is higher than UMX, we chose to use the X-UMX variant of UMX.

The data loader of Open-Unmix returns a single target at a time, or an $(x, y)$ pair where $x$ is the mixed waveform and $y$ is the desired target. With CrossNet-Open-Unmix, the MUSDB18-HQ data loader needs to return $(x, y_{1}, y_{2}, y_{3}, y_{4}$ in training, where $y_{1-4}$ are the ground-truth waveforms of all four isolated targets.

\newpagefill

\subsection{Post-processing}
\label{sec:postprocessing}

\todo[inline]{create a good post-processing diagram, copy from sliceq22 ideas}

After the \Verb#OpenUnmix# class outputs the estimates of the magnitude sliCQT for all four targets, the post-processing Wiener expectation-maximization (EM) step needs to be applied. The code for the Wiener-EM step is essentially unchanged from the original, since both the sliCQT and STFT output complex Fourier coefficients.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/filtering.py\#L290}} The inputs to the Wiener-EM function are the mixed complex spectrogram and a rectangular matrix containing the four target magnitude spectrograms.

The approach in xumx-sliCQ was to add a boolean parameter named \Verb#slicq_wiener#. If \Verb#slicq_wiener# is set to False, the first waveform estimate is taken from the sliCQT and the Wiener-EM step is applied on the STFT,\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L301}} shown in Code Listing \ref{code:wienerstft}. If \Verb#slicq_wiener# is set to True, the Wiener-EM step is performed directly on the sliCQT.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L380}} The ragged sliCQT was zero-padded into a single rectangular matrix, allowing the original Wiener-EM code to be used (with minor modifications) for both the STFT and rectangular zero-padded sliCQT.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L389-L404}} The zero-padding operation is shown in Code Listing \ref{code:wienerslicqt}. Each strategy results in a different music demixing quality and execution time, which will be discussed in section \ref{sec:experiment}. 

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/wiener_stft.py}
\end{inputminted}
  \caption{Using the STFT in the Wiener-EM post-processing step}
  \label{code:wienerstft}
\end{listing}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/wiener_slicqt.py}
\end{inputminted}
  \caption{Zero-padding the sliCQT for the Wiener-EM post-processing step}
  \label{code:wienerslicqt}
\end{listing}

\end{document}
