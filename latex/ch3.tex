\documentclass[report.tex]{subfiles}
\begin{document}

\section{Methodology}
\label{ch:methodology}

The proposed adaptation of CrossNet-Open-Unmix (X-UMX) to use the sliced Constant-Q Transform (sliCQ Transform or sliCQT) is named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ/tree/main}}\textsuperscript{,}\footnote{xumx-sliCQ is pronounced like ``X-U-M-X-slice-Q''} and is the subject and main result of this thesis.

Decisions for the methodology were influenced by the limitations of the computer that xumx-sliCQ was developed on, e.g., the maximum GPU memory available. Refer to Appendix \ref{appendix:computerspec} for the hardware and software specifications of the computer. Appendix \ref{appendix:codeavail} contains links to all of the code referenced in this chapter.

CrossNet-Open-Unmix (X-UMX) is a near-state-of-the-art neural network for music demixing based on Short-Time Fourier Transform (STFT) masking, an overview of which was given in Section \ref{sec:umx} and \ref{sec:xumx}. The STFT has a fixed time-frequency resolution, while the sliCQT has a varying time-frequency resolution that may be more suitable for musical and auditory applications, as described in Section \ref{sec:theorynsgt}. xumx-sliCQ will be an adaptation of X-UMX which uses the sliCQT instead of the STFT, to investigate how this impacts its music demixing performance.

Figure \ref{fig:generalmdx} shows a block diagram for a general deep neural network (DNN) music demixing system that uses the spectrogram as its input and output representation, and uses the phase of the mixture to swap back to the time domain. This strategy was described in Section \ref{sec:noisyphaseoracle}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/generic_mdx.png}
	\caption{General DNN music demixing system with magnitude spectrograms. This diagram is shown for a single estimated waveform, but music demixing systems often estimate four waveforms for vocals, bass, drums, and other, following the example of the MUSDB18-HQ dataset.}
	\label{fig:generalmdx}
\end{figure}

Figure \ref{fig:umxandxumxslicq} shows how X-UMX and xumx-sliCQ are variants of the above general system, and how they differ from each other. X-UMX uses the STFT spectrogram as the input representation of musical signals, and it uses a Bidirectional Long Short-Term Memory (Bi-LSTM) network architecture. xumx-sliCQ will use the sliCQT spectrogram as the input representation of music, and it will use either a Bi-LSTM or convolutional denoising autoencoder (CDAE) network architecture.

\begin{figure}[ht]
	\centering
	\subfloat[X-UMX for one target.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/umx_clean.png}}\\
	\subfloat[xumx-sliCQ for one target.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/xumx_slicq_clean_botharch.png}}
	\caption{X-UMX and xumx-sliCQ compared. Note that only one target is shown for simplicity, but that both X-UMX and xumx-sliCQ estimate the four targets of vocals, drums, bass, and other from MUSDB18-HQ.}
	\label{fig:umxandxumxslicq}
\end{figure}

In Section \ref{sec:inputrepresentation}, I will describe how I ported the sliCQT to use PyTorch, a Python framework to run deep neural networks (DNN) on the GPU. I will also describe new frequency scales I added to the sliCQT for music applications, and how I chose frequency scale parameters for the sliCQT that might perform well in music demixing. In Section \ref{sec:neuralnet}, I will discuss the changes made to X-UMX to use the sliCQT instead of the STFT in xumx-sliCQ, the choice of Bi-LSTM and CDAE neural architectures, and the combining of the four independent target networks into a single model with the X-UMX loss functions. Finally, in Section \ref{sec:postprocessing}, I will show how the post-processing iterative Wiener filtering and Expectation-Maximization (Wiener-EM) step was adapted for the sliCQT.

\subsection{Input representation}
\label{sec:inputrepresentation}

First, in Section \ref{sec:stftslicqtcomp}, \ref{sec:stftslicqtcomp1}, and \ref{sec:stftslicqtcomp2}, I will describe the differences between the output of the STFT and the sliCQT. In Section \ref{sec:torchslicqdatastructure}, I will describe how I chose an appropriate data structure to represent the sliCQT in PyTorch as a result of these differences.

Next, in Section \ref{sec:torchslicq}, I will show the modifications made to the library to compute the transform with PyTorch on the GPU, so that it can be used in the X-UMX neural network code. I will also describe how the PyTorch sliCQT data structure differs from the STFT.

In Section \ref{sec:improvelib}, I will show the addition of new frequency scales that are interesting for music applications that were described in Section \ref{sec:theorynsgt}. Recall from Section \ref{sec:theorynsgt} that the sliCQT can be thought of as a filterbank, and the chosen frequency scale is the most important parameter that defines the spectral representation of the musical signal, which influences the results of the downstream application. In Section \ref{sec:sllenpicker}, I will explain how I added code to automatically choose the slice and transition length of the sliCQT based on the time-frequency resolution requirements of the desired frequency scale.

Finally, in Section \ref{sec:slicqparamsrch}, I will describe the parameter search that was run to choose frequency scales for the sliCQT with parameters that may perform best in a music demixing application. Section \ref{sec:fasterbsscupy} describes modifications made to the BSS (blind source separation) metrics library to make it run faster on the GPU, with the aim to expedite the parameter search process.

\subsubsection{Comparing the sliCQT to the STFT}
\label{sec:stftslicqtcomp}

In this section, I will compare and contrast the expected outputs of the STFT and sliCQT. This lays the groundwork for the chosen data structure to represent the sliCQT in PyTorch and its implementation, which will be shown in upcoming sections.

The STFT was described in Section \ref{sec:jointtfa} and its rectangular matrix output was shown in Figure \ref{fig:stftdiag}.

The sliCQT was described in Section \ref{sec:theoryslicqt} as a realtime variant of the Nonstationary Gabor Transform (NSGT) shown in section \ref{sec:theorynsgt}. The sliCQT and NSGT both output a ragged matrix, characteristic to nonuniform frequency transforms described in \ref{sec:raggedtf} and shown in Figure \ref{fig:raggedslicqt}. By contrast, the STFT can be thought of as a uniform frequency transform, because its frequency bins are uniformly or evenly spaced.

The sliCQT splits the input signal into fixed-size overlapping slices and computes discrete Fourier Transform (DFT) coefficients per slice, while the NSGT computes DFT coefficients for the whole input signal. To make the output of the sliCQT comparable to the NSGT, the slices must be overlap-added as described in Section \ref{sec:theoryslicqt}.

From the above, there are two important and distinct characteristics that make working with the sliCQT different from the STFT:
\begin{enumerate}
	\item
		The sliCQT computes the DFT coefficients for fixed-sized slices of the input signal. These slices need to be overlap-added with their adjacent slices to create the final spectrogram. Note that the shape of the DFT coefficients of each slice is ragged, due to the nonuniform time-frequency resolution.
	\item
		The overlap-added sliCQT spectrogram has the same ragged characteristic as the individual slices.
\end{enumerate}

When designing a data structure for the sliCQT, each of these points has to be addressed separately. 

\subsubsection{3D shape of the sliCQT compared to the 2D STFT}
\label{sec:stftslicqtcomp1}

The sliCQT was described in Section \ref{sec:theoryslicqt} as a realtime implementation of the NSGT, consisting of DFT coefficients computed for the input signal split into overlapping slices, shown in Figure \ref{fig:slicqtukeys}. The sliCQT therefore returns the DFT coefficients of consecutive slices of length $sllen$ of an input signal of length $N > sllen$. These slices need to be overlap-added with 50\% of the previous and next slice to produce a spectrogram, because each slice is symmetrically zero-padded to twice its length to reduce time-aliasing, which was described in Section \ref{sec:theoryslicqt} and shown in Figure \ref{fig:slicqoverlaps}. This is a characteristic specific to the sliCQT. The NSGT, which is the ancestor of the sliCQT not designed for realtime processing, outputs the spectrogram directly without requiring any overlap-add.

Figure \ref{fig:raggedslicqtoverlapadd} shows the procedure of overlap-adding ragged adjacent slices of the sliCQT to produce the final overlap-added ragged transform.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/overlapprocess.png}
	\caption{Overlap-adding adjacent ragged slices of the sliCQT to produce the final ragged output. The green-colored matrix represents the frequency bins analyzed with the first time-frequency resolution, and the pink-colored matrix represents the frequency bins analyzed with the second time-frequency resolution. The red rectangle shows the overlap between slice 1 and 2, and the blue rectangle shows the overlap between slice 2 and 3.}
	\label{fig:raggedslicqtoverlapadd}
\end{figure}

Note that the 50\% overlap is done independently for each of the time-frequency resolutions, since they have different temporal framerates and a different total number of frames per slice. For example, in the illustration above, the overlap for the lower green-colored block of frequency bins is two frames, or half of the total width of four frames, while the overlap for the upper pink-colored block of frequency bins or half the total width of six frames.

\subsubsection{Raggedness of the sliCQT compared to the STFT}
\label{sec:stftslicqtcomp2}

After overlap-adding the sliCQT, the result is a ragged matrix representing a nonuniform time-frequency transform with a different time-frequency resolution for groups of frequency bins. Figure \ref{fig:contraststftslicqt} shows the output of the STFT and an arbitrary ragged nonuniform frequency transform such as the NSGT side-by-side.

\begin{figure}[ht]
	\centering
	\subfloat[STFT, a uniform time-frequency transform. The entire matrix is colored yellow, since every frequency bin is analyzed with the same (i.e., uniform) time-frequency resolution.]{\includegraphics[width=0.4688\textwidth]{./images-blockdiagrams/stftslicqtcmp1.png}}\\
	\subfloat[A nonuniform time-frequency transform. The green, yellow, pink, and blue matrices represent the four different time-frequency resolutions used to analyze the different groupings of frequency bins.]{\includegraphics[width=0.6094\textwidth]{./images-blockdiagrams/stftslicqtcmp2.png}}
	\caption{The uniform STFT compared to a ragged nonuniform transform like the NSGT.}
	\label{fig:contraststftslicqt}
\end{figure}

\subsubsection{Choosing the data structure for the PyTorch sliCQT}
\label{sec:torchslicqdatastructure}

When choosing a data structure to represent the sliCQT in PyTorch, the two characteristics described in the previous sections must be addressed, which are the adjacent slices that need to be overlap-added, and the raggedness of the slices and final overlap-added transform from the nonuniform time-frequency resolution.

Due to limitations in the PyTorch library, jagged or ragged tensors (which are the equivalent of irregular or ragged matrices) are not yet supported.\footnote{\url{https://github.com/pytorch/pytorch/issues/25032}} That means that the ragged sliCQT cannot be represented by a single tensor, due to the irregular or ragged shape of the underlying matrix from the different time-frequency resolutions.

Therefore, to represent the raggedness of the noniform time-frequency resolution, the data structure chosen was a list of tensors. The list data structure in Python was described in Section \ref{sec:pythonbasics}. Each element of the list contains a tensor for the groups of frequency bins that have the same time-frequency resolution and can therefore fit in the same tensor or matrix.

As described in the previous section, the sliCQT returns 2D time-frequency matrices per slice of the input signal, and multiple slices are needed to represent input signals longer than the slice length. Therefore, a 3D tensor with the dimensions: time, frequency, and slice are used to represent the transform of each slice. The 3D tensor, when overlap-added, is converted to a 2D tensor with dimensions of time and frequency, which is equivalent to the 2D time-frequency dimensions of the STFT. Figure \ref{fig:3dslicqola2d} shows how the list of 3D tensors is converted to a list of 2D tensors by the overlap-add procedure.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/3dslicqola2d.png}
	\caption{Overlap-adding the list of 3D tensors to create a list of 2D tensors in the sliCQT. The green-colored matrices represent the first time-frequency resolution, and the pink-colored matrices represent the second time-frequency resolution in the ragged list.}
	\label{fig:3dslicqola2d}
\end{figure}

The final data structure for the sliCQT in PyTorch is a list of 3D tensors. Each 3D tensor in the list represents a grouping of frequency bins by their common time-frequency resolution. The 3D tensors of the list are each independently overlap-added to produce 2D tensors, which constitute the final time-frequency spectrogram representation of the audio signal. Figure \ref{fig:2dslicqolastft} shows the data structure of the ragged overlap-added sliCQT compared to the data structure of the STFT.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.65635\textwidth]{./images-blockdiagrams/2dslicqolastft.png}
	\caption{Ragged list of 2D tensors representing the overlap-added sliCQT spectrogram, compared to the single 2D tensor representing the STFT spectrogram. The sliCQT contains two colors, green and pink, representing two different time-frequency resolutions for different groups of frequency bins. The STFT contains a single color, yellow, representing the uniform time-frequency resolution used for all the frequency bins.}
	\label{fig:2dslicqolastft}
\end{figure}

\subsubsection{Computing the sliCQT on the GPU with PyTorch}
\label{sec:torchslicq}

In this section, I will describe the procedure for porting the CPU-based code of the reference NSGT/sliCQT library to the GPU using PyTorch. This is needed to be able to use the sliCQT in the PyTorch GPU code of X-UMX. All the modifications to the library were made in my copy of the software library,\footnote{\url{https://github.com/sevagh/nsgt}} and the same library was embedded inside the xumx-sliCQ code.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/tree/main}}

The reference NSGT/sliCQT library uses NumPy, a CPU-based Python numerical computation library. PyTorch, in addition to being a deep learning framework, is also a numerical computation library that implements a large amount of NumPy's functions.

The line-profiler\footnote{\url{https://pypi.org/project/line-profiler/}} package for Python prints a line-by-line output of the execution times of Python functions, and it was used to find performance hotspots and bottlenecks in the NSGT library. All NumPy functions and other non-GPU code was replaced with their PyTorch equivalents. To the user, the only difference was the addition of a \Verb#device# parameter to the \Verb#NSGT# and \Verb#NSGT_sliced# classes, which are the NSGT and sliCQT respectively. The choices of device are PyTorch device strings, and it defaults to the value \Verb#device="cpu"#. Using \Verb#device="cuda"# allows one to select their NVIDIA GPU. In practice, any device supported by PyTorch is also supported.

Many internal methods, functions, and utilities were modified during the port to PyTorch, and each in turn was also modified to take a \Verb#device# parameter. As these are internal to the library and not intended to be used directly by the end user, they will not be described here.

Due to requirement of the 50\% overlap-add to create a spectrogram, the overlap-add utility is accompanied with a spectrogram plotting implementation. The spectrogram utility function\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/plot.py}} uses the matplotlib\footnote{\url{https://matplotlib.org/}} library functions pcolormesh\footnote{\url{https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pcolormesh.html}} and colorbar\footnote{\url{https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html}} in its implementation.

Figure \ref{fig:overlappedspectrograms} compares the non-sliced NSGT spectrogram with the overlap-added sliCQT spectrogram, generated with the overlap-add and spectrogram utilities.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT (non-sliced), length is exact same as input signal. No overlap-add required.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_mel_nooverlap.png}}\\
	\subfloat[sliCQT with minimum automatic sllen (6,744 samples) and 50\% overlap-add.]{\includegraphics[width=0.6328\textwidth]{./images-gspi/gspi_nsgt_mel_perfect_slice.png}}
	\caption{NSGT spectrograms for the mel scale with 96 bins in 20--22,050 Hz.}
	\label{fig:overlappedspectrograms}
\end{figure}

\subsubsection{New frequency scales in the sliCQT library}
\label{sec:improvelib}

In this section, I will discuss why I added new frequency scales to the reference NSGT/sliCQT library.\footnote{\url{https://github.com/grrrr/nsgt/blob/master/nsgt/fscale.py}}

The frequency scale is perhaps the most important parameter of the NSGT and sliCQT. It defines the nonuniform frequency bands for audio analysis desired by the user. The choice of frequency scale affects the spectral representation of the music signal. The hypothesis of this thesis is that a spectrogram generated from a musical or auditory frequency scale may improve the results of the downstream music demixing system compared to the STFT, which suffers from the time-frequency tradeoff described in Section \ref{sec:jointtfa}.

The parameters to a frequency scale in the NSGT/sliCQT library are $f_{\text{min}}$ and $f_{\text{max}}$, which are the minimum and maximum frequency in Hz respectively, and the total frequency bins. The type of scale determines how the frequency bins are distributed between $f_{min}$--$f_{\text{max}}$. A custom frequency scale may use additional parameters as needed.

The octave scale is the default scale used to demonstrate the NSGT and sliCQT \parencite{balazs, slicq} in various open-source libraries and implementations.\footnote{\url{https://github.com/grrrr/nsgt}, \url{https://www.mathworks.com/help/wavelet/ref/cqt.html}, \url{http://ltfat.org/doc/filterbank/cqt_code.html}} The octave scale and log scale are almost identical, except that the bins parameter is interpreted as bins-per-octave for the octave scale, and the total number of output bins are computed using equation \eqref{equation:bpo} from Section \ref{sec:cqt}. In the log scale, the bins parameter is directly used as the number of output bins. In this thesis, I used the log scale to avoid the hidden bins-per-octave calculation. Appendix \ref{appendix:octscale} contains a more in-depth comparison of the octave and log scales.

Section \ref{sec:theorynsgt} showed examples of NSGT spectrograms with the log (Constant-Q) and mel scales, in Figures \ref{fig:bunchansgts1} and \ref{fig:bunchansgts2}. These are the scales provided in the reference NSGT/sliCQT library. I also discussed that the Variable-Q scale for music and Bark psychoacoustic scale are both of interest in music and auditory applications. For this reason, I chose to implement the Variable-Q and Bark scales to supplement the Constant-Q and mel scales.

The first frequency scale I added was the Variable-Q scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L105}} which introduces an additional parameter, $\gamma \text{ (Hz)}$ for the offset. $\gamma$ is a frequency offset added to each frequency band to reduce the Q-factor and improve the time resolution in the lower frequency bins, as described in Section \ref{sec:theorynsgt}. Figure \ref{fig:vq} compares the Constant-Q and Variable-Q frequency scales and Q-factors for different values of $\gamma$. The Q-factor of Constant-Q scale was defined as the relative frequency resolution in Section \ref{sec:cqt}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-freqscales/vqlog.png}
	\caption{Frequency bins and Q-factors for the Constant-Q and Variable-Q scales.}
	\label{fig:vq}
\end{figure}

The next frequency scale I added was the Bark psychoacoustic scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L207}} to complement the included mel scale, as discussed in Section \ref{sec:theorynsgt}. Figure \ref{fig:melbarkfsandqs} shows the mel and Bark frequency scales and Q-factors.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-freqscales/melbarkpitchesqs.png}
	\caption{Frequency bins and Q-factors for the mel and Bark scales.}
	\label{fig:melbarkfsandqs}
\end{figure}

\subsubsection{Automatic slice length picker}
\label{sec:sllenpicker}

As mentioned in Section \ref{sec:theoryslicqt}, the sliCQT processes the input signal in fixed-sized slices, ideal for realtime processing and for input signals with arbitrary lengths. Compared to the NSGT, the sliCQT has an additional need for slice length and transition length parameters. To analyze an input musical signal with nonuniform frequency scales, the slice and transition length of sliCQT need to be set appropriately to support the varying time-frequency resolution. I added code to the sliCQT library to automatically choose the minimum appropriate slice length and transition length for a given frequency scale.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L36}} This improves the user experience by only requiring the frequency scale as the parameter to the sliCQT.

\subsubsection{Choosing sliCQT parameters}
\label{sec:slicqparamsrch}

In this section, I will design a sliCQT parameter search to discover optimal sliCQT parameters that produce the best mixed phase inversion (MPI) oracle results. MPI is the music demixing strategy where the target magnitude spectrogram is combined with the phase of the mix spectrogram to create a time-domain target waveform estimate, as was described in Section \ref{sec:noisyphaseoracle}. The MPI oracle represents the best possible result of the MPI strategy by using the ground truth target magnitude spectrogram. Since the MPI strategy is used in X-UMX and xumx-sliCQ, my hypothesis is that maximizing the MPI oracle result would maximize the music demixing performance of the final neural network.

As discussed in Section \ref{sec:ml}, the validation split of a dataset is often used to tune hyperparameters, which are the parameters of a machine learning or deep learning network defined by the user. In xumx-sliCQ, the sliCQT parameters are a hyperparameter, similar to how the STFT settings of the window and hop size are hyperparameters in X-UMX.

As described in Section \ref{sec:improvelib}, the time and frequency resolution of the sliCQT are defined from the frequency scale, and the parameters of the frequency scale are $f_{\text{min}}$ and $f_{\text{max}}$, which are the minimum and maximum frequency in Hz respectively, and the total frequency bins.

The maximum frequency $f_{\text{max}}$ was fixed to 22,050 Hz, which is the Nyquist rate of the 44,100 Hz sample rate of MUSDB18-HQ. I made this decision after early experiments showed that fixing the maximum frequency to 22,050 Hz in several cases led to a smaller size of the sliCQT than choosing a smaller $f_{\text{max}}$, resulting in lower memory demands on the system. The results of these early experiments are shown in Appendix \ref{appendix:slicqdim1}.

The minimum frequency $f_{\text{min}}$ was chosen from the range of 10.0--130.0 Hz, loosely based on the frequencies of the C0 (16.35 Hz) and C3 (130.81 Hz) notes of the well-tempered scale.

The total frequency bins were chosen from the range of 10--300 bins. The 300-bin maximum was determined by the maximum size of sliCQT that could fit in the 12 GB GPU memory of the NVIDIA 3080 Ti GPU. Details of the hardware limitations of the testbench computer are shown in Appendix \ref{appendix:computerspec}.

The maximum slice length was capped to 44,100 samples, which represents one second of music. This value was chosen because full-length songs are typically split into one second subsequences by several demixing papers \parencite{plumbley1, plumbley2, demucs}. Since the slice length and transition length are automatically picked for a given frequency scale using the new feature whose addition was described in Section \ref{sec:improvelib}, the parameter search discards combination of parameters whose slice length exceeds the maximum.

The random search parameters are shown in Table \ref{table:slicqparams}. Note that an instance of the script was run for each of the four frequency scales: Constant-Q, Variable-Q, mel, and Bark.

\begin{table}[ht]
	\centering
	\caption{Parameter ranges for the sliCQT parameter search.}
	\label{table:slicqparams}
\begin{tabular}{ |l|l|l|l| }
	 \hline
	 Scale & Total bins & $f_{\text{min}}$ (Hz) & Additional params \\
	 \hline
	 \hline
	 Constant-Q & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Variable-Q & 10--300 & 10.0--130.0 & $\gamma$ = 10.0--130.0 Hz \\
	 \hline
	 mel & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Bark & 10--300 & 10.0--130.0 & n/a \\
	 \hline
\end{tabular}
\end{table}

The space of possible parameters is infinite, even with the ranges constrained as in Table \ref{table:slicqparams}. Therefore, an exhaustive grid search over all the possible parameters is impossible. To search for hyperparameters in a neural network, \textcite{randomgrid} found that:

\begin{quote}
	[o]ften the extent of a search is determined by a computational budget, and with random search 64 trials are enough to find better models in a larger less promising space \parencite[293]{randomgrid}.
\end{quote}

I decided to use a random search with at least 64 iterations for the sliCQT parameters, since the sliCQT parameters are hyperparameters of the xumx-sliCQ neural network. I ran two instances of the random parameter search script with 60 iterations each, resulting in 120 total iterations of the random parameter search for each of the four frequency scales. Each instance of the script used a different random seed (42 and 1337). The purpose of fixing the seed of Python's random number generator (RNG) is discussed in Section \ref{sec:pythonbasics}.

Recall from Section \ref{sec:evalbss} that the Signal-to-Distortion (SDR) is one of the scores of the BSS metrics, with a unit of decibels (dB), which is commonly used as a single metric that summarizes the global performance of a music demixing system. The selected sliCQT frequency scale parameters were evaluated by computing the mix-phase waveforms, which were described in Section \ref{sec:noisyphaseoracle}. The phase of the complex sliCQT of the mixed song and the magnitude of the complex sliCQT of the ground-truth source signal for all four sources (drums, bass, vocals, other) were combined to create the MPI (mix-phase inversion) waveforms. The median SDR was computed across the four sources and 14 validation tracks of the MUSDB18-HQ dataset, and the sliCQT parameters that resulted in the MPI waveforms with the highest SDR were selected as the final sliCQT parameters to use in the neural network.

\subsubsection{Speeding up the parameter search using the GPU and CuPy}
\label{sec:fasterbsscupy}

In this section, I will explore a speedup of the parameter search script using the GPU. The parameter search script described in the previous section computes the MPI oracle waveforms and measures their SDR score on the 14 validation tracks of MUSDB18-HQ. As multiple instances of the script were run for several iterations during the experimentation phase, reducing the total time of the parameter search script would allow for the script to run in a shorter time frame, leading to a faster development cycle.

As in Section \ref{sec:torchslicq}, the Python line-profiler library was used to discover any performance bottlenecks or slow parts of the MPI oracle parameter search script. The slowest step was the calculation of the SDR score from the BSS metrics library.\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}}

The bottlenecks of the BSS metrics library are caused by NumPy and SciPy operations, which are Python numerical computation libraries that use the CPU. The CuPy library\footnote{\url{https://cupy.dev/}} provides drop-in replacement functions of NumPy and SciPy, which run on NVIDIA GPUs using the CUDA compute framework.\footnote{\url{https://developer.nvidia.com/cuda-toolkit}} This allows for acceleration of numerical operations through GPU parallelization. Note that although PyTorch was used to replace NumPy code in the earlier Section \ref{sec:torchslicq}, CuPy is intended to be a direct replacement of NumPy and SciPy and thus represents an easier porting effort. Also, since the MPI oracle testbench is separate from the xumx-sliCQ PyTorch code, it does not need to use PyTorch.

I copied the BSS metrics library to my own GitHub user account,\footnote{\url{https://github.com/sevagh/sigsep-mus-eval}} which is called \Verb#sigsep-mus-eval# in the SigSep GitHub organization, and replaced the slowest NumPy and SciPy functions with their CuPy equivalents,\footnote{\url{https://github.com/sevagh/sigsep-mus-eval/commit/f3b7540faddc8d2b1bc91cecaf05fd20d96df7c6}} taking special care about GPU out-of-memory errors. The library is unchanged for end users, and the GPU is simply used whenever possible, falling back to the CPU in case of any errors, or if there is no GPU available on the user's computer.

\newpagefill

\subsection{Neural networks}
\label{sec:neuralnet}

First, in Section \ref{sec:replacestft}, I will look at how the X-UMX PyTorch code needs to be modified in xumx-sliCQ to replace the STFT with the ragged sliCQT described in Section \ref{sec:torchslicq}.

Next, in Section \ref{sec:slicqarches}, I will show two neural network architectures, Bi-LSTM and CDAE, that can be used in xumx-sliCQ. I will show how the original Bi-LSTM architecture from X-UMX and the convolutional denoising autoencoder (CDAE) architecture described in Section \ref{sec:cdae}, both originally intended for the STFT, can be adapted to operate on the ragged sliCQT. In Section \ref{sec:bandwidth}, I will describe the bandwidth parameter of X-UMX and how it was adapted for the sliCQT in xumx-sliCQ. In Section \ref{sec:deoverlap}, I will discuss a de-overlap layer, which is necessary for any neural architecture for the sliCQT.

Finally, in Section \ref{sec:xumxinc}, I will show how I incorporated the X-UMX idea of combining the four target models in a single model with the multi-domain loss (MDL) and combination loss (CL) functions.

\subsubsection{Replacing the STFT with the sliCQT in X-UMX}
\label{sec:replacestft}

In this section, I will describe the changes made to X-UMX to support the data-structure of the sliCQT shown in Section \ref{sec:torchslicqdatastructure}, which addressed the distinct traits of the sliCQT compared to the STFT: the 3D slice-wise output, and the ragged shape from the nonuniform time-frequency resolution.

Firstly, X-UMX uses a sequence duration of six seconds, meaning the training input is the STFT of six seconds of music. The dimensionality of the sliCQT for a waveform is larger than the STFT for a variety of different sliCQT parameters, as was discovered during preliminary experiments shown in Appendix \ref{appendix:slicqdim2}. In these experiments, the sliCQT of a six second sequence used too much GPU memory during training. In xumx-sliCQ, I chose a smaller sequence duration of one second, resulting in a sliCQT with fewer coefficients that uses less GPU memory. Also, a one-second sequence duration is common in several demixing papers \parencite{plumbley1, plumbley2, demucs}.

In sections \ref{sec:raggedtf} and \ref{sec:torchslicqdatastructure}, I described the data structure of the ragged sliCQT and how it differed from the single rectangular matrix of the STFT. The ragged sliCQT outputs a list of 3D matrices, representing the sliced transform, that must be overlap-added to create a list of 2D time-frequency matrices. Recall that the list was introduced from the raggedness of the transform, because each group of time-frequency bins has a different temporal framerate, resulting in a different size tensor or matrix. This raggedness is a desired characteristic of nonuniform time-frequency transforms like the NSGT, sliCQT, and even the original CQT.

First, the list of 3D tensors of the sliCQT must always be overlap-added to transform them to the 2D time-frequency matrix. Next, all of the operations of X-UMX that expect a single rectangular matrix of the STFT need to be modified to work on a list of matrices instead, where each separate matrix in the list consists of a group of frequency bins that share the same temporal framerate, ordered from the lowest to highest frequency bins. Lastly, the list of 2D tensors in the overlap-added sliCQT needs to be converted back to the list of 3D tensors in the original sliCQT to synthesize an audio waveform. This was achieved with a final transpose convolutional layer at the end of the neural network, whose effect is to double the time coefficients to reverse their 50\% reduction from the overlap-add. Then, the 2D shape is reshaped back to the original 3D shape. The process of the de-overlap is shown in greater detail in Section \ref{sec:deoverlap}.

\subsubsection{Neural network architectures}
\label{sec:slicqarches}

In this section, a choice of Bi-LSTM and CDAE architectures for the ragged sliCQT in xumx-sliCQ will be presented. I copied the Bi-LSTM code from the original model,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/openunmix/model.py\#L43}} making adaptations as necessary to support the sliCQT instead of the STFT. I implemented the CDAE code from the ground up, recreating the CDAE architectures described in \textcite{plumbley1} and \textcite{plumbley2} with the convolutional layers provided by PyTorch.

Figure \ref{fig:cdaeslicqt} shows how both the Bi-LSTM and CDAE network architectures should be applied to the ragged sliCQT. Instead of having a single neural network operate on the single matrix of the STFT, which includes all of the frequency bins at the same temporal frame rate, there must be a neural network that operates on each of the sub-matrices of the ragged sliCQT.

Note that both the Bi-LSTM and CDAE architectures use the 2D overlap-added sliCQT, and need a de-overlap layer, which is discussed in Section \ref{sec:deoverlap}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/xumx_slicq_pertarget_cdae_bilstm.png}
	\caption{Block diagram of the individual CNNs of xumx-sliCQ, using a simplified ragged sliCQT for demonstration purposes. The green-colored matrix represents the first time-frequency (TF) resolution and the pink-colored matrix represents the second time-frequency resolution in the ragged transform. The de-overlap layer is described in Section \ref{sec:deoverlap}.}
	\label{fig:cdaeslicqt}
\end{figure}

In the original X-UMX, the neural network architecture is a dense (i.e., linear) encoder-decoder with a Bi-LSTM. The STFT in X-UMX uses a window size of 4096, which results in 2049 output frequency bins. The purpose of the dense encoder/decoder with the STFT is to reduce the 2049 frequency bins into a smaller dimension set of 512 values, which are then used as the input and output of the Bi-LSTM. In the case of the sliCQT, the output frequency bins are less numerous than the STFT. As described in Section \ref{sec:slicqparamsrch}, the maximum frequency bins of the sliCQT were capped at 300, which is a small enough value to use directly as the input to the Bi-LSTM without needing to be encoded to a lower dimension with dense layers.

Figure \ref{fig:umxnetworkdetails} shows the details of the Bi-LSTM for the STFT with the X-UMX default 2049 frequency bins, and how it was adapted to a sliCQT that has a smaller set of frequency bins $f \ll 2049$ ($f$ was constrained to $[10, 300]$ in Section \ref{sec:slicqparamsrch}).

\begin{figure}[ht]
	\centering
	\subfloat[Dense encoder/decoder + Bi-LSTM architecture for the STFT with 2049 frequency bins.]{\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/umx_bilstm_orig.png}}\\
	\subfloat[Bi-LSTM architecture for the sliCQT with $f \ll 2049$ frequency bins.]{\includegraphics[width=0.5625\textwidth]{./images-blockdiagrams/umx_bilstm_slicq.png}}
	\caption{The original X-UMX Bi-LSTM for the STFT, and the variant adapted for the sliCQT.}
	\label{fig:umxnetworkdetails}
\end{figure}

As discussed in sections \ref{sec:cnn} and \ref{sec:cdae}, convolutional layers, and specifically convolutional denoising autoencoders (CDAE), are simple ways of defining 2D time-frequency filters that slide over a 2D time-frequency matrix of Fourier coefficients. The CDAE models of \textcite{plumbley1, plumbley2} were adapted for use in xumx-sliCQ.

Using the values from \textcite{plumbley2}, the two layers of convolution in the CDAE of xumx-sliCQ use 25 and 55 channels respectively. This means that the stereo or two-channel sliCQT has 25 output feature maps (or channels) computed from the first encoder convolutional layer, and 55 output feature maps from the second encoder convolutional layer. These are reversed in the symmetric decoder layer. Figure \ref{fig:newcdaefig} shows the CDAE architecture. The sub-matrices of the ragged sliCQT may have different dimensions for time and frequency, due to their varying frequency bins and temporal framerates, so the 2D convolution filter sizes are adapted to the input size. After each layer, a 2D batch normalization (BN) and nonlinear activation function, specifically the rectified linear unit (ReLU) activation, are applied \parencite{plumbley2}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9375\textwidth]{./images-blockdiagrams/xumx_slicq_cdae.png}
	\caption{Symmetric encoder/decoder architecture of the xumx-sliCQ CDAE. The green-colored matrices represent the first time-frequency (TF) resolution and the pink-colored matrices represent the second time-frequency resolution of the ragged sliCQT.}
	\label{fig:newcdaefig}
\end{figure}

The exact parameters and configuration of the convolutional kernels and layers are described in Tables \ref{table:convtable1}, \ref{table:convtable2}, \ref{table:convtable3}, and \ref{table:convtable4}. The parameters were initially chosen from the kernel values and layers in \textcite{plumbley1, plumbley2}. The parameters and network architectures were repeatedly modified in an informal tuning process, and an overview of all the historical experiments of xumx-sliCQ is included in Appendix \ref{appendix:crazyexperiments}.

\begin{table}[ht]
	\centering
	\caption{Kernel parameters, time dimension.}
	\label{table:convtable1}
	\begin{tabular}{ |l|l|l| }
	 \hline
		\# time coefficients & CDAE layer \\
	 \hline
	 \hline
		$\le 100$ & \makecell[l]{size=7\\stride=1\\dilation=2} \\
	 \hline
		$> 100$ & \makecell[l]{size=13\\stride=1\\dilation=2} \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{Kernel parameters, frequency dimension.}
	\label{table:convtable2}
\begin{tabular}{ |l|l|l| }
	 \hline
		\# frequency bins & CDAE layer \\
	 \hline
	 \hline
		$\le 10$ & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
		$\ge 10, < 20$ & \makecell[l]{size=3\\stride=1\\dilation=1} \\
	 \hline
		$\ge 20$ & \makecell[l]{size=5\\stride=1\\dilation=1} \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{CDAE encoder layers.}
	\label{table:convtable3}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		Conv2d$\Big($\makecell[l]{in=2\\out=25}$\Big)$, BN, ReLU & Conv2d$\Big($\makecell[l]{in=25\\out=55}$\Big)$, BN, ReLU \\
	 \hline
\end{tabular}
	\vspace{1em}
	\caption{CDAE decoder layers.}
	\label{table:convtable4}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		ConvTranspose2d$\Big($\makecell[l]{in=55\\out=25}$\Big)$, BN, ReLU & ConvTranspose2d$\Big($\makecell[l]{in=25\\out=2}$\Big)$, BN, ReLU \\
	 \hline
\end{tabular}
\end{table}

\subsubsection{Bandwidth parameter}
\label{sec:bandwidth}

X-UMX has a bandwidth parameter that controls how much of the 2049 output frequencies of the STFT with a window size of 4096 are used as an input to the neural network. The default value of the bandwidth parameter is 16,000 Hz, which means that the frequency bins of the STFT $> 16,000 \text{ Hz}$ are cropped before the STFT is input into the neural network. I implemented the same parameter in both Bi-LSTM and CDAE architectures of xumx-sliCQ, by not using frequency bins of the sliCQT $> 16,000 \text{ Hz}$ as inputs to the neural network.

\subsubsection{De-overlap layer}
\label{sec:deoverlap}

In this section, I will discuss the ``de-overlap layer'' that I created, which is necessary for both the Bi-LSTM and CDAE architectures of xumx-sliCQ. I described in Section \ref{sec:theoryslicqt} and \ref{sec:stftslicqtcomp1} that the 3D sliCQT needs its adjacent slices to be overlap-added by 50\% to create a spectrogram, due to the symmetric zero-padding of each slice introduced by \textcite{slicq} in the sliCQT to reduce time-domain aliasing. This 50\% overlap-add procedure does not have an exact inverse operation.

In the neural network of xumx-sliCQ, before applying either the Bi-LSTM or CDAE architectures, the first step is to overlap-add the 3D sliCQT to create a 2D time-frequency sliCQT. This means that in the final step of the neural network, the 2D overlap-added sliCQT needs to be transformed back to its original, non-overlap-added 3D shape. For this, I needed to create a custom layer in the neural network that could learn how to do the inverse overlap-add.

In Figure \ref{fig:convtranspose}, I showed how the transpose convolution or deconvolution operation, which can grow its input to a larger size. For the de-overlap layer of xumx-sliCQ, I chose to use a transpose convolution layer with a kernel size of $(1, 3)$ and a stride of $(1, 2)$ where the first dimension is frequency and the second is time. This means that for each frequency bin, the kernel slides over three time coefficients to produce six time coefficients. The effect is to double the number of time coefficients, which is a reversal of the 50\% reduction of time coefficients from the overlap-add procedure. Finally, the 2D sliCQT is reshaped to the original 3D shape to separate it back out into adjacent slices. The final activation after the transpose convolution layer is a Sigmoid function, which is real-valued $\in [0.0, 1.0]$. This makes the network estimate a soft or ratio mask that can then be multiplied with the input spectrogram.

\subsubsection{Modifying the mixing coefficient for the MDL loss}
\label{sec:xumxinc}

Recall from Section \ref{sec:xumx} that X-UMX includes the multi-domain loss (MDL), with a mixing coefficient to sum the magnitude spectrogram loss and the time-domain SDR loss. The mixing coefficient in X-UMX is 10.0. In xumx-sliCQ, it is set to 0.1 to reflect the observed order of magnitude difference in the coefficients of the STFT and the sliCQT.

To implement the time-domain SDR loss, I use the auraloss Python package.\footnote{\url{https://github.com/csteinmetz1/auraloss}} I used the SI-SDR (scale-invariant SDR) \parencite{roux2018sdr} variant of SDR as the loss function. SI-SDR is a simpler and more robust variant of SDR which improves on some failure cases of SDR \parencite{roux2018sdr}.

\subsection{Post-processing}
\label{sec:postprocessing}

In this section, I will discuss how the final post-processing step of X-UMX was modified for in xumx-sliCQ. In X-UMX, after the neural network outputs the estimates of the magnitude STFT for all four targets, the post-processing step of iterative Wiener expectation-maximization (EM) is applied to potentially improve the estimates (see Section \ref{sec:umx}).

Both the sliCQT and STFT output complex Fourier coefficients, which means that the Wiener-EM step can be run on both the sliCQT and the STFT representations of the estimated targets. Therefore, in xumx-sliCQ, I chose to implement two strategies of Wiener-EM: one that operates on the sliCQT outputs of xumx-sliCQ directly, and the other that swaps the sliCQT outputs into the STFT domain before applying Wiener-EM.

The music demixing quality and running time of both strategies will be shown in Section \ref{sec:demixresults}.

\end{document}
