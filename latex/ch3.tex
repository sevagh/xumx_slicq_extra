\documentclass[report.tex]{subfiles}
\begin{document}

\section{Methodology}
\label{sec:methodology}

The proposed adaptation of Open-Unmix (\cite{umx}) to use the sliCQ Transform (sliCQT) (\cite{invertiblecqt, slicq}) is named xumx-sliCQ,\footnote{\url{https://github.com/sevagh/xumx-sliCQ}} and is the subject and main result of this thesis.

Open-Unmix (\cite{umx}) has a reference implementation released as an open-source Python codebase for general music demixing,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} which uses PyTorch (\cite{pytorch}) for GPU-accelerated numerical computation and deep learning, and includes a MUSDB18-HQ data loader (\cite{musdb18hq}) and BSS metrics evaluator (\cite{bss}), both necessary components for this thesis.

xumx-sliCQ will be based on the Open-Unmix PyTorch template, and as a result, all of the code developed will be in Python, using PyTorch for the deep learning network to run on the GPU. The rest of this chapter will be focused on the Python and PyTorch implementation details of xumx-sliCQ, including improvements or changes made to the reference NSGT/sliCQT Python library.\footnote{\url{https://github.com/grrrr/nsgt}} External projects will be forked to my GitHub account\footnote{\url{https://github.com/sevagh/nsgt}} to contain all of the modifications required for this thesis.

\subsection{Improving the NSGT/sliCQT reference library}
\label{sec:improvelib}

\subsubsection{New frequency scales}
\label{sec:freqscales}

The time and frequency resolution of the NSGT and sliCQT in the reference library are defined from the frequency scale. In essence the NSGT/sliCQT can be thought of as a filterbank (\cite{variableq1}). The essential parameters to a frequency scale in the library are $f_{\text{min}}$ and $f_{\text{max}}$, which are the minimum and maximum frequency in Hz, and the total frequency bins. The type of scale determines how the frequency bins are distributed between $f_{min}$--$f_{\text{max}}$. A custom frequency scale may use additional parameters as needed, like the frequency offset parameter of the Variable-Q scale shown in section \ref{sec:theorynsgt}.

The provided scales\footnote{\url{https://github.com/grrrr/nsgt/blob/master/nsgt/fscale.py}} are octave, log, linear (i.e. the regular STFT), and mel. The octave scale is similar to the log scale, except that the octave scale takes a bins-per-octave (bpo) argument, and the log scale takes a bins argument. Recall from section \ref{sec:cqt} that to compute the total bins from the bins-per-octave setting of the CQ-NSGT, we can use $K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}}) + 1]$, where $K$ is the total bins, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies.

In the Python implementation, using the CQ-NSGT is the same as the base NSGT (or sliCQT) with a manually-defined octave scale. One can use the logarithmic scale for more direct control over the total bins. The difference is shown in listing \ref{code:octvlog}. The resulting frequency scale is plotted in figure \ref{fig:octvlog}.

\begin{figure}[h]
  \centering
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{inputminted}[linenos,breaklines,frame=single,firstline=4,lastline=16,fontsize=\scriptsize]{text}{./scripts/fscale.py}
\end{inputminted}
 \subfloat{(a) Code snippet defining octave and log scales}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|\$ python scripts/fscale.py 3 # invoke script for bpo=3|
|bpo: 3, bins: 21, len(oct): 21, len(log): 21|
\end{minted}
 \subfloat{(b) Output printed by above}
 \end{minipage}
  \captionof{listing}{Octave and log scales for the NSGT}
  \label{code:octvlog}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/log_vs_oct.png}
	\caption{Frequency bins for the octave and log scales, annotated with their nearest musical note}
	\label{fig:octvlog}
\end{figure}

To supplement the Constant-Q scales (octave and log), the Variable-Q scale (\cite{variableq1, variableq2}) was implemented.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L105}}

The Variable-Q scale is the same as the log or octave scales, except with a small fixed frequency offset, denoted by gamma or $\gamma \text{ (Hz)}$, added to each frequency bin. \textcite[5]{variableq1} provide the motivation for the Variable-Q scale:
\begin{quote}
	... [The] CQT has several advantages over STFT when analysing music signals. However, one considerable practical drawback is the fact that the analysis/synthesis atoms get very long towards lower frequencies. This is unreasonable both from a perceptual viewpoint and from a musical viewpoint. Auditory filters in the human auditory system are approximately Constant-Q only for frequencies above 500 Hz and smoothly approach a constant bandwidth towards lower frequencies. Accordingly, music signals generally do not contain closely spaced pitches at low frequencies, thus the Q-factors (relative frequency resolution) can safely be reduced towards lower frequencies, which in turn improves the time resolution.
\end{quote}

Both \textcite{variableq1} and \textcite{variableq2} cite the ERBlet transform (\cite{erblet}) as a psychoacoustic transform which in turn motivates the Variable-Q Transform. \textcite{variableq1, variableq2} provides the following equations in \eqref{equation:variablebw} for computing the bandwidth $B_{k}$ of the frequency bin (or filter channel) $k$ where $b$ is the bins-per-octave or bpo:
\begin{align}\tag{5}\label{equation:variablebw}
	\nonumber & B_{k} = \alpha f_{k} + \gamma, \alpha = 2^{\frac{1}{b}} - 2^{\frac{1}{b}}
\end{align}

The is to widen the bandwidths of the windows at the low frequency bins significantly, since the offset is comparable in its order of magnitude to the lower frequency bandwidths. \textcite{variableq1} show some examples with $\gamma = [0, 3, 6.6, 10, 30] \text{ Hz}$. As the center frequency increases, the effect of the small offset becomes negligible. This results in a widening of the Q-factors in the low frequency bins, but it becomes Constant-Q in the high frequency bins. Figure \ref{fig:vq} shows the Constant-Q/log, and Variable-Q scales for different values of the frequency offset $\gamma$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/vqlog.png}
	\caption{Frequency bins and Q factors for the Constant-Q/log and Variable-Q scales}
	\label{fig:vq}
\end{figure}

The next frequency scale added was the Bark psychoacoustic scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L207}} to complement the included mel scale. We used one of the possible formulas used to convert between Bark and Hz frequencies (\cite{barktan}) and vice versa, sometimes colloquially called the ``Barktan formula''\footnote{\url{https://ccrma.stanford.edu/~jos/bbt/Optimal_Frequency_Warpings.html}}:
\begin{align}
	\nonumber & f_{\text{Bark}} = 6 \cdot arcsinh(\frac{f_{\text{Hz}}}{600}), f_{\text{Hz}} = 600 \cdot sinh(\frac{f_{\text{Bark}}}{6})
\end{align}

Figure \ref{fig:melbarkfsandqs} shows the mel and Bark frequency scales and Q-factors from the NSGT library for 12 total mel or Bark bands.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-freqscales/melbarkpitchesqs.png}
	\caption{Mel and Bark frequency curves and Q-factors from the NSGT library}
	\label{fig:melbarkfsandqs}
\end{figure}

\subsubsection{Automatic slice length picker}
\label{sec:autosllen}

An addition made to the sliced version of the NSGT, or the sliCQT, was to automatically suggest an appropriate slice length and transition area for a given frequency scale. The slice length and transition area, as mentioned in section \ref{sec:theoryslicqt}, define how the input signal can be processed in smaller-sized chunks (or slices). Choosing an incorrect slice length which cannot support the windows needed to support the desired frequency resolution will result in a warning from the NSGT library, creating confusion for users on how to pick a slice length.\footnote{\url{https://github.com/grrrr/nsgt/issues/18}, \url{https://github.com/grrrr/nsgt/issues/24}} The warning code was adapted to suggest the minimum slice length and transition area for a given frequency scale,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/fscale.py\#L36}} and the result is shown in listing \ref{code:slicenoslice}.

\begin{figure}[h]
  \centering
\begin{minipage}{\textwidth}
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|f,q = self() # get the frequency bins and q-factors for scale|
|Ls = int(np.ceil(max((q*8.*sr)/f))) # sr = sample rate|
|Ls = Ls + -Ls \% 4|
|sllen = Ls|
|trlen = sllen//4|
|trlen = trlen + -trlen \% 2 # make trlen divisible by 2|
\end{minted}
 \subfloat{(a) Code for computing the minimum sllen and trlen}
\vspace{1em}
\end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/slicenoslice.py}
\end{inputminted}
 \subfloat{(b) Code snippet for NSGT and sliCQT with slice and transition lengths}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|\$ python scripts/slicenoslice.py|
|suggested sllen, trlen: 22040 5510|
|/home/sevagh/repos/nsgt/nsgt/nsgfwin\_sl.py:66: UserWarning: Q-factor too high for frequencies 82.41,90.81,100.07,110.27,121.52,133.91,147.56,162.61,179.18,...|
|  warn("Q-factor too high for frequencies \%s"\%",".join("\%.2f"\%fi for fi in f[q >= qneeded]))|
\end{minted}
 \subfloat{(c) Output printed by above}
 \end{minipage}
  \captionof{listing}{Suggested slice and transition lengths based on the desired frequency scale}
  \label{code:slicenoslice}
\end{figure}

\subsubsection{GPU-accelerated NSGT/sliCQT with PyTorch}
\label{sec:torchslicq}

The reference NSGT/sliCQT library uses NumPy, a CPU-based Python numerical computation library. To train deep neural networks efficiently, using PyTorch with GPU support is required (\cite{pytorch}). PyTorch, in addition to being a deep learning framework, is also a numerical computation library which implements a large amount of NumPy's functions.

The line-profiler\footnote{\url{https://pypi.org/project/line-profiler/}} package for Python prints a line-by-line output of the execution times of Python functions that you annotate with \Verb#@profile#, and it was used to find performance hotspots and bottlenecks in the NSGT library. An example output is shown in listing \ref{lst:profilerout} for the \Verb#forward# function of the sliced NSGT.

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   150                                               @profile
   151                                               def forward(self, sig):
   152         1          4.0      4.0      0.0          sig = self.channelize(sig)
   153
   154         1          1.0      1.0      0.0          f_sliced = slicing(sig, self.sl_len, self.tr_area)
   155
   156         1      19481.0  19481.0     19.5          cseq = chnmap(self.fwd, f_sliced)
   157
   158         1      80658.0  80658.0     80.5          cseq = arrange(cseq, self.M, True)
   159
   160         1          3.0      3.0      0.0          cseq = self.unchannelize(cseq)
   161
   162         1          1.0      1.0      0.0          return cseq
\end{minted}
  \caption{Example line profiler output}
  \label{lst:profilerout}
\end{listing}

The starting point of porting the library to the GPU was to simply replace all NumPy functions with their PyTorch equivalents. When refering to code changes, NumPy and PyTorch will be referred to as numpy and torch, their respective Python module names. The torch port to the NSGT/sliCQT can be viewed on GitHub through the commit browser, starting from the following commit.\footnote{\url{https://github.com/sevagh/nsgt/commits/main?after=9aed369865989d90664445c3a6f74843be62f778+34&branch=main}} Figure \ref{fig:ghcommitbrowser} shows how to use the GitHub UI to examine the code changes made in each commit. An excerpt of the change is shown in listing \ref{code:firstdiff}.

\begin{figure}[!ht]
	\centering
	\subfloat[GitHub commit browser]{\includegraphics[width=0.65\textwidth]{./images-misc/ghcommitbrowser.png}}\\
	\subfloat[View file and line changes in each commit]{\includegraphics[width=0.65\textwidth]{./images-misc/ghcommitsingle.png}}
	\caption{How to browse the individual steps of the torch porting effort}
	\label{fig:ghcommitbrowser}
\end{figure}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{diff}{./diffs/diff1.txt}
\end{inputminted}
  \caption{Simple examples of porting from numpy to torch}
  \label{code:firstdiff}
\end{listing}

Next, the list and generator for the forward operation of the transform, which is one of the performance hotspots, were converted to a preallocated tensor, shown in the listing \ref{code:seconddiff}. The significant commit which introduced this change is here.\footnote{\url{https://github.com/sevagh/nsgt/commit/49a70d112ba4091d271a9438a8c38cf380ce62b4}}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{diff}{./diffs/diff2.txt}
\end{inputminted}
  \caption{Excerpt of the conversion of lists, for loops, and generators of the forward NSGT to torch tensors and parallel matrix operations}
  \label{code:seconddiff}
\end{listing}

In this section, only short excerpts of the torch port were shown, due to requiring to display hundreds of lines of code to show the full effort. Instead, the workflow for how to use the GitHub UI for browsing the individual steps of the torch port was described in figure \ref{fig:ghcommitbrowser}. This list describes several important commits:

\begin{tight_enumerate}
	 \item
		 Beginning the torch port of the sliced NSGT (or sliCQT):\\
		 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/ce9785d40d1053fcffc72115d5fbdcb28d44bf7c}}
	 \item
		 Optimizing the main loop of the forward transform:\\
		 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/49a70d112ba4091d271a9438a8c38cf380ce62b4}}
	 \item
		 Adding a configurable device (CPU/GPU) to the sliCQT for finer user control:\\
		 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/2d41fe002d95e4dbd1cdb76ef24aa03c95cfbe46}}
	 \item
		 Optimizing the backward/inverse transform:\\
		 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/9552aa1eb0735e93ce91b648a5b7f629066693b3}}
	 \item
		 Returning tensors (i.e. ndarrays or matrices) instead of generators:\\
		 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/1a5886bced951af132ab8659d30a565bcade87b2}}
	 \item
	 	 Supporting torch in the non-sliced NSGT:\\
	 	 {\scriptsize \url{https://github.com/sevagh/nsgt/commit/8d873f7dc8053c553664dcc5c0816eaf3e1ddae0}}
\end{tight_enumerate}

\newpagefill

\subsubsection{Matrix and ragged forms}
\label{sec:matrixvragged}

Although the CQT implementation they are referencing is different from the NSGT, \textcite[1]{klapuricqt} describe that
\begin{quote}
	[the] CQT produces a data structure that is more difficult to work with than the time-frequency matrix (spectrogram) obtained by using Short-Time Fourier transform in successive time frames. The last problem is due to the fact that in CQT, the time resolution varies for different frequency bins, in effect meaning that the ``sampling'' of different frequency bins is not synchronized.
\end{quote}

This same statement also applies to the NSGT, where different frequency bins have a different time resolution. An illustration is shown in figure \ref{fig:raggedslicqt}. In the original list-based design of the library, a list of time-frequency coefficients per frequency bin is returned, and a parameter called \Verb#matrixform# ensures that each item in the list of time-frequency coefficients has the same length with zero-padding.\footnote{\url{https://github.com/grrrr/nsgt/blob/master/nsgt/nsgtf_loop.py\#L51}}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-misc/slicq_shape.png}
	\caption{Illustration of the different time resolution per frequency bin of the NSGT}
	\label{fig:raggedslicqt}
\end{figure}

After the port to torch, the matrix form in the forked NSGT/sliCQT library was simply a tensor where each row represented a frequency bin, and the total number of columns represented the maximum time resolution from the highest frequency bin. Every lower frequency bin had zero-padding applied to have a row of time coefficients with the same length as the highest frequency bin. The non-matrix form could be referred to the ragged or jagged form, from its non-rectangular shape.

The following commit\footnote{\url{https://github.com/sevagh/nsgt/commit/4fb0f55350bc4c2a6fa254d9b7dfd51933ad5540}} introduced a better ragged/jagged form in the torch port. Just as shown in figure \ref{fig:raggedslicqt}, frequency bins that share the same time resolution are grouped together in a rectangular matrix. This results in a list of tensors or matrices, where each tensor contains a block of frequency bins that share the same time-frequency resolution, and thus have the same length of time coefficients. We avoid zero-padding and work with meaningful time coefficients in each frequency bin.

There is a provided example script, \Verb#ragged_vs_matrix.py#,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/examples/ragged_vs_matrix.py}} which shows how to work with the ragged or jagged form instead of the matrix. Listing \ref{code:raggedvmatrix} shows an example and the printed output for the matrix and ragged forms.

\begin{figure}[h]
  \centering
 \begin{minipage}{\textwidth}
  \centering
\setlength\partopsep{-\topsep}
\begin{minted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}
if args.matrixform:
    print(f'NSGT-sliCQ matrix shape: {c.shape}')
else:
    print(f'NSGT-sliCQ jagged shape:')
    freq_idx = 0
    for i, C_block in enumerate(c):
        freq_start = freq_idx
        freq_idx += C_block.shape[2]
        print(f'\tblock {i}, f {freq_start}: {C_block.shape}')
\end{minted}
 \subfloat{(a) Code snippet showing the matrix and ragged forms}
 \vspace{1em}
 \end{minipage}
 \begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|\$ python examples/ragged\_vs\_matrix.py --scale bark --bins 20 --fmin 20 --fmax 22050 --matrixform ./gspi.wav|
|NSGT-sliCQ matrix shape: torch.Size([401, 2, 21, 264])|
|recon error (mse): 1.087067076355197e-07|
\end{minted}
 \subfloat{(b) Output printed by above for the matrix form}
 \vspace{1em}
 \end{minipage}
\begin{minipage}{\textwidth}
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
|\$ python examples/ragged\_vs\_matrix.py --scale bark --bins 20 --fmin 20 --fmax 22050 ./gspi.wav|
|NSGT-sliCQ jagged shape:|
|        block 0, f 0: torch.Size([401, 2, 8, 16])|
|        block 1, f 8: torch.Size([401, 2, 1, 20])|
|        block 2, f 9: torch.Size([401, 2, 1, 24])|
|        block 3, f 10: torch.Size([401, 2, 1, 32])|
|        block 4, f 11: torch.Size([401, 2, 1, 40])|
|        block 5, f 12: torch.Size([401, 2, 1, 48])|
|        block 6, f 13: torch.Size([401, 2, 1, 60])|
|        block 7, f 14: torch.Size([401, 2, 1, 76])|
|        block 8, f 15: torch.Size([401, 2, 1, 96])|
|        block 9, f 16: torch.Size([401, 2, 1, 120])|
|        block 10, f 17: torch.Size([401, 2, 1, 152])|
|        block 11, f 18: torch.Size([401, 2, 1, 188])|
|        block 12, f 19: torch.Size([401, 2, 1, 236])|
|        block 13, f 20: torch.Size([401, 2, 1, 264])|
|recon error (mse): 1.087067076355197e-07|
\end{minted}
 \subfloat{(c) Output printed by above for the ragged form}
 \vspace{1em}
 \end{minipage}
  \captionof{listing}{Matrix and ragged forms of a sliCQT using 20-22050 Hz on the Bark scale with 20 frequency bins}
  \label{code:raggedvmatrix}
\end{figure}

Note the shape of the matrix form is $401 \text{ slices} \times 2 \text{ channels} \times 21 \text{ frequency bins} \times 264 \text{ time coefficents}$. The shape of the first matrix in the ragged form is $401 \text{ slices} \times 2 \text{ channels} \times 8 \text{ frequency bins} \times 16 \text{ time coefficents}$. This is the lowest time-frequency resolution of this sliCQT, with only outputs 16 time coefficients, shared by the first 8 frequency bins. In the matrix form, these 16 values are padded up to 264 to fit the maximum time resolution. The total summed frequency bins of the list of matrices in the ragged form is 21, leading to an identical transform. Also note the identical reconstruction error from both forms.

\subsubsection{Slice overlap-add utility}
\label{sec:slicqola}

In section \ref{sec:theoryslicqt}, the sliCQT was described to have a 50\% overlap between adjacent slices, and that overlap-adding the slices was necessary to generate a good spectrogram. Figure \ref{fig:slicqoverlaps} showed the difference between the overlap-added spectrogram and one where adjacent slices were simply joined without any overlap.

The next addition to the slice utilities was to include a utility function for the 50\% overlapping of adjacent slices,\footnote{\url{https://github.com/sevagh/nsgt/blob/main/nsgt/slicq.py\#L38}} where the overlap length is automatically inferred from the shape of the transform. The function is named \Verb#overlap_add_slicq# and is imported from the \Verb#nsgt.slicq# file. This function was included in the original project in the example spectrogram code.\footnote{\url{https://github.com/grrrr/nsgt/blob/master/examples/spectrogram.py\#L19}}

We believe that due to necessity of the 50\% overlap to create a meaningful spectrogram, the overlap-add utility should be included in the core of the library. In the spectrogram example of the forked library, the new \Verb#overlap_add_slicq# function is imported from the core library.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/examples/spectrogram.py\#L19}} Figure \ref{fig:overlappedspectrograms} compares the non-sliced NSGT spectrogram with the overlap-added sliCQT spectrogram.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT (non-sliced), length is exact same as input signal. No overlap-add required]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_mel_nooverlap.png}}
	\hspace{0.1em}
	\subfloat[sliCQT with minimum automatic sllen (6744) and 50\% overlap-add]{\includegraphics[width=0.475\textwidth]{./images-gspi/gspi_nsgt_mel_perfect_slice.png}}
	\caption{NSGT spectrograms for the mel scale with 96 bins in 20--22050 Hz}
	\label{fig:overlappedspectrograms}
\end{figure}

The overlap-add procedure for the matrix form of the sliCQT operates on the whole matrix, and for the ragged transform, it needs to be applied to each of the tensors in the list.

\newpagefill

\subsubsection{Spectrogram plotting utility}
\label{sec:slicqspec}

\newpagefill

\subsection{Choosing sliCQT parameters}
\label{sec:slicqparamsrch}

The initial exploration is on whether the sliCQT is suitable as an STFT replacement for music demixing with magnitude spectrogram masking, since this is how Open-Unmix works. A simple approach is to experiment with the sliCQT replacing the STFT inside oracle mask estimators, which were described in \ref{sec:masksandoracles}. If the oracle experiments show promising results, such as the sliCQT oracle performance surpassing STFT oracle performance, it's a good first step towards replacing the STFT with the sliCQT in a neural network.

To recap, the oracle mask is a perfect time-frequency mask which is computed from ground-truth training data. The oracle is used to give an idea of the upper limit of audio quality from an algorithm or model for music demixing. The mask is typically applied to the magnitude transform (\cite{umx}), and the phase is discarded.

In the parameter exploration, we require the following components to build the oracle testbench:

\begin{tight_enumerate}
	\item
		Python MUSDB18-HQ (\cite{musdb18hq}) loader library\footnote{\url{https://github.com/sigsep/sigsep-mus-db}}
	\item
		Python BSS metrics (\cite{bss}) evaluator library\footnote{\url{https://github.com/sigsep/sigsep-mus-eval/}} by the SigSep community
	\item
		Reference Python NSGT/sliCQT library\footnote{\url{https://github.com/grrrr/nsgt}}
\end{tight_enumerate}

\subsubsection{Mix-phase inversion (MPI) oracle}
\label{sec:mpi}

\textbf{N.B.} The words TFTransform (and iTFTransform for the inverse or backward transform) will be used to refer to either the STFT or sliCQT in the equations shown in this section, since both are complex-valued, invertible time-frequency transforms with Fourier coefficients

As shown in section \ref{sec:masksandoracles}, the definition of the IRM1 oracle based on the STFT, or ideal ratio/soft mask with the magnitude spectrogram raised to the first power, for a mixed song waveform $x_{\text{mix}}[n]$ and one of its isolated sources $x_{\text{source}}[n]$, and the subsequent oracle-estimated source waveform $\hat{x}_{\text{source}}[n]$ are computed with the following equations in \eqref{equation:irm1}:
\begin{align}\tag{1}\label{equation:irm1}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n]), X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), |X_{\text{source}}| = \text{abs}(X_{\text{source}})\\
	\nonumber & \text{IRM1}_{\text{source}} = \frac{|X_{\text{source}}|^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, IRM1}} = X_{\text{mix}} \cdot \text{IRM1}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, IRM1}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, IRM1}})
\end{align}

Similar to the IRM1 calculation, let us examine the real case where we only have the mix available, and the magnitude spectrogram of the source was estimated by a neural network. Let us call this $|X_{\text{source}}|_{\text{est}}$, and see how the source waveform is computed with ratio masks in equations \eqref{equation:softneural}:
\begin{align}\tag{2}\label{equation:softneural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & \text{EstRatioMask}_{\text{source}} = \frac{{|X_{\text{source}}|_{\text{est}}}^{1}}{|X_{\text{mix}}|^{1}}\\
	\nonumber & \hat{X}_{\text{source, est}} = X_{\text{mix}} \cdot \text{EstRatioMask}_{\text{source}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

This is the soft mask output of Open-Unmix, which is supported by one of the configuration options. However, by default Open-Unmix prefers to use the estimated magnitude and the phase of the mixture, over soft masks. The source waveform is estimated using the source magnitude spectrogram and mix phase spectrogram, and show in equations \eqref{equation:mpineural}:
\begin{align}\tag{3}\label{equation:mpineural}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{NeuralNetwork}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, est}})
\end{align}

In other words, the magnitude of the isolated source is combined with the phase, or the angle, of the mixed waveform, to produce a complex time-frequency transform, which is then inverted with the backward transform to obtain the estimated isolated source waveform. There are some mentions in demixing or source separation literature for the ``noisy phase'' oracle (\cite{noisyphase1, noisyphase2}), which is a more speech-based definition -- recall that in speech separation, a common task is to separate speech from noise signals. In music demixing, referring to interfering musical instruments as ``noise'' is inappropriate, so we should call it the mix-phase inversion (or MPI) oracle.

Let us write out the equations for the MPI oracle derived from the mixed-phase neural network estimate equations from \eqref{equation:mpineural}. The MPI oracle can be computed from ground-truth data, using the equations \eqref{equation:mpioracle}:
\begin{align}\tag{4}\label{equation:mpioracle}
	\nonumber & X_{\text{mix}} = \text{TFTransform}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & X_{\text{source}} = \text{TFTransform}(x_{\text{source}}[n])\\
	\nonumber & |X_{\text{source}}| = \text{abs}(X_{\text{source}}), \measuredangle{X_{\text{source}}} = \text{angle}(X_{\text{source}})\\
	\nonumber & \hat{X}_{\text{source, MPI}} = |X_{\text{source}}| \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, MPI}}[n] = \text{iTFTransform}(\hat{X}_{\text{source, MPI}})
\end{align}

This is the first estimate of the mix-phase inversion strategy, and does not assume further post-processing of the estimates. It gives us the idea of the performance of a time-frequency transform of an isolated source when the phase is discarded and the magnitude is combined with the phase of the mixture.

The mix-phase inversion oracle (MPI) score is the target we will choose to maximize when searching for sliCQT parameters to beat the STFT.

\subsubsection{MPI oracle parameter search testbench}
\label{sec:mpiparam}

The splitting of a dataset into training, test, and validation sets was covered in section \ref{sec:ml}. To recap, the training set is used by the model to make predictions from the input and comparing its predicted output to the ground truth training output. The test set must be unseen during training, and the model's performance is tested on this set at the end of each training iteration (also called epoch) to show the generalization performance of the network. The validation set is also a set of data unseen in training, which can be used for the hyperparameter search.

In Open-Unmix, the STFT window size is set to 4096 by default (with a 1024 overlap), and it is a hyperparameter of the network. Therefore, the sliCQT parameters should equivalently be considered a hyperparameter in xumx-sliCQ. The MPI oracle testbench should use the predefined validation set of MUSDB18-HQ, which consists of 14 tracks. sliCQT parameters should be selected and be evaluated on their MPI score using BSS metrics for the 14 validation tracks.

The performance of the IRM1 (soft magnitude mask) and MPI (mixed-phase inversion) oracles will be shown for the best (and worst) sliCQT parameters, alongside different STFT window sizes. The intent is to show how the window size and time-frequency resolution of the STFT and sliCQT affect music demixing results per target (\cite{tftradeoff1}).

According to \textcite{randomgrid}, 60 iterations of random grid will find a statistically good set of hyperparameters in a case where an exhaustive grid search is impossible. The strategy chosen was to choose random hyperparameters for 60 iterations and select the highest MPI score, because an exhaustive grid search of the possible NSGT/sliCQT parameters would be too large. 

The random search parameters are shown in table \ref{table:slicqparams}. Several decisions were made to limit the size of the parameter search.

The maximum bins were capped to 300 and the maximum slice length was capped to 44100 samples, which represents 1 second of music. This is a nice round number, and re-appears in several demixing papers (\cite{plumbley1, plumbley2, demucs}) as a good subsequence length to extract from a larger musical track. Although the NSGT/sliCQT can be configured to output thousands of frequency bins and use arbitrarily large slice lengths, larger sizes (and the corresponding large slice length) will lead to higher-dimension outputs that are more difficult to use in a neural network.

The maximum frequency was capped to 22050 Hz, the Nyquist rate of the 44100 sample rate of MUSDB18-HQ. This is for performance reasons -- the NSGT/sliCQT automatically adds additional frequency bins for the DC (0 Hz) and Nyquist ($\sfrac{f_{s}}{2} \text{ Hz}$) frequencies if they are missing from the user-specified scale, and having two large, adjacent upper frequency bins (e.g. say that the user sets 17000 Hz, and 22050 Hz is the automatically-added Nyquist) creates a very performance-intensive transform with huge dimensionality which does not fit on the GPU of the test machine (with 12GB of memory).

The minimum frequency range (10.0--130.0 Hz) was inspired by one of the sliCQT papers (\cite{slicq}) which demonstrated the sliCQT with those frequency ranges.

The slice length and transition length were automatically picked for the randomly selected frequency scale, making use of the new feature whose addition was described in section \ref{sec:autosllen}.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|l| }
	 \hline
	 Scale & Bins & Minimum frequency (Hz) & Additional params \\
	 \hline
	 \hline
	 Constant-Q/log & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Variable-Q & 10--300 & 10.0--130.0 & Offset = 10.0--130.0 Hz \\
	 \hline
	 \hline
	 mel & 10--300 & 10.0--130.0 & n/a \\
	 \hline
	 Bark & 10--300 & 10.0--130.0 & n/a \\
	 \hline
\end{tabular}
	\caption{Parameter ranges for the sliCQT hyperparameter search}
	\label{table:slicqparams}
\end{table}

The maximized metric is the median SDR score across the 4 targets of the 14 validation tracks of MUSDB18-HQ. The SDR is a score in dB which commonly is the single evaluation metric that summarizes the global performance of a music demixing system, which was described as part of the total set of BSS metrics in section \ref{sec:evalbss}, and which was the objective metric used to rank demixing systems in the ISMIR 2021 Music Demixing Challenge (\cite{mdx21}).\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}}

\newpagefill

\subsubsection{GPU-accelerated BSS metrics with CuPy}
\label{sec:fasterbsscupy}

During the oracle evaluation, line-profiler showed that the slowest step was the BSS metrics calculation from the sigsep-mus-eval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} package. Listing \ref{lst:slowbssprofile} shows some of the line-profiler output of the IRM1 oracle mask with the STFT, using a window size of 4096, on 1 out of the 14 MUSDB18-HQ validation tracks, with the \Verb#@profile# decorator placed on the BSS inner computation functions.\footnote{\url{https://github.com/sigsep/sigsep-mus-eval/blob/master/museval/metrics.py\#L512-L601}}

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
   527         2    2217873.0 1108936.5     14.7      sf = scipy.fftpack.fft(reference_sources, n=n_fft, axis=2)

   538        46    1513499.0  32902.2     10.0          ssf = sf[j, c2] * np.conj(sf[i, c1])

   539        46   10838284.0 235614.9     71.8          ssf = np.real(scipy.fftpack.ifft(ssf))

   574        12    4278634.0 356552.8     12.2      sef = scipy.fftpack.fft(estimated_source, n=n_fft)

   582       104   24634279.0 236868.1     70.0          ssef = np.real(scipy.fftpack.ifft(ssef))

   591        24    2432539.0 101355.8      6.9          C = np.linalg.solve(G + eps*np.eye(G.shape[0]), D).reshape(
\end{minted}
  \caption{Some of the slowest lines in the BSS metrics evaluation}
  \label{lst:slowbssprofile}
\end{listing}

The bottlenecks of the library use NumPy and SciPy operations, which are Python numerical computation libraries that use the CPU. The CuPy library\footnote{\url{https://cupy.dev/}} provides replacement functions of NumPy and SciPy which run on NVIDIA GPUs using the CUDA compute framework.\footnote{\url{https://developer.nvidia.com/cuda-toolkit}} This allows for acceleration of numerical operations through GPU parallelization. Note that although PyTorch (\cite{pytorch}) also uses CUDA and was used in an earlier section \ref{sec:torchslicq}, for the BSS evaluation library CuPy was easier to work with as a drop-in NumPy/SciPy replacement.

Similar to the NSGT/sliCQT library, the modifications made to the museval library were stored in my forked copy of the repository.\footnote{\url{https://github.com/sevagh/sigsep-mus-eval/commit/f3b7540faddc8d2b1bc91cecaf05fd20d96df7c6}} The replacement of the NumPy or SciPy function is as simple as using the CuPy equivalent, taking special care about GPU out-of-memory errors. Code listing \ref{code:thirddiff} shows an example of the CuPy replacement of one of the slow computations discovered by the profiler.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{diff}{./diffs/diff3.txt}
\end{inputminted}
  \caption{Example of porting SciPy to CuPy with an out-of-memory fallback}
  \label{code:thirddiff}
\end{listing}

\newpagefill

\subsection{Open-Unmix with the sliCQ Transform}
\label{sec:slicqumx}

First, we will look at incorporating CrossNet-Open-Unmix (X-UMX) (\cite{xumx}) ideas with Open-Unmix. Since X-UMX and UMX are both based on the STFT spectrogram, the general model will stay the same even if we replace the STFT with the sliCQT.

As mentioned, Open-Unmix (\cite{umx}) has a reference PyTorch implementation\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} on which xumx-sliCQ, the neural network of this thesis, will be based. X-UMX has an open-source implementation available as well,\footnote{\url{https://github.com/sony/ai-research-code/tree/master/x-umx}} which uses NNabla,\footnote{\url{https://nnabla.org/}} a different Python deep learning framework. Therefore part of the methodology of this thesis involves incorporating the X-UMX ideas inside the PyTorch version of UMX.

After the X-UMX modifications are described, we will look at which parts of Open-Unmix need to incorporate the PyTorch implementation of the sliCQT that was described in section \ref{sec:torchslicq}.\footnote{\url{https://github.com/sevagh/nsgt}} Finally, the convolutional denoising autoencoder model architectures of \textcite{plumbley1, plumbley2}  will be described, along with how they might be applied to the sliCQT.

\subsubsection{Incorporating CrossNet-Open-Unmix}
\label{sec:xumxinc}

CrossNet-Open-Unmix (X-UMX) (\cite{xumx}) is a further development on Open-Unmix which improves the music demixing performance without adding any additional trainable parameters. All 4 target models are trained simultaneously instead of separately, which allows for loss functions that consider multiple targets. However, it requires more compute resources for training, since the GPU needs to store and train 4 independent target models simultaneously instead of separately.

For simplicity, having a single training loop and single trained model to perform the separation is easier to consider than 4 independent models. For this reason, and that the final music demixing performance of X-UMX is higher than UMX, we chose to use the X-UMX variant of Open-Unmix.

In Open-Unmix, the loss function is only for the single target being trained, and only applied by the mean squared error (MSE) function on the magnitude STFT coefficients, making it a loss of the frequency domain. The MSE loss of the magnitude STFT is computed from the following equation, where $Y$ represents the ground truth magnitude STFT, $\hat{Y}$ represents the network estimate, and there are $n$ total elements in the STFT matrix:
\begin{align}
	\nonumber & \text{MSE}(Y, \hat{Y}) = \frac{1}{n} \sum_{i = 1}^{n}{(Y_{i}-\hat{Y}_{i})^{2}}
\end{align}

The loss function of X-UMX is first modified to include a multi-domain loss (MDL), which refers to the existing frequency-domain loss (spectrogram MSE) in addition to a new time-domain loss. The time-domain loss is in fact the SDR score from the BSS metrics which we have seen before. Recall the equations from section \ref{sec:mpi} on the mixed-phase inversion (MPI) oracle that show how the magnitude STFT of the Open-Unmix neural network is converted back to the time-domain waveform estimate for a target (or source):
\begin{align}
	\nonumber & X_{\text{mix}} = \text{STFT}(x_{\text{mix}}[n])\\
	\nonumber & |X_{\text{mix}}| = \text{abs}(X_{\text{mix}}), \measuredangle{X_{\text{mix}}} = \text{angle}(X_{\text{mix}})\\
	\nonumber & {|X_{\text{source}}|}_{\text{est}} = \text{Open-Unmix}(x_{\text{mix}})\\
	\nonumber & X_{\text{source, est}} = {|X_{\text{source}}|}_{\text{est}} \cdot \measuredangle{X_{\text{mix}}}\\
	\nonumber & \hat{x}_{\text{source, est}}[n] = \text{iSTFT}(\hat{X}_{\text{source, est}})
\end{align}

We can use these equations in the loss function to convert from the estimated magnitude spectrogram of the neural network to the waveform estimate. In xumx-sliCQ, we use the auraloss Python package,\footnote{\url{https://github.com/csteinmetz1/auraloss}} which implements the SI-SDR (scale-invariant SDR) (\cite{roux2018sdr}) loss function for time-domain audio waveforms. SI-SDR is a simpler and more robust variant of SDR which improves on some failure cases of SDR (\cite{roux2018sdr}).

We will call this one $\text{sdr\_loss}$ to denote time-domain loss which is applied to waveforms $x$ and $\hat{x}$ (the ground-truth and estimated waveform respectively). The SI-SDR loss is computed as follows:
\begin{align}
	\nonumber & \text{SI-SDR}(x, \hat{x}) = 10 \log_{10}\Big(\frac{||\frac{\hat{x}^{T}x}{||x||^{2}} x||^{2}}{||\frac{\hat{x}^{T}x}{||x||^{2}} x - \hat{x}||^{2}}\Big)
\end{align}

There are 14 MSE loss combinations made from the 4 targets to implement the combination loss (CL). In the spectral/frequency domain, we will use $\text{mse\_loss}$ as shorthand for frequency-domain loss. The equations \eqref{equation:cl} show the computation of the total frequency-domain loss, where $x$ represents the time-domain waveform, the subscript represents the target (1--4 for vocals, drums, bass, and other):
\begin{align}\tag{5}\label{equation:cl}
	\nonumber & \text{mse\_loss}_{1} = \text{MSE}(|\text{STFT}(\hat{x}_{1})|, |\text{STFT}(x_{1})|)\\
	\nonumber & \text{mse\_loss}_{2} = \text{MSE}(|\text{STFT}(\hat{x}_{1} + \hat{x}_{2})|, |\text{STFT}(x_{1} + x_{2}|))\\
	\nonumber & \text{(... repeat for 14 combinations)}\\
	\nonumber & \text{mse\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{mse\_loss}_{n}}
\end{align}

For the time-domain loss, the same 14 combinations of the combination targets are considered, shown in the equations \eqref{equation:mdl}, where $x$ represents the time-domain waveform, the subscript represents the target:
\begin{align}\tag{6}\label{equation:mdl}
	\nonumber & \text{sdr\_loss}_{1} = \text{SI-SDR}(\hat{x}_{1}, x_{1})\\
	\nonumber & \text{sdr\_loss}_{2} = \text{SI-SDR}(\hat{x}_{1} + \hat{x}_{2}, x_{1} + x_{2})\\
	\nonumber & \text{(... repeat for 14 combinations)}\\
	\nonumber & \text{sdr\_loss}_{\text{total}} = \frac{1}{14} \cdot \sum_{n = 1}^{14}{\text{sdr\_loss}_{n}}
\end{align}

As a last step, the time-domain loss is multiplied by a mixing coefficient, $\alpha$, before adding it to the frequency-domain loss to create the total loss function:
\begin{align}
	\nonumber & \text{loss}_{X-UMX} = \text{mse\_loss}_{\text{total}} + \alpha \cdot \text{sdr\_loss}_{\text{total}}
\end{align}

 A visualization of the multi-domain loss (MDL) and combination loss (CL) for 4 targets is shown in figure \ref{fig:xumxlosses}.

\begin{figure}[ht]
	\centering
	\subfloat[Multi-domain loss (MDL)]{\includegraphics[width=0.75\textwidth]{./images-neural/xumx1.png}}\\
	\subfloat[Combination loss (CL)]{\includegraphics[width=0.75\textwidth]{./images-neural/xumx2.png}}
	\caption{Loss functions of X-UMX (\cite[2]{xumx})}
	\label{fig:xumxlosses}
\end{figure}

The data loader of Open-Unmix returns a single target at a time, or an $(x, y)$ pair where $x$ is the mixed waveform and $y$ is the desired target. With CrossNet-Open-Unmix, the MUSDB18-HQ data loader needs to return $(x, y_{1}, y_{2}, y_{3}, y_{4})$ in training, where $y_{1-4}$ are the ground-truth waveforms of all 4 isolated targets.

\newpagefill

\subsubsection{Replacing the STFT with the sliCQT}
\label{sec:replacestft}

Continuing from the previous statement, the data loader of Open-Unmix returns a single target at a time, or an $(x, y)$ pair where $x$ is the mixed waveform and $y$ is the desired target. Also, the returned estimate of Open-Unmix is the estimated source waveform $\hat{y}$. However, during most of the steps between the loading of the time-domain waveforms and the return of the estimated time-domain waveform, the STFT is used to transform the audio into the time-frequency or spectral domain.

The MUSDDB18-HQ data loader is the starting point of the neural network, and the important training and inference steps are as follows:

\begin{tight_enumerate}
	\item
		The data loader is instantiated,\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/scripts/train.py\#L222}} ready to return $(x, y)$ pairs of waveforms for every training example.
	\item
		The \Verb#get_statistics#\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/scripts/train.py\#L56}} function is applied, which computes the mean and standard deviation from the magnitude STFTs of the entire training dataset. \textcite[309]{Kessy_2018} describes data whitening as a process that ``greatly simplifies multivariate data analysis both from a computational and a statistical standpoint,'' and that ``whitening is a critically important tool, most often employed in preprocessing.''
	\item
		The $(x, y)$ pairs have their magnitude STFT taken in the training loop.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/scripts/train.py\#L23}} The magnitude STFTs, denoted $(|X|, |Y|)$ are passed into the inner neural network. $|X|$ is modified with $|X| = \text{std}*(|X|+\text{mean})$, where the std and mean are from the data whitening step.
	\item
		The whitened $|X|$ is used as an input to the neural network layers, which as mentioned in section \ref{sec:umx} is a bi-directional LSTM.
	\item
		The estimate of the neural network is $|\hat{Y}|$. The loss function is $\text{MSE}(|Y|, |\hat{Y}|)$, and this is used as the optimization target for the parameter updates.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/scripts/train.py\#L34}}
	\item
		This process is repeated until the network is fully trained (i.e. the loss stops improving), and 4 independent networks are each trained on the 4 MUSDB18-HQ sources (vocals, drums, bass, other) respectively.
	\item
		The inference step of the neural network is when Open-Unmix loads all 4 independently trained networks to create time-domain estimates of the sources.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/openunmix/model.py\#L258}}
	\item
		For a given mixed waveform $x$, Open-Unmix estimates $|Y_{1}|, |Y_{2}|, |Y_{3}|, |Y_{4}|$ for each source. Then, a process called Wiener filtering and expectation maximization is applied (\cite{umxorig1, wiener2, wiener3, wiener4}).\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/openunmix/filtering.py\#L338}} First the mix-phase inversion is used to come up with the first waveform estimate $y_{1}, y_{2}, y_{3}, y_{4}$. Then, the iterative Wiener expectation maximization process uses the 4 target STFT spectrograms in an iterative procedure to refine the estimates.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch/blob/master/openunmix/filtering.py\#L154}}
\end{tight_enumerate}

Let us recap where the STFT is used in the above steps. The STFT is used to compute magnitude spectrogram statistics from the training data. The magnitude STFT of the training waveform $x$ is taken and passed as an input to the bi-directional LSTM. Then, the output estimated magnitude STFT from the 4 target models are considered together to perform an iterative Wiener-EM refinement step in the STFT domain. Finally, the estimates are transformed into time-domain waveforms with the iSTFT and the mix-phase inversion.

These are are the places in the Open-Unmix PyTorch template where the STFT must be replaced with the sliCQT. Moreover, it was mentioned that the ragged sliCQT will be used, which is a list of time-frequency matrices for frequency bins that share the same time resolution, and not a single rectangular matrix like the STFT. This arises from the different temporal frame rates of each frequency bin which is the desire of the NSGT/sliCQT (or CQT). This means that all of the operations which expect a single rectangular real-valued matrix of the STFT returns need to be modified to work on a list of matrices instead.

Figure \ref{fig:xumxslicq} shows the proposed modification to the basic Open-Unmix architecture, which shows where the sliCQT is used. It also shows how the 4 models are trained together to apply the X-UMX loss functions.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-blockdiagrams/xumx_slicq_system_compressed.png}
	\caption{Block diagram of xumx-sliCQ architecture}
	\label{fig:xumxslicq}
\end{figure}

In section \ref{sec:matrixvragged}, we saw that to use the ragged sliCQT one must simply loop over the list of matrices where each matrix (or block) contains at least 1 frequency bin that share a specific time-frequency resolution. A simple example of the STFT to sliCQT conversion is the custom MSE loss function\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/loss.py\#L8}} shown in listing \ref{code:simplemse}.

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{python}
|def \_custom\_mse\_loss(pred\_magnitude, target\_magnitude):|
|    loss = 0|
|    for i in range(len(target\_magnitude)):|
|        loss += torch.mean((pred\_magnitude[i] - target\_magnitude[i])**2)|
|    return loss/len(target\_magnitude)|
\end{minted}
  \caption{Modified MSE loss for the ragged sliCQT}
  \label{code:simplemse}
\end{listing}

The MSE for the STFT is applied on the whole matrix. For the ragged sliCQT, we simply sum the MSE loss of each matrix in the list of matrices.

The \Verb#get_statistics# function\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/scripts/train.py\#L102}} for the data whitening step was modified to return a list of standard deviations and means, where each entry of the list corresponds to the standard deviation and mean of the respective matrix in the ragged sliCQT. Note that the sliCQT is overlap-added before computing its statistics. This change is shown in listing \ref{code:fourthdiff}. Recall in sections \ref{sec:theoryslicqt} and \ref{sec:slicqola} that the 50\% overlap of adjacent slices was necessary for a meaningful spectrogram. The sliCQT is almost always overlap-added before being used in xumx-sliCQ.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{diff}{./diffs/diff4.txt}
\end{inputminted}
  \caption{Modifying the data whitening step of Open-Unmix}
  \label{code:fourthdiff}
\end{listing}

The neural network was modified to instantiate an inner network\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L150-L187}} for each matrix in the ragged sliCQT. The class name for the total neural network is called \Verb#OpenUnmix#. The inner network for each sub-matrix of the sliCQT is called \Verb#OpenUnmixTimeBucket#, referring to how each matrix is a ``bucket'' of frequency bins that share the same time resolution. Finally, the \Verb#DummyTimeBucket# class is a network that lets a single ragged sliCQT bucket pass through unchanged. This is to respect the \Verb#bandwidth# parameter of Open-Unmix (16000 Hz by default), which lets the part of the STFT above 16000 Hz to pass through the network unmodified. Similarly for the ragged sliCQT, all of the matrices whose first frequency bin is above 16000 Hz passes through untouched. An excerpt of the neural network code is shown in listing \ref{code:raggedumx}.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/openunmixbuckets.py}
\end{inputminted}
  \caption{OpenUnmix with a network per time bucket in the ragged sliCQT}
  \label{code:raggedumx}
\end{listing}

Within each sub-network, or \Verb#OpenUnmixTimeBucket#, the implementation is similar to the STFT-based Open-Unmix. The $i$th block from the list of magnitude sliCQT $|X|$ of the mixed-song waveform $x$ , or $|X_{i}|$, is the input to the network, and it is overlap-added to produce a 2D time-frequency matrix. Then, the 2D time-frequency matrix can be used just like an STFT, as an input to an LSTM or any other neural network layer. An excerpt of the neural network code is shown in listing \ref{code:raggedumxoneblock}.  A necessary consideration in the network design is the non-invertibility of the overlap-add procedure, since the original transform shape is needed to synthesize an audio waveform. The neural network needs a strategy to compute a non-overlap-added estimate from the overlap-added mixed input spectrogram. This was achieved with an additional transpose convolutional layer, which will be covered in greater detail in the next section \ref{sec:convlayers}.

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/openunmixbucket.py}
\end{inputminted}
  \caption{Single network for one time bucket from the ragged sliCQT}
  \label{code:raggedumxoneblock}
\end{listing}

After the \Verb#OpenUnmix# class outputs the estimates of the magnitude sliCQT for all 4 targets, the post-processing Wiener expectation maximization step needs to be applied. The code for the Wiener-EM step is essentially unchanged from the original, since both the sliCQT and STFT output complex Fourier coefficients.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/filtering.py\#L290}} The inputs to the Wiener-EM function are the mixed complex spectrogram and a rectangular matrix containing the 4 target magnitude spectrograms.

The approach in xumx-sliCQ was to add a boolean parameter named \Verb#slicq_wiener#. If \Verb#slicq_wiener# is set to False, the first waveform estimate is taken from the sliCQT and the Wiener-EM step is applied on the STFT,\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L301}} shown in listing \ref{code:wienerstft}. If \Verb#slicq_wiener# is set to True, the Wiener-EM step is performed directly on the sliCQT.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L380}} The ragged sliCQT was zero-padded into a single rectangular matrix, allowing the original Wiener-EM code to be used (with minor modifications) for both the STFT and rectangular zero-padded sliCQT.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/xumx_slicq/model.py\#L389-L404}} The zero-padding operation is shown in listing \ref{code:wienerslicqt}. Each strategy results in a different music demixing quality and execution time, which will be discussed in section \ref{sec:experiment}. 

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/wiener_stft.py}
\end{inputminted}
  \caption{Using the STFT in the Wiener-EM post-processing step}
  \label{code:wienerstft}
\end{listing}

\begin{listing}[ht]
  \centering
\begin{inputminted}[linenos,breaklines,frame=single,fontsize=\scriptsize]{python}{./scripts/wiener_slicqt.py}
\end{inputminted}
  \caption{Zero-padding the sliCQT for the Wiener-EM post-processing step}
  \label{code:wienerslicqt}
\end{listing}

\subsubsection{Defining the convolutional layers}
\label{sec:convlayers}

The convolutional denoising autoencoder (CDAE) models of \textcite{plumbley1, plumbley2} were used in xumx-sliCQ. In the early experiments, they showed more promising results than the bi-LSTM model. The bi-LSTM model was first adapted to work on each time bucket of the ragged list of the sliCQT, but the longest time resolution (highest frequency bin) was prohibitively slow in both training and inference. Convolutional layers are a simple way of defining 2D time-frequency filters that slide over the 2D time-frequency matrices that correspond to different time buckets of the ragged sliCQT. The diagram in figure \ref{fig:cdaeslicqt} shows how the convolutional layers should be applied to the ragged sliCQT.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-blockdiagrams/xumx_slicq_pertarget.png}
	\caption{Block diagram of the individual convolutional blocks of xumx-sliCQ}
	\label{fig:cdaeslicqt}
\end{figure}

CDAEs are symmetric: a stack of convolutional layers in the encoder are applied to the input spectrogram to extract spectral audio features with sliding 2D (frequency filter, time filter) kernels. The corresponding reverse stack of transpose convolution (or deconvolution) layer in the decoder use the same size of filter (in reverse order) to grow the feature map back to the original spectrogram. For example, a 2-layer encoder/decoder might consist of the following layers: $\text{Conv}(11,42) \rightarrow \text{Conv}(3,5) \rightarrow \text{ConvTranspose}(3,5) \rightarrow \text{ConvTranspose}(11,42)$. Figure \ref{fig:convtranspose} shows how a convolutional and transpose convolutional or deconvolutional layer apply inverse operations.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{./images-neural/convtranspose.png}
	\caption{Deconvolution operation (\cite[11]{convtranspose})}
	\label{fig:convtranspose}
\end{figure}

To describe convolutional layers, \textcite[6]{convguide} state that ``images, sound clips and many other similar kinds of data have an intrinsic structure,'' and share the following properties:

\begin{tight_enumerate}
	\item
		They are stored as multi-dimensional arrays (e.g. the STFT or sliCQT)
	\item
		They feature one or more axes for which ordering matters (e.g., time and frequency for a spectrogram)
	\item
		The channel axis is used to access different views of the data (e.g., the left and right channels of a stereo audio track)
\end{tight_enumerate}

The STFT and sliCQT of stereo (2-channel) music produce a 2-channel spectrogram, such that the total dimensions of the 2D time-frequency spectrogram are $\text{time} \times \text{frequency} \times \text{channel}$. The 2D convolution kernels slide across the spectrograms to learn a feature representation, and the number of output channels determines how many feature maps are created. Using the values from \textcite{plumbley2}, the two layers of convolution in the CDAE of xumx-sliCQ use 25 and 55 channels respectively. This means that the stereo or 2-channel sliCQT has 25 output feature maps (or channels) computed from the first encoder convolutional layer, and 55 output feature maps from the second encoder convolutional layer. These are reversed in the decoder layer i.e., $2 \rightarrow \text{encoder}(25 \rightarrow 55) \rightarrow \text{decoder}(25 \rightarrow 2)$.

The kernel parameters define the size and movement of the 2D convolution kernel in the time and frequency dimensions in each output channel. Besides the kernel size, the kernel has these additional parameters: stride, dilation, and padding (\cite{convguide}). Figure \ref{fig:convdiags} shows the behavior of these different kernel parameters. The stride is the amount by which the kernel moves; a stride larger than one implies \textit{subsampling}, because it dictates how much of the output is retained (\cite{convguide}). The dilation defines ``holes'' or gaps in the kernel to increase the receptive field (\cite{convguide}), which is a cheap way to increase the amount of data points considered in a feature map. The padding defines zeros concatenated to the beginning and end of an axis.

\begin{figure}[ht]
	\centering
	\subfloat[2D convolution kernel]{\includegraphics[width=\textwidth]{./images-neural/conv1.png}}\\
	\subfloat[2D convolution kernel with padding > 0]{\includegraphics[width=\textwidth]{./images-neural/conv2.png}}\\
	\subfloat[2D convolution kernel with stride > 1]{\includegraphics[width=\textwidth]{./images-neural/conv3.png}}\\
	\subfloat[2D convolution kernel with dilation > 1]{\includegraphics[width=\textwidth]{./images-neural/conv4.png}}
	\caption{Different behaviors of kernel parameters (\cite[14, 29]{convguide})}
	\label{fig:convdiags}
\end{figure}

Different buckets in the ragged transform may have different dimensions for time and frequency, so the 2D convolution filter sizes are different depending on the input size. The training input is split into 1 second sequences, which is the same sequence size used in both CDAE papers (\cite{plumbley1, plumbley2}). This means that the ragged sliCQT dimensions (frequency bins and time coefficients) for 1 second of audio dictate the maximum size of time and frequency kernel that can be applied in each convolution layer. After each layer, a 2D batch normalization (BN) and nonlinear activation function, specifically the rectified linear unit (ReLU) activation, are applied (\cite{plumbley2}).

Note that after the encoder-decoder layers for the sliCQT, we need to add a ``growth layer.'' This is the inverse of the slice overlap-add procedure with learnable parameters. It uses a transpose convolution layer with a kernel size of $(1, 3)$ and a stride of $(1, 2)$ where the first dimension is frequency and the second is time. This means that for each frequency bin, the kernel slides and considers 3 time coefficients to produce 6 time coefficients. The final activation is a Sigmoid, which is real-valued $\in [0.0, 1.0]$. This makes the network estimate a soft or ratio mask which can then be applied to the input mixed magnitude spectrogram.

The exact parameters and configuration of the convolutional kernels and layers are described in tables \ref{table:convtable1}, \ref{table:convtable2}, \ref{table:convtable3}, \ref{table:convtable4}, and \ref{table:convtable5}. The parameters were initially chosen from the kernel values and layers in \textcite{plumbley1, plumbley2}. The parameters were repeatedly modified in an informal hyperparameter tuning process during my submissions to the ISMIR 2021 Music Demixing Challenge. The various tested networks can be viewed in the submission repository here.\footnote{\url{https://gitlab.aicrowd.com/sevagh/music-demixing-challenge-starter-kit}} The design was also constrained by the time limits of the network inference in the challenge. For example, dilations in the time dimension were chosen to efficiently increase the temporal receptive field on the sliCQT without slowing down the inference step.

\begin{table}[ht]
	\centering
	\begin{tabular}{ |l|l|l| }
	 \hline
		\# time coefficients & CDAE layer & Growth layer \\
	 \hline
	 \hline
		$\le 100$ & \makecell[l]{size=7\\stride=1\\dilation=2} & \makecell[l]{size=3\\stride=2\\dilation=1} \\
	 \hline
		$> 100$ & \makecell[l]{size=13\\stride=1\\dilation=2} & \makecell[l]{size=3\\stride=2\\dilation=1} \\
	 \hline
\end{tabular}
	\caption{Kernel parameters, time dimension}
	\label{table:convtable1}
	\vspace{1em}
\begin{tabular}{ |l|l|l| }
	 \hline
		\# frequency bins & CDAE layer & Growth layer \\
	 \hline
	 \hline
		$\le 10$ & \makecell[l]{size=1\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
		$\ge 10, < 20$ & \makecell[l]{size=3\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
		$\ge 20$ & \makecell[l]{size=5\\stride=1\\dilation=1} & \makecell[l]{size=1\\stride=1\\dilation=1} \\
	 \hline
\end{tabular}
	\caption{Kernel parameters, frequency dimension}
	\label{table:convtable2}
	\vspace{1em}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		Conv2d \makecell[l]{in=2\\out=25}, BN, ReLU & Conv2d \makecell[l]{in=25\\out=55}, BN, ReLU \\
	 \hline
\end{tabular}
	\caption{CDAE encoder layers}
	\label{table:convtable3}
	\vspace{1em}
\begin{tabular}{ |l|l|l|l|l| }
	 \hline
		Layer 1 & Layer 2 \\
	 \hline
	 \hline
		ConvTranspose2d \makecell[l]{in=55\\out=25}, BN, ReLU & ConvTranspose2d \makecell[l]{in=25\\out=2}, BN, ReLU \\
	 \hline
\end{tabular}
	\caption{CDAE decoder layers}
	\label{table:convtable4}
	\vspace{1em}
\begin{tabular}{ |l| }
	 \hline
		Growth layer \\
	 \hline
	 \hline
		ConvTranspose2d \makecell[l]{in=2\\out=2}, Sigmoid \\
	 \hline
\end{tabular}
	\caption{Growth (inverse 50\% overlap-add) layer}
	\label{table:convtable5}
\end{table}

\end{document}
