\documentclass[letter,12pt,notitlepage]{article}
\usepackage[left=4cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage[titletoc,title]{appendix}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{minted}
\usepackage{subfig}
\usepackage{titling}
\usepackage[compatibility=false]{caption}
\usepackage[parfill]{parskip}
\setlength{\droptitle}{1cm}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[
    %backend=biber, 
    natbib=true,
    style=numeric,
    sorting=none,
]{biblatex}
\addbibresource{citations.bib}
\input{variables.tex}

\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\newenvironment{tight_itemize}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{itemize}}

\newlength{\mintednumbersep}
\AtBeginDocument{%
  \sbox0{\tiny00}%
  \setlength\mintednumbersep{8pt}%
  \addtolength\mintednumbersep{-\wd0}%
}

\title{\ThesisTitle}

\author{\vspace{1em}\\Sevag Hanssian \\
  McGill University \\
 \small{\texttt{sevag.hanssian@mail.mcgill.ca}} \\
 \small{\texttt{sevagh@protonmail.com}} \\\ \\\ \\
 \small{Thesis for Master of Arts in Music Technology}\\
 \small{Date TBD, 2021}}

% nil out the auto date
\date{}

\begin{document}

\maketitle

\vspace{3.5em}

\begin{abstract}
	The short-time Fourier transform (STFT) is an important tool for the time-frequency analysis of acoustic signals. The STFT is commonly used as the input representation of music signals in deep learning models. Examples of music information retrieval (MIR) tasks where such models have achieved success are onset detection, beat tracking, and music source separation. Despite its ubiquity, the STFT has a fixed and bounded time-frequency resolution, such that one must sacrifice time for frequency resolution (or vice versa) by changing the window size. The Nonstationary Gabor Transform (NSGT) is an adaptive time-frequency transform which can vary its time-frequency resolution to better represent music signals. In this thesis, first the STFT and NSGT are described as tools for representing and manipulating music signals. Next, the STFT is replaced with different configurations of the NSGT in deep learning models for onset detection, beat tracking, and music source separation respectively, showing significant improvements in the results.
\end{abstract}

\vfill
\clearpage %force a page break

\tableofcontents

\vfill
\clearpage %force a page break

\listoffigures

\listoflistings

\vfill
\clearpage %force a page break

\section{Introduction}
\label{sec:intro}

\todo[inline]{core axioms here}

The DFT and FFT \cite{cooleytukey} are important \cite{dftimportant}. \todo{more stuff here}. Despite its importance, the DFT has some deficiencies in real-world music applications.

First, the human auditory system has a nonlinear frequency resolution, with finer frequency discrimination at low frequencies \todo{cite this}, but the DFT outputs linearly-spaced frequencies.

Second, real-world signals are typically sparse, with most of their energy concentrated in few frequency components \cite{sparseintro}, yet the DFT outputs values for many frequencies, most of which are discarded.

\subsection{Motivation}

Music, speech, and general signal processing literature often show improved results by substituting the standard DFT with a variant. Examples include improved music source separation \cite{betterbss}, improved speech recognition \cite{bettermfcc}, and better speech and music coding \cite{warpedcomparison} with the warped Fourier transform, and reduced acquisition time and artifacts for medical magnetic resonance with the sparse Fourier transform \cite{bettersparse}.\todo[inline]{find more compelling musical use cases here, not medical - trawl ismir dafx etc.}

However, implementations of these variants are not readily found or available in common DFT/FFT software libraries, which may be a barrier to their use in music and audio research. Popular software libraries implementing the FFT, including FFTW\footnote{\href{http://www.fftw.org/}{http://www.fftw.org/}} \cite{fftw1}, ffts\footnote{\href{https://github.com/anthonix/ffts/}{https://github.com/anthonix/ffts/}} \cite{ffts}, numpy\footnote{\href{https://numpy.org/doc/stable/reference/routines.fft.html}{https://numpy.org/doc/stable/reference/routines.fft.html}} \cite{numpy}, scipy\footnote{\href{https://docs.scipy.org/doc/scipy/reference/tutorial/fft.html}{https://docs.scipy.org/doc/scipy/reference/tutorial/fft.html}} \cite{scipy}, and hardware-specific libraries such as Intel Performance Primitives\footnote{\href{https://software.intel.com/content/www/us/en/develop/documentation/ipp-dev-reference/top/volume-1-signal-and-data-processing/transform-functions/fourier-transform-functions/fast-fourier-transform-functions.html}{https://software.intel.com/content/www/us/en/develop/documentation/ipp-dev-reference/top/volume-1-signal-and-data-processing/transform-functions/fourier-transform-functions/fast-fourier-transform-functions.html}} (IPP) \cite{ipp}, cuFFT\footnote{\href{https://docs.nvidia.com/cuda/cufft/index.html}{https://docs.nvidia.com/cuda/cufft/index.html}} \cite{cufft}, and Ne10\footnote{\href{https://projectne10.github.io/Ne10/doc/group__groupDSPs.html}{https://projectne10.github.io/Ne10/doc/group\_\_groupDSPs.html}} for Arm processors \cite{ne10} only contain the standard FFT algorithms.

New FFT libraries try to improve the performance of the FFT with new processor instructions\footnote{\href{https://github.com/DEWETRON/otfft}{https://github.com/DEWETRON/otfft}} or by using faster and safer programming languages like Rust.\footnote{\href{https://users.rust-lang.org/t/rustfft-5-0-0-experimental-1-now-faster-than-fftw/53049}{https://users.rust-lang.org/t/rustfft-5-0-0-experimental-1-now-faster-than-fftw/53049}} While performance is an important aspect of software libraries, it is not the only one. The authors of FFTW state that ``[f]lexibility [for scientific computing libraries] is often far more important [than speed], because one wants to be limited only by one's imagination, rather than by one's software, in the kinds of problems that can be studied  \cite{fftw2}.'' In this thesis, I seek to provide a flexible choice of alternative FFTs for future music research.

\subsection{Thesis objective}

The primary objective of this thesis is to write a library which implements variants of the FFT that may be useful for music systems. The interface of the library should resemble the standard FFT as closely as possible, to reduce the friction of substituting any of the variants in existing systems. FFTW justifies their use of the C programming language as follows \cite{fftw2}:

\begin{quote}
	[the user-callable interface of FFTW is] purely in C with lowest-common-denominator data types (arrays of floating-point values). The advantage of this is that FFTW can be (and has been) called from almost any other programming language, from Java to Perl to Fortran 77. Similar lowest-common-denominator interfaces are apparent in many other popular numerical libraries, such as LAPACK.
\end{quote}

For the same reason of maximum compatibility with programming languages, the core library of this thesis will be written in C. Python bindings will be written as well released initially as it's \todo{unfuck and cite this} the language of choice for numeric computing/prototyping and especially machine and deep learning. 

An explicit non-goal of this thesis is rewriting the FFT. The most typical computation scheme of an FFT variant involves optional pre-processing, the application of a standard FFT, and optional post-processing. For example, \citet{sparse} uses FFTW, NUFFT uses FFTW, and the scheme for the warped Fourier transform uses the FFTW. The FFT implementation that will be used in fftn is \todo{maybe not ffts!} ffts, for its more liberal open-source license. \todo[inline]{of course make this better} It will be vendored also.

The secondary objective of this thesis is to substitute the implemented variants in a state-of-the-art music system, to determine whether.

\subsection{Related work}

foo

\subsection{Contribution and results}

The primary contribution of this thesis is the creation and open-source publication of a warped STFT library\todo{make better}.

A secondary contribution of this thesis showed that substituting variants of FFT in a recent music system \todo{PICK SOTA!} showed a simultaneous reduction of the computational footprint and accuracy of results. This indicates that the use of these variants could be a promising avenue for future research, and it is hoped that the fftn library will become an important tool in future music systems.

\subsection{Outline}

This thesis is organized as follows. Section \ref{sec:theorystandard} will cover the theory of the DFT and its fast FFT implementation, and their use in music systems. Section \ref{sec:theoryvariant} will describe several variants of the FFT that may be useful for music applications. Section \ref{sec:libraries} will describe and benchmark existing open-source FFT libraries, and develop a new FFT library containing implementations of the studied variants. Section \ref{sec:results} will describe the outputs and performance analysis of the variants in the new library, as well as results achieved in a real-world music system by substituting the standard FFT with each variant. Finally, section \ref{sec:conclusion} will discuss the findings and explore whether there is value in using a variant instead of the standard FFT when creating new music systems.

\vfill
\clearpage

\section{Short-time Fourier transform}
\label{sec:theorytf}

\subsection{Acoustic signals and time domain waveforms}

\subsection{Frequency analysis and the spectral domain}

\subsubsection{Discrete Fourier transform}

\todo[inline]{theory of fourier, history, continuous-time fourier transform}

\todo[inline]{digital sampling theory intro, then map ct to dt}

A characteristic of the standard DFT is that the output frequencies are evenly spaced. For an $N$-point DFT, the DFT coefficient at bin (or index) 0 corresponds to the DC component of the sound, and every non-zero coefficient corresponds to a frequency $\frac{n}{\mathit{fs}}, n \in (0, N)$, where $\mathit{fs}$ is the sampling rate of the input signal. In practice, since frequencies above the Nyquist rate cannot be represented, the useful bins of a DFT are between $1$ and $\frac{N}{2}+1$, representing the frequencies $\frac{1}{\mathit{fs}}$ Hz and $\frac{\mathit{fs}}{2} = f_{\text{nyq}}$ Hz. Each bin in between is spaced $\frac{1}{\mathit{fs}}$ apart from the previous and next bin.

\todo[inline]{unit circle z plane blah}

\subsubsection{Fast Fourier transform}

\citet{cooleytukey} did a cool thing. Bluestein? stockham? so many

\subsection{Time-frequency analysis and the spectrogram}

\citet{gabor1946}'s seminal signal processing paper, \textit{The Theory of Communication}, contained the first suggestion of joint time-frequency decomposition of a signal by applying the Fourier transform locally to overlapping portions of the signal multiplied by Gaussian windows. In other words, Gabor proposed that any signal of finite energy can be decomposed into a linear combination of time-frequency shifts of the Gaussian function. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described in equation (1):
\begin{flalign}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)]\\
	G_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n},
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform. With his transform, Gabor also introduced the first formulation of the time-frequency uncertainty principle (which is minimized by using the Gaussian function as a window), stating that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit.'' Gabor called named the time-frequency tile the \textit{logon}, or smallest possible unit of time-frequency information. Mathematically, this can be stated as:
\[ \Delta t\Delta f \ge 1 \]

$\Delta t$ and $\Delta f$ are, as defined by Gabor, ``the uncertainties inherent in the definition of the epoch $t$ and frequency $f$ of an oscillation.'' The TF uncertainty principle arises from the fact that time and frequency are, in quantum physics terms, conjugate variables, or Fourier transforms of each other. This is further illustrated in figure \ref{fig:gabortf}, which shows the tiling of the time-frequency plane, and how frequency and time resolution must be sacrificed for one another by the lower bound of the time-frequency tile area.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[height=3cm]{./gabor3.png}}
	\hspace{0.1em}
	\subfloat{\includegraphics[height=2.56cm]{./gabor4.png}}
	\caption{A demonstration of the mutually exclusive formulations of time analysis and frequency analysis, and the lower bound of time-frequency resolution defined by Gabor's TF uncertainty principle \cite{gabordiagrams}}
	\label{fig:gabortf}
\end{figure}

Gabor noted the need for variable-frequency analysis, stating that ``the foregoing solutions [of the Fourier transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren.'' Similarly, psychoacoustics research shows that humans have been able to beat the time-frequency uncertainty principle \cite{psycho1, psycho2}, indicating the presence of nonlinear operators in the auditory system.

The STFT, or short-time Fourier transform, has been described independently from Gabor's work \cite{stftindie}, but additional research in the 1980s \cite{dictionary} led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described in equation (2):
\begin{flalign}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)]\\
	X_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n},
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window. Note how similar equations (1) and (2) are, which is expected since the original Gabor transform is the STFT with a Gaussian window. Practically, the STFT allows the use of different windows and overlap sizes \cite{stftinvertible}, as long as overlap-add conditions are respected.\footnote{\url{https://www.mathworks.com/help/signal/ref/iscola.html}}

\citet{dictionary} describe the shift from the term \textit{transform}, e.g., the Gabor transform or STFT, to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began the ``fundamental shift from transforms to dictionaries for sparse signal representation.'' Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task.'' Similar ideas were shown in \citet{doerflerphd}'s dissertation, suggesting the use of multiple Gabor dictionaries for representing music, for the two following reasons:

\begin{tight_itemize}
	\item
		\textit{Transients} are important musically, driving instrument identification and temporal events such as beats. As transients and broadband signals occur in the high frequency range, good time resolution in that range allows clearer identification of transient events.
	\item
		 Notes in the low frequency lay the harmonic basis of the song, requiring very fine frequency resolution.
\end{tight_itemize}

\citet{tfjigsaw}'s TF Jigsaw Puzzle algorithm works along these lines, establishing a multi-dictionary Gabor analysis by using two different window sizes ($R = 2$ for two dictionaries, using \citet{doerflerphd}'s terminology) for tonal and transient representation. In fact, this idea appears in most of the studied harmonic/percussive separation algorithms: the large window analysis leading to high frequency resolution for the optimal separation of pitched instruments, and the small window analysis leading to high time resolution for the optimal separation of transients.

However, \citet{doerflerphd} notes that there is a downside of high redundancy (or overcompleteness) in such multi-dictionary analyses (e.g., using two STFTs is an example of high redundancy). There should be an ideal \textit{single} transform which adapts itself to the different characteristics of the signal being studied, rather than creating a new transform or dictionary for each characteristic.

A useful exercise is to represent the classic glockenspiel signal with different dictionaries to gain some practical, hands-on experience with the characteristics, advantages, and disadvantages of each transform mentioned so far. All plots are generated with MATLAB and LTFAT.

The glockenspiel signal is well-known and useful to demonstrate transient and tonal properties of a musical signal -- it is available in the LTFAT,\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} loaded by the \Verb#gspi# function. Figure \ref{fig:glockwaveform} shows the time-domain waveform:

\begin{figure}[ht]
	\centering
	\includegraphics[height=6.5cm]{./gspi_waveform.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

Let's first consider the STFT. The standard \Verb#spectrogram# in MATLAB\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} is computed using the short-time Fourier transform. The number of DFT output bins is set to twice the window size, and the overlap is set to half of the window size. An STFT with 3 different window sizes is shown in figure \ref{fig:glockspecs}.

\begin{figure}[ht]
	\vspace{-1em}
	\centering
	\subfloat[Window size 256]{\includegraphics[height=5cm]{./glock_stft256.png}}
	\hspace{0.35em}
	\subfloat[Window size 1024]{\includegraphics[height=5cm]{./glock_stft1024.png}}
	\hspace{0.35em}
	\subfloat[Window size 4096]{\includegraphics[height=5cm]{./glock_stft4096.png}}
	\caption{STFT-based spectrograms of the glockenspiel signal}
	\label{fig:glockspecs}
\end{figure}

The maximum frequency is the Nyquist frequency, or half the sample rate -- in this case, it is 22050 Hz (half of 44100 Hz, the gspi sample rate). The frequency resolution of the linear STFT is fixed at $\frac{\mathit{fs}}{N}$, where $\mathit{fs}$ is the sampling frequency or sample rate, and $N$ is the number of output bins of the DFT. The time resolution of the linear STFT depends on the window size. The table \ref{table:stftparams} shows the frequency resolution, or $\Delta f$, per window size of the STFT.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|l|c|c|c|c| }
	 \hline
	  Window (samples) & Window (ms) & DFT outputs & $\Delta f$ (Hz/bin) \\
	 \hline
	 \hline
	 256 & 5.8 & 512 & 86.1 \\
	 \hline
	 1024 & 23.2 & 2048 & 21.5  \\
	 \hline
	 4096 & 92.9 & 8192 & 5.4  \\
	 \hline
\end{tabular}
	\caption{STFT window sizes and frequency resolution}
	\label{table:stftparams}
\end{table}

Note that the time resolution is getting worse as the window size increases -- consider that within a window of 92.9ms, there may be several musical events occurring, such as transients or onsets, which cannot be separately distinguished, and are blurred together. However, as a tradeoff, the frequency resolution is becoming sharper -- each row of the STFT for a window of 92.9ms represents a frequency increment of 5.4Hz. Visually, we observe the time resolution of the spectrogram becoming blurrier while the frequency resolution gets sharper as the window size increases.

\subsubsection{Time-frequency uncertainty principle}

\subsubsection{Gabor transform}

\subsubsection{Short-time Fourier transform}

\vfill
\clearpage

\section{Nonstationary Gabor transform}
\label{sec:theorymusic}

\subsection{Temporal resolution and transients}
\label{sec:freqscales}

\subsection{Frequency scales for music}
\label{sec:freqscales}

\todo[inline]{constant-Q and octave}

The human auditory system has a non-linear frequency response, with a finer frequency resolution at lower frequencies. \todo{BCJ Moore or Plack} This has implications in both speech and music processing. In music, low frequency bass notes establish the harmonic basis of a song. \todo{doerfler}

\todo[inline]{nice psychoacoustic diagrams here}

A common approach is to apply post-processing on the linear frequencies of the spectrogram output by the standard DFT to modify the scale to logarithmic, or the mel or Bark psychoacoustic frequency scales \todo{cite psychoacoustic scales}. Figure \ref{fig:melfilter} from the MATLAB documentation\footnote{\href{https://www.mathworks.com/help/audio/ref/melspectrogram.html}{https://www.mathworks.com/help/audio/ref/melspectrogram.html}} shows the computation of a mel spectrogram from the standard FFT as described by \citet[463]{melbook}.

\todo[inline]{BY SUMMING THE POWER SPECTRAL DENSITIES! so we can't just output frequencies at the isolated edges of the mel bands}

\begin{figure}[ht]
	\centering
	\subfloat[Computation of mel spectrogram]{\includegraphics[width=11cm]{./melspectrogram_1.png}}
	\subfloat[DFT weighting functions for mel scale]{\includegraphics[height=3cm]{./melspectrogram_2.png}}
	\caption{Computation of mel spectrogram}
	\label{fig:melfilter}
\end{figure}

Instead of post-processing the standard DFT to map to a psychoacoustic frequency scale, one option is to use a variant of the DFT which outputs the psychoacoustic frequencies directly. This may lead to significant computational savings in systems. The nonuniform DFT (NDFT) and warped DFT (WDFT) are two such possibilities.

\subsection{Constant-Q transform}

The idea for a musically-appropriate single transform, the Constant-Q Transform, was first proposed by \citet{jbrown}. The goal was to create a transform which maintained a constant ratio of frequency to frequency resolution, for the following reasons:

\begin{tight_itemize}
	\item
		The harmonics of the fundamental frequency created by musical instruments have a consistent spacing in the logarithmic scale, or the \textit{constant pattern}
	\item
		Log-frequency spectra, demonstrating the constant pattern for harmonics, would be more useful in musical tasks than linear-frequency spectra
\end{tight_itemize}

This is illustrated in figure \ref{fig:violin}, showing a linear and CQT representation of violin playing a scale. Note that the musical features of the violin -- distinct notes played, even spacing between the harmonics, and the strong formant frequency in the \textasciitilde3000 Hz region -- which are clearly visible in the CQT, and not in the linear-frequency DFT.

\begin{figure}[ht]
	\centering
	\subfloat[Linear-frequency DFT]{\includegraphics[height=5.2cm]{./violindft.png}}
	\hspace{0.5em}
	\subfloat[Constant-Q transform]{\includegraphics[height=5.5cm]{./violincqt.png}}
	\caption{Violin playing the diatonic scale, $G_{3} \text{(196Hz)} - G_{5} \text{(784Hz)}$}
	\label{fig:violin}
\end{figure}

\todo[inline]{describe puckette brown cqt with FFT operations}

\todo[inline]{cite here the good time resolution of the 2-dictionary CQT, dorfler stuff thesis}

The first formulation of the CQT was constructed using specific window sizes for each frequency region of interest, and was not designed to be invertible, thus making it only suitable for analysis.

Several papers created CQT implementations with some level of approximation error in the inverse transform \cite{klapuricqt, fitzgeraldcqt}. In fact, \citet{klapuricqt}'s implementation is currently the implementation of the CQT in librosa \cite{librosa}, a popular and modern open-source Music Information Retrieval library for Python.

Subsequent work by \citet{balazs, jaillet, invertiblecqt} applied formal frame theory to create a perfectly invertible and minimally redundant Constant-Q transform using the Nonstationary Gabor Transform (NSGT, or NSDGT for the discrete-time variant). The CQ-NSGT is the implementation of the CQT in the MATLAB Wavelet Toolbox.\footnote{\href{https://www.mathworks.com/help/wavelet/ref/cqt.html}{https://www.mathworks.com/help/wavelet/ref/cqt.html}} \todo[inline]{deal with number of FFTs in the computation}

In their CQT paper, \citet{klapuricqt} reference the warped DFT from \citet{warpedold1, warpedold2} as an interesting option for a CQT, but ruled it out for lacking a stable inverse transform. Those implementations are in fact similar to those already mentioned \cite{earlywarped1, earlywarped2} in that they use the warped delay line (WDLDFT) implementation, shown previously in figure \ref{fig:wdftsb}. Therefore, \citet{warped2}'s WDFT with the low error IIWDFT inversion scheme may be a better choice as an alternative to the CQT.

Recall equation (1) for the discrete Gabor transform, which used time-shifted copies of identical Gaussian windows as the Gabor atoms for the decomposition:
\[ g_{m,n} = g(n-\beta m)e^{-j2\pi \alpha n} \]

The parameters $\beta$ and $\alpha$ control the time and frequency resolution of the transform. In the case of the nonstationary Gabor transform with resolution changing over time, the Gabor atoms consist of window functions selected from a set of functions $g_{m}$ using a fixed frequency sampling step $\alpha_{m}$:
\[ g_{m,n} = g_{m}(n)e^{-j2\pi \alpha_{m} n} \]

\citet{balazs} state that ``this is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{m}$ for each position $\beta_{m}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position.'' Moreover, if we set $g_{m}(n) = g(n - \beta m)$ and fix the time and frequency constants $\beta$, $\alpha_{m} = \alpha$, we get back the classic form of the stationary discrete Gabor transform.

Similarly, for the nonstationary Gabor transform with resolution changing over frequency, the Gabor atoms used are selected from a set of functions $h_{n}$ using a fixed time sampling step $\beta_{n}$:
\[ h_{m,n} = h_{n}(n - \beta_{n}m) \]

\citet{balazs} state that ``in practice we will choose each function $h_{n}$ as a well-localized band-pass function with center frequency $\alpha_{m}$.'' We get back the classic form of the stationary discrete Gabor transform by selecting functions $h_{n}(n) = g(n - \beta_{n} m)e^{-j2\pi \alpha n}$ and fix the time and frequency constants $\alpha$, $\beta_{n} = \beta$.

The time-frequency lattice of the stationary and nonstationary Gabor transforms can be seen in figure \ref{fig:nsgts}.

\begin{figure}[ht]
	\centering
	\subfloat[Stationary]{\includegraphics[height=4cm]{./sgt.png}}
	\hspace{0.35em}
	\subfloat[Nonstationary in time]{\includegraphics[height=4cm]{./nsgt_time.png}}
	\hspace{0.35em}
	\subfloat[Nonstationary in frequency]{\includegraphics[height=4cm]{./nsgt_freq.png}}
	\caption{Time-frequency lattice of different Gabor transforms}
	\label{fig:nsgts}
\end{figure}

To recap, \citet{adaptivecqt} describes the concepts presented so far as follows:

\begin{quote}
	The definition of multiple Gabor frames, which is comprehensively treated in \cite{doerflerphd}, provides Gabor frames with analysis techniques with multiple resolutions. The nonstationary Gabor frames (see \cite{balazs, jaillet} for their definition and implementation) are a further development; they fully exploit theoretical properties [...], and they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas.
\end{quote}

Several important outcomes of the NSGT were the invertible CQT with a fast FFT-based implementation \cite{invertiblecqt}, and the realtime invertible CQT \cite{rtcqt}. Aside from being available in LTFAT, the official MATLAB Wavelet Toolbox contains perfectly invertible implementations of the CQT and ICQT (inverse CQT) based on the NSGT.\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} These transforms are used extensively throughout this paper, present in every MATLAB algorithm which uses the CQT. In the Python case, the NSGT library is used,\footnote{\url{https://github.com/grrrr/nsgt}} which contains both the invertible and realtime implementations \cite{invertiblecqt, rtcqt}.

Finally, figure \ref{fig:nsgtglock} demonstrates the advantages of the NSGT for a musical signal -- note that the small-window STFT has sharply defined time events but blurry frequency bins, and the inverse case for the large-window STFT. Finally, the NSGT contains sharp time and frequency resolution.

\begin{figure}[ht]
	\centering
	\subfloat[STFT with 6ms window]{\includegraphics[height=4cm]{./tf_tradeoff_balasz1.png}}
	\hspace{0.35em}
	\subfloat[STFT with 93ms window]{\includegraphics[height=4cm]{./tf_tradeoff_balasz2.png}}
	\hspace{0.35em}
	\subfloat[NSGT using window evolving between 6--93ms]{\includegraphics[height=4cm]{./tf_tradeoff_balasz3.png}}
	\caption{Transforms of a glockenspiel signal \cite{balazs}}
	\label{fig:nsgtglock}
\end{figure}

Next, let's consider the NSGT-CQT from the MATLAB Wavelet Toolbox function \Verb#cqt#.\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} Various plots of the CQT are shown in figure \ref{fig:glockcqts}. Note that the bins per octave parameter controls the frequency resolution -- more bins per octave gives us a finer frequency discrimination. The Gabor atoms used to create the CQT are not constant like the STFT -- they consist of a set of window functions with different center frequencies and bandwidths to maintain the constant-Q ratio. The bins-per-octave settings shown are 12, 24, and 48, which correspond to a constant-Q ratio of $2^{\frac{1}{\text{bins}}}$, or 1.059, 1.029, and 1.014 respectively. Finally, \citet{jbrown}'s desired constant pattern in the logarithmic scale is shown in the last row of plots.

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins per octave]{\includegraphics[height=5cm]{./glock_cqt12.png}}
	\hspace{0.35em}
	\subfloat[CQT spectrogram, 24 bins per octave]{\includegraphics[height=5cm]{./glock_cqt24.png}}
	\hspace{0.35em}
	\subfloat[CQT spectrogram, 48 bins per octave]{\includegraphics[height=5cm]{./glock_cqt48.png}}
	\vspace{0.1em}
	\subfloat[Ratio, 12 bins per octave]{\includegraphics[height=4cm]{./glock_cqt12_ratio.png}}
	\hspace{0.35em}
	\subfloat[Ratio, 24 bins per octave]{\includegraphics[height=4cm]{./glock_cqt24_ratio.png}}
	\hspace{0.35em}
	\subfloat[Ratio, 48 bins per octave]{\includegraphics[height=4cm]{./glock_cqt48_ratio.png}}
	\vspace{0.1em}
	\subfloat[Gabor frame at Nyquist frequency, 12 bins per octave]{\includegraphics[height=4.5cm]{./glock_cqt12_gaborframe.png}}
	\hspace{0.35em}
	\subfloat[Gabor frame at Nyquist frequency, 24 bins per octave]{\includegraphics[height=4.5cm]{./glock_cqt24_gaborframe.png}}
	\hspace{0.35em}
	\subfloat[Gabor frame at Nyquist frequency, 48 bins per octave]{\includegraphics[height=4.5cm]{./glock_cqt48_gaborframe.png}}
	\vspace{0.1em}
	\subfloat[Center frequencies, 12 bins per octave]{\includegraphics[height=4.25cm]{./glock_cqt12_cf.png}}
	\hspace{0.35em}
	\subfloat[Center frequencies, 24 bins per octave]{\includegraphics[height=4.25cm]{./glock_cqt24_cf.png}}
	\hspace{0.35em}
	\subfloat[Center frequencies, 48 bins per octave]{\includegraphics[height=4.25cm]{./glock_cqt48_cf.png}}
	\caption{CQT plots of glockenspiel signal}
	\label{fig:glockcqts}
\end{figure}

The advantage of the CQT over the linear-frequency STFT can be seen by displaying the spectrograms side-by-side, in figure \ref{fig:cqtvstft}. Note that the CQT (regardless of bins per octave) seems to be identifying more frequency components from the glockenspiel signal, even if in the case of less bins per octave, the frequency bins are blurry. The time resolution in the CQTs is also sharp, even with high frequency resolution.

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins per octave]{\includegraphics[height=5.25cm]{./glock_cqt12.png}}
	\hspace{0.35em}
	\subfloat[STFT spectrogram, window size 256]{\includegraphics[height=5.25cm]{./glock_stft256.png}}\\
	\vspace{0.1em}
	\subfloat[CQT spectrogram, 24 bins per octave]{\includegraphics[height=5.25cm]{./glock_cqt24.png}}
	\hspace{0.35em}
	\subfloat[STFT spectrogram, window size 1024]{\includegraphics[height=5.25cm]{./glock_stft1024.png}}\\
	\vspace{0.1em}
	\subfloat[CQT spectrogram, 48 bins per octave]{\includegraphics[height=5.25cm]{./glock_cqt48.png}}
	\hspace{0.35em}
	\subfloat[STFT spectrogram, window size 4096]{\includegraphics[height=5.25cm]{./glock_stft4096.png}}
	\caption{CQT versus STFT}
	\label{fig:cqtvstft}
\end{figure}

\subsubsection{Earlier implementations}

\subsubsection{Nonstationary Gabor transform}

\vfill
\clearpage

\section{Onset detection and beat tracking}
\label{sec:beattrack}

\subsection{Task definition}

\textcite{ellis} formulated the task of beat tracking in music information retrieval as a two-way optimization problem. The algorithm starts with computing onset strengths, $O(t)$, for every time $t$ (discrete samples) in the input signal, also known as the onset strength envelope. Onsets refer to the start of musical notes or events (\cite{onsets}). Autocorrelation (a measure of signal self-similarity) is then applied on the onset strength envelope to find the dominant onset periodicity. Some further perceptual post-processing is done to take into account the natural human bias towards 120bpm. Finally, the most dominant periodicity is chosen as the global tempo estimate for the entire song. An assumption (and limitation) of the algorithm is that there is a single stable tempo throughout the song.

\subsection{Use of the STFT}

\subsection{Datasets and evaluation metrics}

Summary of MIREX beat tracking datasets\footfullcite{beatmeta}
	\begin{enumerate}
		\item[2006]
			First appearance of challenge; \textit{the MCK (McKinney) dataset contains 160 30-second audio excerpts created by the MIREX team in 2006. Characterized by stable tempo, wide variety of instrumentations and musical styles. 20\% of the files have non-binary meters.}
		\item[2009]
			Second dataset, Chopin Mazurkas; \textit{the MAZ (mazurka) dataset contains piano recordings of 322 Chopin Mazurkas, which include tempo changes.}
		\item[2012]
			Third dataset; \textit{the SMC (Sound and Music Computing, University of Porto) dataset consists of 217 excerpts around 40s each, majority is difficult to track (e.g. changes in meter and tempo, bad sound quality, expressive timing). It includes romantic music, film soundtracks, blues, chanson, and solo guitar}
	\end{enumerate}

The MIREX SMC12 dataset \cite{smcdataset}, using the mir\_eval\footnote{\href{https://github.com/craffel/mir_eval}{https://github.com/craffel/mir\_eval}} library. MIREX 2019 \footnote{\href{https://www.music-ir.org/mirex/wiki/2019:MIREX2019_Results}{https://www.music-ir.org/mirex/wiki/2019:MIREX2019\_Results}} is the most recent year of the audio beat tracking challenge (2020 results are not ready yet).

The summary of MIREX 2019 results on the SMC dataset is:

The 4 measures that will be evaluated (F-measure, Cemgil, Goto, and McKinney P-Score) are the same as those used in MIREX, and are borrowed from the Beat Evaluation Toolbox \footnote{\href{https://code.soundsoftware.ac.uk/projects/beat-evaluation/}{https://code.soundsoftware.ac.uk/projects/beat-evaluation/}}.

An additional 2 measures were added by splitting the F-measure into its constituent precision and recall (simply by copy-pasting the mir\_eval f\_measure function and returning the individual measures). This should help with a more fine-grained analysis of results.

The mir\_beat\_eval.py script (in the `tests/` directory of the repository) loads the SMC dataset (which you can download here \footnote{\href{http://smc.inesctec.pt/research/data-2/}{http://smc.inesctec.pt/research/data-2/}}. The SMC dataset contains wav files and ground-truth annotations.

\subsection{Survey and state-of-the-art}

\subsection{Model under study: DBNBeatTracker}

\textcite{mcfee} state that onset detection is a critical first stage of most beat tracking algorithms. Works by \textcite{goto}, \textcite{ellis}, and \textcite{btrack} are some examples among many of onset-based beat tracking. \textcite{onsets} provide multiple definitions for onsets, but the simplest is that of the beginning of musical events. A typical onset-based beat tracker, as described by \textcite{mcfee}, operates as follows:
\begin{quote}
	\vspace{-0.25em}
	First, the audio signal is processed by an onset strength function, which measures the likelihood that a musically salient change (e.g., note onset) has occurred at each time point. The tracking algorithm then selects the beat times from among the peaks of the onset strength profile
\end{quote}

\textcite{rnnonset} used a recurrent neural network architecture to achieve state-of-the-art results in onset detection. This was first adapted by \textcite{bock1} to create the ``RNNBeatProcessor,'' so named in the open-source Python madmom library (\cite{madmom}). The network architecture is based on Bi-directional Long Short-Term Memory recurrent neural networks. This network architecture was chosen for the following reasons:

\begin{tight_itemize}
	\vspace{-0.25em}
	\item
		In the most basic form of feed-forward neural network, the relationship of inputs to outputs is strictly causal, i.e., inputs at the current time compute outputs at the current time.
	\item
		To introduce using inputs from the past to influence current outputs, which is important in beat tracking, cycles are created in the network, leading to recurrent neural networks. However, these suffer from the vanishing gradient problem, which causes inputs to decay or blow up exponentially over time.
	\item
		The Long Short-Term Memory (LSTM) network solves the problem of the vanishing gradient by introducing memory gates within the recurrent unit.
	\item
		Finally, to consider inputs from the future in the output, a Bi-directional LSTM (BLSTM) network introduces a second hidden layer which introduces the inputs to the network in a reverse temporal order.
\end{tight_itemize}

As past, present, and future inputs are all useful in a beat tracking algorithm, the BLSTM network from \textcite{rnnonset} was considered to be an appropriate choice. An additional modification was to introduce a peak picking stage for beat selection.

In their next paper, \textcite{bock3} adapted the RNNBeatProcessor to introduce multiple RNN models, each of which was trained on a different musical style, and added a dynamic Bayesian network in the front-end for the beat estimation. The choice of dynamic Bayesian network was based on prior work by \textcite{whiteley}, which achieved robust results in joint tempo and beat estimation.

In yet another follow-up paper, \textcite{bock2} modified the probabilistic tempo and beat sequence model of \cite{whiteley} to make it more computationally efficient (both in CPU cycles and memory) while maintaining the high accuracy of their solution in \cite{bock3}. The original joint tempo and beat model of \textcite{whiteley} is referred to as the \textit{tempo-bar model}, and it has two main deficiencies which were adjusted:

\begin{tight_enumerate}
	\vspace{-0.25em}
	\item
		The tempo-bar model assumes that human tempo discrimination is consistent across all tempi and use linearly-spaced tempi, while \textcite{bock2} describe that in fact humans have finer tempo resolution at lower tempos. To achieve a tempo spacing consistent with the just-noticeable-difference (JND) limits of human tempo resolution with the linearly-spaced model requires a huge number of tempi, while using a nonlinear spacing of more tempi in the low tempo range and fewer tempi in the high tempo range significantly reduces the total number of possible tempi and matches human perception better.
	\item
		The tempo-bar model allows tempo transitions at any possible time, while \textcite{bock2} limit their more efficient model to only allow tempo transitions on beat locations, which is consistent with tempo transitions in music.
\end{tight_enumerate}

The final algorithm for state-of-the-art beat tracking is therefore presented by \cite{bock2}, incorporating the algorithms from \cite{bock1} and \cite{bock3}, and achieving the best results in the last \textbf{M}usic \textbf{I}nformation \textbf{R}etrieval \textbf{E}valuation e\textbf{X}change audio beat tracking challenge.\footnote{\url{https://www.music-ir.org/mirex/wiki/2019:MIREX2019_Results}}


\vfill
\clearpage

\section{Music source separation}
\label{sec:musicsep}

\subsection{Task definition and survey}

The task of music source separation is to split a mixed song into its constituent components, or sources. \citet{musicsepgood} describe that music source separation could operate on the level of instruments, or for broader categories of sources, grouped into harmonic, percussive, and singing voice.

Surveys on speech \cite{speechmask} and music separation \cite{musicmask} indicate that the majority of separation algorithms use the technique of time-frequency masking (or spectral masking) to separate the sources.

\begin{wrapfigure}{r}{8cm}
	\vspace{-1.0em}
	\includegraphics[width=8cm]{./maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising. The oracle mask is the ideal mask for a given signal -- to compute it, the target and interference signals must be known.}
	\label{fig:masks}
	\vspace{-1.5em}
\end{wrapfigure}

\citet{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins. A soft mask has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only 0 and 1. The soft mask used in \cite{fitzgerald1, fitzgerald2} is a Wiener filter given in the following equation, where $\hat{S}$ represents the complex-valued spectrogram:
\[ M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|^{2}}{|\hat{S}_{\text{interference}}|^{2} + |\hat{S}_{\text{target}}|^{2}} \]

Soft masks generally produce higher quality sound. An illustration of spectral masking is shown in figure \ref{fig:masks}.

Most recently, the SigSep\footnote{\url{https://sigsep.github.io/}} community has been running the Signal Separation Evaluation Campaign (SISEC), which sets the tone for the modern state-of-the-art models. SiSec uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality, or BSSv4 variant.

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset \cite{musdb18} (or the HQ, high-quality, equivalent \cite{musdb18-hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research.

In modern music source separation, the stems of MUSDB18 have determined the four most common sources to separate -- drums, bass, vocals, and other. SigSep's own network, Open-Unmix, is trained only on MUSDB18, and produces near-state-of-the-art results. The absolute state-of-the-art crown is jointly held by Conv-Tasnet and Demucs, both of which surpass Open-Unmix. Both models operate directly on the waveform domain, which indicates that they could surpass the maximum possible poerformance of time-frequency masking approaches, as is done in speech separation. However, in practise they are still below the limits of masking-based approaches.

\subsection{Performance optimizations for testbench}

\subsection{Surpassing the ideal STFT mask}

The first experiment to run is to discover which configuration of the NSGT has the potential to surpass the maximum performance of time-frequency masking using the spectrogram.

Given ground truth data, such as what is available in MUSDB18, we have available to us the individually recorded sources for each track. From this, we can compute the ideal or oracle masks.

\begin{flalign}
	\nonumber \text{given: } & x_{\text{drums}}, x_{\text{vocals}}, x_{\text{bass}}, x_{\text{other}}\\
	\nonumber & x_{\text{mix}} = \sum{x}\\
	\nonumber & \hat{X} = \text{STFT}(x), \text{complex-valued}\\
	\nonumber & |\hat{X}|^{\alpha} = \text{magnitude STFT of } x \text{ raised to } \alpha \text{ power, real-valued}\\
	\nonumber & \text{Mask}_{\text{source}} = \frac{|\hat{X}|_{\text{source}}^{\alpha}}{\sum{|\hat{X}|^{\alpha}}}\\
	\nonumber & \hat{Y}_{\text{source}} = \text{Mask}_{\text{source}} * \hat{X}_{\text{mix}}\\
	\nonumber & y = \text{ISTFT}(\hat{Y})\\
	\nonumber & y = \text{ideal estimate of } x
\end{flalign}

Finally, using these pairs of $x, y$, we can get the maximum possible BSS metrics of any algorithm or model based on time-frequency masking.

A testbench was designed to select 5 random tracks from MUSDB18-HQ at a time and evaluate a range of NSGT parameters. In each iteration of the testbench, the range of parameters was made narrower to only include the top-scoring configurations, and the step size between each parameter was reduced to perform a more precise search. An exhaustive search on the entire space of NSGT configs would be unfeasible, considering that even with optimizations from cupy, it takes over 30 seconds to evaluate the BSS scores of an ideal mask for single track and NSGT configuration. Table \ref{table:nsgtparamsirm} contains the iterations of testing run along with the parameter ranges.

The  first 3 iterations were run using a fixed starting frequency of 20.0Hz, the lower limit of the human hearing range. The maximum frequency is always chosen as 22050 (or 44100/2), the Nyquist rate of MUSDB18-HQ, for performance benefits. Finally, in the final iteration, values near 20 Hz were considered.

The vocal SDR and median score (across all targets and metrics) per coefficient of the transform was considered to rank the algorithms. For example, the octave and Bark scales are consistently using more transform coefficients than the Mel scale, with lower performance, so they were eventually eliminated.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|l|c|c|c|c| }
	 \hline
	  Iteration & Scales & Bins & Bin step & Fmin & Fmin step \\
	 \hline
	 \hline
	 1 & Bark, Mel, Oct & 12-333 & 64 & 20 & 0 \\
	 \hline
	 2 & Bark, Mel & 204-333 & 32 & 20 & 0 \\
	 \hline
	 3 & Bark, Mel & 204-301 & 12 & 20 & 0 \\
	 \hline
	 4 & Mel & 216-301 & 6 & 15-35 & 2.5 \\
	 \hline
	 \textbf{Winner} & Mel & 234 & 0 & 27.5 & 0 \\
	 \hline
\end{tabular}
	\caption{NSGT parameter ranges evaluated}
	\label{table:nsgtparamsirm}
\end{table}

Several plots of the intermediate results are shown in figure \ref{fig:melnsgt}. An interesting phenomenon is that two NSGTs with almost-identical combinations of parameters can lead to a drastic drop in performance. Some examples of these are shown in table \ref{table:poorconfigs}. However, one can see that, ignoring anomalies, the best performers are in the region of 230--250 bins, using the Mel scale, and a starting frequency between 25-30 Hz. Also, these have the best BSS-score-per-coefficient value after the STFT, as the aforementioned octave and Bark scales produce larger matrices (which are more expensive to store and perform computations on).

The results led to the best NSGT selected as the Mel-frequency NSGT using 234 frequency bins and a starting frequency of 27.5 Hz. Further refinement would have been possible, such as reducing the bin step to 1 bin, or the frequency step to 0.1 Hz, but as mentioned, the resulting testbenches would be infeasible to run due to the large number of configurations. Moreover, the value of 27.5 Hz is coincidental with the lowest frequency on a standard piano, providing additional musical justification for its use.

\begin{figure}[ht]
	\centering
	\includegraphics[width=16cm]{./mel_nsgt_params.png}
	\caption{SDR scores of evaluated NSGT configurations}
	\label{fig:melnsgt}
\end{figure}


\vfill
\clearpage

\section{Conclusions}
\label{sec:conclusion}

foo

\subsection{Evaluation}

foo

\subsection{Outlook}

foo

\subsection{Summary}

foo

\vfill
\clearpage % force a page break before references

%\nocite{*}
\section{References}
\printbibliography[heading=none]

\vfill
\clearpage %force a page break

\begin{appendices}

\section{Code availability and replicating the results}
\label{appendix:coderesultsrepro}

\end{appendices}

\end{document}
