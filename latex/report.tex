\documentclass[letter,12pt,notitlepage]{article}
\usepackage[left=4cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage[titletoc,title]{appendix}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{minted}
\usepackage{subfig}
\usepackage{titling}
\usepackage[compatibility=false]{caption}
\usepackage[parfill]{parskip}
\setlength{\droptitle}{1cm}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate,annotation]{biblatex-chicago}
\addbibresource{citations.bib}
\input{variables.tex}

\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\newenvironment{tight_itemize}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{itemize}}

\newlength{\mintednumbersep}
\AtBeginDocument{%
  \sbox0{\tiny00}%
  \setlength\mintednumbersep{8pt}%
  \addtolength\mintednumbersep{-\wd0}%
}

\newcommand{\ichfeedback}[1]{\todo[color=green!40,inline]{#1}}

\title{\ThesisTitle}

\author{\vspace{1em}\\Sevag Hanssian \\
  McGill University \\
 \small{\texttt{sevag.hanssian@mail.mcgill.ca}} \\
 \small{\texttt{sevagh@protonmail.com}} \\\ \\\ \\
 \small{Thesis for Master of Arts in Music Technology}\\
 \small{Date TBD, 2021}}

% nil out the auto date
\date{}

\begin{document}

\maketitle

\vspace{3.5em}

\begin{abstract}
	The short-time Fourier transform (STFT) is an important tool for the time-frequency analysis of acoustic signals. The STFT is commonly used to represent musical signals in digital signal processing algorithms and machine learning models for music information retrieval (MIR). Despite the ubiquity of the STFT, it is limited by its fixed and bounded time-frequency resolution. The Nonstationary Gabor Transform (NSGT) is an alternative time-frequency transform which better represents musical signals by varying its time-frequency resolution. In the task of music source separation, STFT-based models are popular and have achieved success for many years. In this thesis, first the STFT and NSGT are described as tools for representing and manipulating musical signals. Next, the task of music source separation is described. Finally, the STFT is replaced with the NSGT in a contemporary model for music source separation, demonstrating improved results.
\end{abstract}

\vfill
\clearpage %force a page break

\tableofcontents

\vfill
\clearpage %force a page break

\listoffigures

\listoflistings

\vfill
\clearpage %force a page break

\section{Introduction}
\label{sec:intro}

The study of acoustic signals forms the core of many fields including sonar, seismology, audio, music, and speech. Acoustic signals are functions of time, representing the evolving amplitude of the sound pressure wave \todo{cite something}. Another way to characterize acoustic signals is by their frequency components. The Fourier transform can be used to obtain the frequency components present in a signal, but contains no temporal information. However, many important signals such as speech and music contain frequency components that change with time.

For this class of signal, joint time-frequency analysis is an important tool. Time-frequency analysis is performed by multiplying the signal being studied by finite, consecutive windows of a short duration, and taking the Fourier transform of each windowed section. This is also referred to as the short-time Fourier transform (STFT) or spectrogram \todo{cite textbook}. The STFT is subject to a fixed and bounded time-frequency resolution, such that one cannot be arbitrarily precise in both time and frequency, and must trade them off by varying the duration of the STFT window.

\subsection{Motivation}

Musical signals have characteristics that lead to conflicting requirements in the STFT \cite{doerflerphd}. In the low frequency region, music needs to be analyzed with long-duration windows for a high frequency resolution, as bass notes lay the harmonic basis of a song \todo{heavy citation stuff here}. Conversely, the high frequency region contains transients and broadband sounds, which are useful for timbre identification and rhythm. These transients have fast attacks and decays and need to be analyzed with short-duration windows.

Figure \ref{fig:stfttradeoff} shows a musical glockenspiel signal analyzed by different window sizes of STFT. Note how in figure \ref{fig:stfttradeoff}a, due to the high frequency resolution, the temporal events are blurry or smeared such that the times of note onsets are unclear. The inverse case is shown in figure \ref{fig:stfttradeoff}b, which contains sharp localization of note onsets from the high time resolution, but blurry or smeared frequency components.

\begin{figure}[ht]
	\centering
	\subfloat[Wide window STFT (93ms)]{\includegraphics[height=5cm]{./images-tftheory/tf_tradeoff_balasz2.png}}
	\hspace{0.5em}
	\subfloat[Narrow window STFT (6ms)]{\includegraphics[height=5cm]{./images-tftheory/tf_tradeoff_balasz1.png}}
	\caption{Different window size STFT spectrograms of a glockenspiel signal}
	\label{fig:stfttradeoff}
\end{figure}

The Nonstationary Gabor transform (NSGT) \cite{balazs} is a time-frequency transform with a varying time-frequency resolution and a perfect inverse. The NSGT spectrogram of the musical glockenspiel signal is shown in figure \ref{fig:nsgttradeoff}, where one can observe minimal blurriness, or good resolution, in both time and frequency, arising from the varying window size of the NSGT.

\begin{figure}[ht]
	\centering
	\includegraphics[height=5cm]{./images-tftheory/tf_tradeoff_balasz3.png}
	\caption{NSGT spectrogram of a glockenspiel signal, varying window (6-93ms)}
	\label{fig:nsgttradeoff}
\end{figure}

The NSGT is based on the constant-Q transform, which was originally proposed by \textcite{jbrown} to analyze musical signals with a logarithmic frequency scale to better show the relationship between the fundamental frequency of a musical instruments and its harmonics. Although it was not one of the original goals, by the nature of its nonlinear frequency resolution and short-duration windows in the high frequency regions, the CQT demonstrates a good time resolution at high frequencies \cite{cqtransient}, making it a good choice for representing transients \todo{cite the musical importance}.

The NSGT is a generalization of the CQT, and can be used to construct a CQT with perfect inverse, or CQ-NSGT \cite{invertiblecqt, variableq1}. The NSGT can in practice be constructed with any arbitrary frequency scale. Using the NSGT instead of the STFT to represent musical signals may be of interest, as it is a single transform which can replace the need to explicitly trade-off of time and frequency resolution with different window sizes of STFT, and it can be configured to use frequency scales that are most appropriate for the task at hand.

\subsection{Related work and thesis objectives}

\todo[inline]{read more surveys, come up with something better here}

\textcite{musicsepgood} describe the motivation and task of music source separation as follows:

\begin{quote}
	Many people listen to recorded music as part of their everyday lives [...] Sometimes we might want to remix the balance within the music, perhaps to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1- channel surround sound system. We might also want to change the spatial location of a musical instrument within the mix. All of these applications are relatively straightforward, provided we have access to separate sound channels (stems) for each musical audio object. However, if we only have access to the final recording mix, which is usually the case, this is much more challenging. To estimate the original musical sources, which would allow us to remix, suppress or upmix the sources, we need to perform musical source separation (MSS). In the general source separation problem, we are given one or more mixture signals that contain different mixtures of some original source signals. [...] The task is to recover one or more of the source signals given the mixtures.
\end{quote}

\textcite{umx}'s deep learning model, named Open-Unmix, is intended to be a near state-of-the-art, open implementation of music source separation based on the MUSDB18 dataset \cite{musdb18} and intended to foster reproducible research and a baseline of high performance. It uses the STFT spectrogram as the input and output representation of musical signals, and a deep neural network to estimate invidual sources from a mixture. The choice of window size in the task of source separation based on STFT spectrograms can have an impact on the results of speech and music source separation \cite{musicsepwindow}, but this consideration is not explored in Open-Unmix.

The CQT was shown to surpass the performance of the STFT in speech separation \cite{cqtseparation}. A simpler and older algorithm for harmonic/percussive source separation (HPSS) \cite{fitzgerald1}, also based on the STFT spectrogram, was evaluated alongside Open-Unmix in the SiSec 2018 music source separation evaluation campaign \cite{sisec2018} as a primitive (and low performance) baseline. The HPSS algorithm was later modified to use STFT spectrograms with different window sizes \cite{driedger, fitzgerald2}, and to use the CQT in place of the STFT \cite{fitzgerald2}. Using the STFT with a warped, or non-linear frequency scale, was also shown to improve music source separation results \cite{bettermusicsep}.

As the NSGT is a generalization of the CQT, and can be used to implement different frequency scales, it is chosen as the transform to study in this thesis. The objectives of this thesis are to discover which configurations of the NSGT can surpass different window sizes of the STFT in music source separation, and to adapt Open-Unmix to use the NSGT.

\subsection{Contribution and results}

The contributions of this thesis would be the demonstration of the successful use of the NSGT in a domain which is generally dominated by the STFT. 

The results will be compare the results to the classic Open-Unmix and to the current state-of-the-art, Demucs and Conv-Tasnet \cite{demucs}.

\subsection{Outline}

This thesis is organized as follows. Section \todo{actually fill this in}.

\begin{enumerate}
	\item
		Describe the STFT and its bounded and fixed time-frequency resolution
	\item
		Describe the NSGT, its theoretical background, its suitability for musical signals, and demonstrate different configurations of NSGT
	\item
		Describe and present a review of the task of music source separation, including datasets, evaluation measures, and existing models and algorithms
	\item
		Find configurations of the NSGT that can outperform the STFT in spectrogram-based music source separation
	\item
		Compare the performance of the NSGT to the separation performance of different window sizes of STFT
	\item
		Adapt Open-Unmix to use the NSGT and surpass its previous STFT-based performance, and to also beat the current state-of-the-art, Demucs and Conv-Tasnet \cite{demucs}
\end{enumerate}

\vfill
\clearpage

\section{Background}
\label{sec:background}

\subsection{Joint time-frequency analysis of acoustic signals}
\label{sec:theorystft}

\subsubsection{Acoustic signals and the waveform}

\ichfeedback{is this a good section? time-domain stuff}

\todo[inline]{acoustic signals, pressure waves, etc.}

\todo[inline]{discrete time, shannon-nyquist sampling theorem}

\subsubsection{Fourier transform and the frequency domain}

\todo[inline]{discrete-time fourier transform, spectral analysis, spectrogram}

\subsubsection{Time-frequency analysis for time-varying frequency}

\ichfeedback{time-frequency analysis was first done by Gabor because frequencies of some natural sounds (speech, music, sirens, etc.) have a time-varying frequency such that only time or only frequency analysis alone cannot describe the signal}

\subsubsection{Time-frequency uncertainty and the Gabor transform}

\todo[inline]{Joint time-frequency analysis}

\todo[inline]{dennis gabor also noted the need for joint TFA, so the above and below will have similarities}

\todo[inline]{Time-frequency uncertainty principle}

Dennis Gabor's seminal paper on information theory describes the analysis of a speech signal by applying the Fourier transform to finite, windowed segments of the input signal, referred to as the Gabor transform, and considered synonymous with the short-time Fourier transform. From the Gabor transform, which used a fixed or stationary window size, 

\subsection{Transforms of acoustic signals}

\ichfeedback{for each, describe algorithmic complexity and common libraries - FFTW, librosa, essentia, etc.}

\todo[inline]{algorithmic analysis - how many window operations, how many FFTs are done, etc.}

\todo[inline]{reference implementations and related code available: ltfat, matlab, python nsgt, essentia}

\subsubsection{Fast Fourier Transform (FFT)}

\subsubsection{Short-time Fourier Transform (STFT)}

\subsubsection{Constant-Q Transform (CQT)}


Summary of the most relevant CQT implementations:

\begin{enumerate}
	\item
	    Brown, 1991, first proposed CQT with a naive very slow implementation.
    \item
	    Brown and Puckette, 1992: implemented a faster CQT based on a sparse representation in the frequency domain. This is the current Essentia implementation!
    \item
	    Sch{\"o}rkhuber and Klapuri, 2010: a faster CQT based on the same principles as Brown and Puckette, 1992. For first time, it is introduced an algorithm for an approximated reconstruction of the CQT coefficients. Code: \url{http://www.iem.at/~schoerkhuber/cqt2010/} - this is the librosa implementation
    \item
	    Velasco, Holighaus, D{\"o}rfler and Grill, 2011: they approach the problem differently - by means of a nonstationary Gabor transform. This allows perfect reconstruction for first time while the transform is still computationally efficient (faster than Sch{\"o}rkhuber and Klapuri, 2010). However, it does not allow real-time implementations and phases are not accurate. Code: \url{http://www.univie.ac.at/nonstatgab/toolbox.php}
    \item
	    Holighaus, D{\"o}rfler, Velasco and Grill, 2012: Based on the Velasco, Holighaus, Dörfler and Grill, 2011 - allowing perfect reconstruction. They propose sliCQT (slicing by using an overlapping window) to allow real-time computations. Code: \url{http://www.univie.ac.at/nonstatgab/toolbox.php}
    \item
	    Sch{\"o}rkhuber, Klapuri, Holighaus and Dörfler, 2014: Based on the Velasco, Holighaus, Dörfler and Grill, 2011 - allowing perfect reconstruction. They solve the problem with phases by means of a frequency mapping and they also propose a Variable-Q transform (that allows ie. ERBlets). Code: \url{http://www.cs.tut.fi/sgn/arg/CQT/}
    \item
	    Sch{\"o}rkhuber, Klapuri, Holighaus and D{\"o}rfler, 2014, a nonstationary Gabor transform, allows perfect reconstruction while the phases are still accurate. It might be interesting to implement this in Essentia.
\end{enumerate}

\todo[inline]{the original judith brown CQT is the origin of the research that led to the NSGT}

The Constant-Q transform (CQT) is time-frequency transform for musical signals, originally designed by \textcite{jbrown}, the relationship between the fundamental frequency and its harmonics on a logarithmic frequency scale more clearly than the linear frequency scale of the traditional discrete Fourier transform (DFT).

The original CQT had no inverse transform, but later works led to approximate inverses \cite{klapuricqt, fitzgeraldcqt}. 
, which has an important application in the perfectly-invertible CQT, or CQ-NSGT \cite{invertiblecqt}

The more general NSGT should be studied instead the CQT for the following reasons:
\begin{itemize}
	\item
		It solves the earlier CQT's \cite{jbrown, klapuricqt, fitzgeraldcqt} lack of stable inverse, which was a known weakness \cite{lackinverse}
	\item
		It can use other potentially interesting frequency scales besides the constant-Q logarithmic scale, such as the psychoacoustically-motivated mel, Bark, or ERB scales \todo{cite me}, or variable-Q scales \todo{cite gamma and other}
\end{itemize}

 The CQT has a high temporal resolution at high frequencies \cite{cqtransient} .

 A demonstration of the CQT is shown in figure \ref{fig:earlycqt}.

\begin{figure}[ht]
	\centering
	\subfloat[Linear frequency spectrum]{\includegraphics[height=4.75cm]{./images-tftheory/violindft.png}}
	\subfloat[Constant-Q transform]{\includegraphics[height=4.75cm]{./images-tftheory/violincqt.png}}
	\caption{Violin playing the diatonic scale, $G_{3} \text{(196Hz)} - G_{5} \text{(784Hz)}$}
	\label{fig:earlycqt}
\end{figure}

Additionally, the constant-Q transform \cite{jbrown, klapuricqt, invertiblecqt} even before its formulation as a specialized variant of the nonstationary Gabor transform \cite{balazs}, is an STFT applied with window of different sizes, which are of long duration at low frequencies to create a fine frequency resolution (and sacrificing time resolution as per the time-frequency uncertainty principle), and gradually decrease the windows in duration to improve the time resolution (and sacrifice frequency resolution). At the same time, consider that the iterative harmonic-percussive source separation algorithms in \cite{driedger, fitzgerald2} use two-pass spectral masking with two different configurations of spectrograms -- one with a large window size (4096 samples in \cite{driedger}, 16384 samples in \cite{fitzgerald2}) for representing the harmonic or pitched instruments sharply and estimating the harmonic mask, and one with a short window size (256 samples in \cite{driedger}, 1024 samples in \cite{fitzgerald2}) for representing percussion or transients more sharply and estimating the percussive mask.

The connection to the CQT, or NSGT, is that these contain within a single transform the high frequency resolution of a the large-size spectrogram in the low frequency regions, and the high time resolution of the small-size spectrogram in the high frequency regions. According to \textcite{musicsepgood}'s survey on music source separation, most spectral masking techniques try to exploit the 

\begin{figure}[ht]
	\centering
	\includegraphics[width=9cm]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal}
	\label{fig:dorflertradeoff}
\end{figure}

The time-frequency tradeoff is demonstrated on a musical glockenspiel signal in figure \ref{fig:dorflertradeoff}. Notice how the wide window spectrogram shows frequency components (horizontal lines) with a sharper definition than the blurry lines in the narrow window spectrogram, while the narrow window spectrogram shows temporal events (vertical lines) with a sharper definition than the wide window spectrogram.

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

\todo[inline]{brief intro to frame theory and math stuff}

\todo[inline]{irregular time and frequency sampling, show grids, varying time-frequency resolution etc.}

\todo[inline]{arbitrary f scales and time scales}

\subsubsection{sliCQ Transform}
\label{sec:theoryslicq}

\todo[inline]{slicq is the realtime variant}

\todo[inline]{whats the output, what does it mean, how does it relate to the FFT coefficients, time-frequency matrix}

\vfill
\clearpage

\subsection{Nonlinear frequency scales for music analysis}
\label{sec:freqscales}

\ichfeedback{better title? this is a bit long. ``Frequency scales that may be useful for music or psychoacoustic purposes'' seems like a mouthful - ``Frequency scales for music analysis``?}

\subsubsection{Scales based on Western pitch}

constant-q, log, western pitch scale, octave

variable-q - same with gamma offset

from \cite{variableq1, variableq2}, same as cq-log but with a gamma parameter

\subsubsection{Psychoacoustic scales}

mel, bark

\vfill
\clearpage

\subsection{Machine learning}
\label{sec:ml}

\ichfeedback{if i'm presenting a neural network, it's probably necessary to have this section?}

\subsubsection{Deep learning}
\label{sec:dl}

\vfill
\clearpage

\subsection{Music source separation}
\label{sec:musicsep}

\subsubsection{Task motivation and definition}

\todo[inline]{purposes and uses - why do we want to do this}

The task of music source separation is to split a mixed song into its constituent components, or sources. \textcite{musicsepgood} describe that music source separation could operate on the level of instruments, or for broader categories of sources, grouped into harmonic, percussive, and singing voice.

\subsubsection{Public datasets}

\ichfeedback{feels like a good section to have? MUSDB18-HQ, different datasets - there are many and it's an important consideration in music source separation}

\subsubsection{Evaluation measures}

\todo[inline]{bss, peass, snr-si, sdr-si? subjective, objective, MOS}

\subsubsection{Survey of computational approaches}

\ichfeedback{spectral masking, NMF, machine learning, deep learning - i can lean on the machine learning introduction section right before}

\todo[inline]{summary of approaches over the year e.g. nonnegative matrix factorization to machine learning to deep learning}

\subsubsection{Time-frequency masking and oracle estimators}

\ichfeedback{i think the idea of the oracle mask computed from ground truths is important enough to be in the section title}

\ichfeedback{it will come up later in the thesis when choosing hyperparameters for the sliCQ}

\textcite{musicsepgood}'s survey on music source separation describes how many contemporary methods of separation are based on the common technique of spectrogram masking \cite{masking, speechmask, musicmask}, and exploit that different sources have different characteristic representations in the spectrogram -- see figure \ref{fig:sepgood} for an illustration. Optimally sparse representation of music signals in the transform domain is an active area of research \cite{sparsitykowalski, sparsitykowalski2}.

\begin{figure}[ht]
	\centering
	\subfloat[hello world]{\includegraphics[height=4.5cm]{./images-mss/mss1.png}}
	\subfloat[hello world]{\includegraphics[height=4.7cm]{./images-mss/mss2.png}}
	\caption{MSS sparsity}
	\label{fig:sepgood}
\end{figure}

Harmonic/percussive source separation is the quintessential example.

\begin{enumerate}
	\item
		Apply the algorithm in two passes, using a wide window STFT to attain a high frequency resolution and perform a better harmonic estimate, and a narrow window STFT to attain a high time resolution and perform a better percussive estimate \cite{fitzgerald2, driedger}
	\item
		Replace the STFT with the CQT to perform a vocal or singing voice separation \cite{fitzgerald2}
\end{enumerate}

The harmonic-percussive source separation algorithm of \textcite{fitzgerald1} is based on median filtering and spectral masking using magnitude spectrograms. The original algorithm with soft masks, and the binary masking and iterative variants introduced in \cite{driedger}, are both implemented in librosa \cite{librosa}, a popular open-source MIR library -- see figure \ref{fig:hpsslibrosa} for a diagram of how the algorithm works. The algorithm exploits transform sparsity, by noticing that harmonic or pitched sounds as horizontal lines in the spectrogram, percussive sounds as vertical lines, and estimating each using median filtering. \textcite{driedger}'s iterative variant uses two passes, with a large-window STFT to generate a spectrogram with a higher frequency resolution in the first pass for an improved harmonic estimate, and a small-window STFT to generate a spectrogram with a higher time resolution in the second pass for an improved percussive estimate.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/sphx_glr_plot_hprss_001.png}
	\caption{Harmonic/percussive/residual source separation in librosa}
	\label{fig:hpsslibrosa}
\end{figure}


These algorithms sacrifice separation quality for simplicity, scoring rather low in recent objective evaluations\cite{sisec2018}, losing in objective metrics and subjective evaluations to solutions based on deep learning\cite{umx, demucs}.
Surveys on speech \cite{speechmask} and music separation \cite{musicmask} indicate that the majority of separation algorithms use the technique of time-frequency masking (or spectral masking) to separate the sources.

%\begin{wrapfigure}{r}{8cm}
\begin{figure}[ht]
	\vspace{-1.0em}
	\includegraphics[width=8cm]{./images-mss/maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising. The oracle mask is the ideal mask for a given signal -- to compute it, the target and interference signals must be known.}
	\label{fig:masks}
	\vspace{-1.5em}
%\end{wrapfigure}
\end{figure}

\textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins. A soft mask has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only 0 and 1. The soft mask used in \cite{fitzgerald1, fitzgerald2} is a Wiener filter given in the following equation, where $\hat{S}$ represents the complex-valued spectrogram:
\[ M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|^{2}}{|\hat{S}_{\text{interference}}|^{2} + |\hat{S}_{\text{target}}|^{2}} \]

Soft masks generally produce higher quality sound. An illustration of spectral masking is shown in figure \ref{fig:masks}.

Most recently, the SigSep\footnote{\url{https://sigsep.github.io/}} community has been running the Signal Separation Evaluation Campaign (SISEC), which sets the tone for the modern state-of-the-art models. SiSec uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality, or BSSv4 variant.

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset \cite{musdb18} (or the HQ, high-quality, equivalent \cite{musdb18-hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research.

In modern music source separation, the stems of MUSDB18 have determined the four most common sources to separate -- drums, bass, vocals, and other. SigSep's own network, Open-Unmix, is trained only on MUSDB18, and produces near-state-of-the-art results. The absolute state-of-the-art crown is jointly held by Conv-Tasnet and Demucs, both of which surpass Open-Unmix. Both models operate directly on the waveform domain, which indicates that they could surpass the maximum possible poerformance of time-frequency masking approaches, as is done in speech separation. However, in practise they are still below the limits of masking-based approaches.

\subsubsection{Open-Unmix (UMX)}

 \textcite{umx}'s deep learning model for music source separation is intended to be a near state-of-the-art, open implementation based on the open MUSDB18 and MUSDB18-HQ datasets and designed to foster source separation research \cite{musdb18, musdb18hq}. A deep neural network is used to estimate the magnitude spectrograms of the sources given a mixed song as an input. The sources are the same as the four stems per track in MUSDB18: drums, vocals, bass, other. Finally, the estimate is used to compute a soft mask.

\vfill
\clearpage

\section{Methodology}

\subsection{sliCQ transform in deep learning}

\subsubsection{Matrix and ragged forms}

\todo[inline]{first describe the reference library's generator-based design for ragged and matrix}

\subsubsection{PyTorch implementation}

\todo[inline]{explain how this is necessary for neural networks and deep learning}

\todo[inline]{describe optimizations done here}

\todo[inline]{describe optimizations case by case or line by line?}

\subsubsection{Grouping frequency bins by time resolution}

\todo[inline]{call it the ragged transform}

\subsubsection{Performance benchmarks}

\subsubsection{Overlapping slices and plotting sliCQ spectrograms}

\ichfeedback{overlap-add 50\% slice business here too, showing its non-invertible, use Essentia to support argument, justifying how i need to add a final convolutional layer to grow by 50\%}

\todo[inline]{overlap essentia stuff: \url{https://mtg.github.io/essentia-labs/news/2019/02/07/invertible-constant-q/}}

\todo[inline]{show slicq overlap and plotting code from library}

\subsection{Choosing sliCQ parameters with oracle estimators}

The first experiment to run is to discover which configuration of the NSGT has the potential to surpass the maximum performance of time-frequency masking using the STFT spectrogram. This should indicate whether substituting the NSGT in Open-Unmix is worthwhile.

\subsubsection{Mix-phase oracle}

Given ground truth data, such as what is available in MUSDB18, we have available to us the individually recorded sources for each track. From this, we can compute the ideal or oracle masks::

\begin{flalign}
	\nonumber \text{given: } & x_{\text{drums}}, x_{\text{vocals}}, x_{\text{bass}}, x_{\text{other}}\\
	\nonumber & x_{\text{mix}} = \sum{x}\\
	\nonumber & \hat{X} = \text{STFT}(x), \text{complex-valued}\\
	\nonumber & |\hat{X}|^{\alpha} = \text{magnitude STFT of } x \text{ raised to } \alpha \text{ power, real-valued}\\
	\nonumber & \text{Mask}_{\text{source}} = \frac{|\hat{X}|_{\text{source}}^{\alpha}}{\sum{|\hat{X}|^{\alpha}}}\\
	\nonumber & \hat{Y}_{\text{source}} = \text{Mask}_{\text{source}} * \hat{X}_{\text{mix}}\\
	\nonumber & y = \text{ISTFT}(\hat{Y})\\
	\nonumber & y = \text{ideal estimate of } x
\end{flalign}

Finally, using these pairs of $x, y$, we can get the maximum possible BSS metrics of any algorithm or model based on time-frequency masking. This is the methodology used to find the so-called oracle estimator or oracle mask, which represents the maximum possible performance of any algorithm or model for music source separation that uses spectrogram masking (CITE UMX, etc.).

Approaching or surpassing the ideal mask performance is a common benchmark in source separation literature. \todo{weak} Demucs talks about it but doesn't achieve it.

\subsubsection{Random grid search}

The space of possible NSGT parameters is large. First, there are multiple frequency scales to choose from -- the log (or octave) scale is the case of the CQT, but the NSGT can also use the Mel and Bark psychoacoustic scales. For the frequency bins, the original CQT papers \cite{klapuricqt, invertiblecqt} use multiples of 12 bins or pitches per octave (based on the 12-tone pitch scale), and use one or many octaves, leading to CQTs with 12-96 frequency bins in steps of 12. When using the Mel or Bark scales however, there is no need to continue with the 12-tone pitch per octave strategy, since we are already departing from music theory. Also, bins above 96 are not described in the literature, but without a good justification. Therefore, the range of bins evaluated are integers between 12-348, not just multiples of 12.

The minimum and maximum frequencies of the scales are also configurable. To eliminate one parameter from the evaluation, the maximum frequency was fixed at 22050 Hz, or the Nyquist rate, half of the sampling rate of 44.1 kHz in MUSDB18-HQ. The minimum frequency is varied between 15-60 Hz (below the 20 Hz limit of human hearing\todo{citeme}, up to 60 Hz to include the value of 57 Hz used in \cite{klapuricqt}). Finally, the variable-Q log scale introduces the additional parameter of $\gamma$ for the frequency offset, which is varied between 0-100 Hz.

The BSS evaluation is computationally expensive -- for an average track in MUSDB18-HQ, it takes 30 seconds to compute all of the BSS metrics, even after the significant performance optimizations described previously.

\todo[inline]{describe optimizations here - small-ish section nbd - cupy replacing scipy, and that's that}

For a more fair evaluation, 3 separate randomly-selected tracks from the MUSDB18-HQ test set are considered (with a fixed random seed for consistency). We would prefer to find an NSGT configuration that performs best across multiple tracks, and not overfit to one particular song. For 3 songs, the BSS evaluation of the ideal magnitude ratio mask for a single configuration of NSGT therefore takes 1.5 minutes. Although there are 4 possible types of oracle mask based on spectrograms (ideal magnitude binary mask, ideal power binary mask, ideal magnitude ratio mask, ideal power ratio mask), for the optimization process the most common case of ideal ratio magnitude mask is used.

With all of this information in mind, an exhaustive grid search across all NSGT configurations is not feasible. The Bayesian optimization technique \cite{bayesian} was selected, as it is ``an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate'' and ``best-suited for optimization over continuous domains of less than 20 dimensions.'' The open-source Python library \textit{BayesianOptimization}\footnote{\href{https://github.com/fmfn/BayesianOptimization}{https://github.com/fmfn/BayesianOptimization}} was used in the testbench implementation.

The Bayesian optimization process considers a black-box function (that being optimized) which takes varying input parameters and outputs a single scalar value. The function was designed to accept the configuration of the NSGT as an input, and return a single scalar value representing the median SDR score with the ideal magnitude ratio mask across all 4 MUSDB18-HQ targets (drums, vocals, bass, other) and across the 3 tracks. A different value can be returned to target different aspects of source separation performance -- for example, one could optimize on the SAR if their primary goal is to reduce artifacts. However, SDR is the most commonly used single-value metric to demonstrate overall source separation effectiveness in literature.

The Bayesian optimization process can additionally perform some random parameter selections in the beginning, to help improve the subsequent iterative optimization. Table \ref{table:nsgtparamsirm} contains the parameter ranges of the evaluated NSGT configs and the number of Bayesian optimization iterations. Table \ref{table:nsgtbayesresults} shows the best configurations selected by the optimization process, compared to the control ideal ratio magnitude mask performance of the STFT with the default Open-Unmix settings of window size = 4096 and overlap = 1024 \cite{umx}.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|c|c|c| }
	 \hline
	  Scale & Parameters & Random iter & Optimization iter \\
	 \hline
	 \hline
	 VQ-Log & 12-348 bins, 15-60 Hz fmin, 0-100 Hz $\gamma$ & 20 & 180 \\
	 \hline
	 CQ-Log & 12-348 bins, 15-60 Hz fmin & 20 & 180 \\
	 \hline
	 Mel & 12-348 bins, 15-60 Hz fmin & 20 & 180 \\
	 \hline
	 Bark & 12-348 bins, 15-60 Hz fmin & 20 & 180 \\
	 \hline
\end{tabular}
	\caption{NSGT parameter ranges evaluated by Bayesian optimization}
	\label{table:nsgtparamsirm}
\end{table}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|c|c|c| }
	 \hline
	  Transform & Parameters & Median SDR \\
	 \hline
	 \hline
	 STFT, UMX default & window = 4096, overlap = 1024 & 6.65 \\
	 \hline
	 STFT, wide & window = 16384, overlap = 4096 & 6.16 \\
	 \hline
	 STFT, narrow & window = 1024, overlap = 256 & 5.37 \\
	 \hline
	 NSGT, CQ-Log & 138 bins, 47.3 Hz fmin & 7.20 \\
	 \hline
	 NSGT, VQ-Log & 134 bins, 47.3 Hz fmin, 1.7 Hz gamma & 7.20 \\
	 \hline
	 NSGT, Mel & 263 bins, 24.9 Hz fmin & 7.44 \\
	 \hline
	 NSGT, Bark & 239 bins, 30.1 Hz fmin & 7.45 \\
	 \hline
\end{tabular}
	\caption{Best NSGT configurations selected by Bayesian optimization}
	\label{table:nsgtbayesresults}
\end{table}

Finally, a full evaluation was done on all 50 tracks of the MUSDB18-HQ test set. The output is a box plot, in the same style as the SiSec 2018 campaign using the same evaluation code.\footnote{\href{https://github.com/sigsep/sigsep-mus-2018-analysis}{https://github.com/sigsep/sigsep-mus-2018-analysis}} The resulting box plots can be seen in figure \ref{fig:nsgtboxplots}.

\begin{figure}[ht]
	\centering
\makebox[\textwidth]{\includegraphics[width=15cm]{./images-bss/oracle_boxplot.pdf}}
\caption{Music separation results for NSGT oracle masks}
\label{fig:nsgtboxplots}
\end{figure}

Considering again the IRM1 (for simplicity), the median SDR score across all tracks for each of the 4 sources is shown in table \ref{table:nsgtbayesresults2} for the same STFT and NSGT configurations from table \ref{table:nsgtbayesresults} are shown along with the SDR score per target, median across all tracks.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|l|l|c|c|c|c|c| }
	 \hline
	  Transform & Vocals SDR & Drums SDR & Bass SDR & Other SDR \\
	 \hline
	 \hline
	 STFT, UMX default & 8.62 & 7.07 & 6.68 & 6.87 \\
	 \hline
	 STFT, wider & 6.16 \\
	 \hline
	 STFT, narrow & 5.37 \\
	 \hline
	 NSGT, CQ-Log & 7.20 \\
	 \hline
	 NSGT, VQ-Log & 7.20 \\
	 \hline
	 NSGT, Mel & 7.44 \\
	 \hline
	 NSGT, Bark & 10.42 & 9.29 & 8.06 & 8.21 \\
	 \hline
\end{tabular}
	\caption{SDR per source, full MUSDB18-HQ test set evaluation}
	\label{table:nsgtbayesresults2}
\end{table}

To sum up, using the best selected Bark-scale NSGT discovered by 200 iterations of Bayesian optimization achieved improvements of \textbf{+1.3-2.2} \todo{adjust for fuller testbench} points in median SDR score over the STFT. However, recall that this is simply the maximum possible performance of spectral masking with the NSGT, assuming that the system can produce a perfect estimate of the sources.

The next step is to adapt UMX to use the Bark-scale NSGT instead of the STFT, and check how much of the theoretical SDR improvements occur in the actual separation model, trained only on MUSDB18-HQ. The goal is to try to surpass Conv-Tasnet and Demucs with the UMX-NSGT variant.

\subsubsection{Comparing different sliCQ and STFT configurations}

\ichfeedback{I don't love the title here but what i mean is i want to pick, as we discussed, multiple STFT window sizes e.g. 512 1024 2048 4096 8192, and multiple sliCQ configurations e.g. Bark, Mel, 33Hz, 57Hz, 200 bins, 500 bins, 12 bins and contrast them all side by side - visual spectrograms, describe the time-frequency resolution, etc.}

\ichfeedback{if you pick a bad scoring config, e.g. Constant-Q Log scale with 14 frequency bins and frequency limits 12.3 Hz -- 22050 Hz, you can get an oracle score of -7 dB SDR (completely terrible performance - i've seen it happen), and if you pick a good one, you get +8.9 dB SDR (surpassing the STFT)}

\ichfeedback{it would be good to plot the magnitude and phase of the good and bad slicq. most likely the ``bad'' one is losing phase information when we ignore its phase. something must be controlling why a few hz in either direction is causing such a dramatic SDR difference}

\subsection{Open-Unmix with the sliCQ transform}

\todo[inline]{incorporate my sparsity hypothesis/justification for nsgt}

\todo[inline]{better hope umx shines here}

\subsubsection{Working with the ragged sliCQ transform}

\ichfeedback{describe here how frequency bins are grouped by the same time-resolution, to produce a list of ``blocks'' of time-frequency coefficients}

\subsubsection{Convolutional neural network architecture}

\todo[inline]{grais and plumbley 2 papers can go here}

\subsubsection{Improved loss functions from CrossNet-Open-Unmix}

\vfill
\clearpage

\section{Experiment}

\subsection{Training}

foo

\subsection{Results}

\ichfeedback{this will be the real results section evaluated on the MUSDB18-HQ test set on my computer, compared to the pretrained models of classic umx and xumx}

\subsection{ISMIR 2021 Music Demixing Challenge}

\ichfeedback{i know this shouldn't be a core part of the thesis, but it's still fun to describe?}

\vfill
\clearpage

\section{Conclusion}
\label{sec:conclusion}

foo

\subsection{Outlook}

foo

\subsection{Summary}

foo

\vfill
\clearpage % force a page break before references

%\nocite{*}
\section{References}
\printbibliography[heading=none]

\vfill
\clearpage %force a page break

\begin{appendices}

\section{Code availability and replicating the results}
\label{appendix:coderesultsrepro}

\end{appendices}

\end{document}
