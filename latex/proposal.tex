\documentclass[letter,12pt]{article}
\usepackage[svgnames]{xcolor}
\usepackage[left=2.5cm, right=2.5cm, top=2cm, bottom=2cm]{geometry}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = DarkBlue, %Colour for external hyperlinks
  linkcolor    = black, %Colour of internal links
  citecolor   = black %Colour of citations
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate,annotation,url=false,doi=false]{biblatex-chicago}
\addbibresource{citations.bib}
\input{variables.tex}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\renewcommand*{\postvolpunct}{\addcolon\addspace}

\title{\vspace{-2.25em}\ThesisTitle\vspace{-0.5em}}
\author{Sevag Hanssian}
\date{\vspace{-0.5em}November 12, 2021\vspace{-1.25em}}

\begin{document}

\maketitle

\section{Introduction / Motivation}

Music source separation, or music demixing, is the task of decomposing musical audio into its constituent sources, which are typically isolated instruments (e.g., drums, bass, and vocals). Music demixing algorithms commonly operate on the Short-Time Fourier Transform (STFT) of the audio signal \parencite{musicsepgood}. Due to the time-frequency uncertainty principle \parencite{gabor1946}, the STFT of a signal cannot be maximally precise in both time and frequency, and the tradeoff in time-frequency resolution can significantly affect music demixing results \parencite{tftradeoff1}. In this thesis, the sliCQT \parencite{slicq}, an STFT-like transform with varying time-frequency resolution, is explored as a replacement for the STFT in a state-of-the-art deep learning model for music demixing.

The STFT is computed by applying the discrete Fourier Transform on fixed-size windows of the input signal. \textcite{doerflerphd} argues, based on auditory and musical considerations, that musical signals should be analyzed with long windows in the low-frequency regions to capture detailed harmonic information, and short windows in the high-frequency regions to capture transients with sharp temporal localization. The sliCQ Transform, or sliCQT, is a realtime implementation of the Nonstationary Gabor Transform (NSGT) of \textcite{balazs}. The NSGT is an invertible time-frequency transform that applies the Fourier Transform on windows of the input signal that can be varied by frequency region. An important application of the NSGT and sliCQT is to implement the Constant-Q Transform (CQT) \parencite{jbrown} for music analysis, which uses a logarithmic frequency scale to better show the relationship between the fundamental frequency of a musical sound and its harmonics.

Machine learning models for music source separation have achieved recent success \parencite{sisec2018}, and the STFT is used by several of the top performers. Open-Unmix \parencite{umx} was released as a state-of-the-art baseline and reference implementation for music demixing based on the STFT and published as open-source software.\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} In this thesis, Open-Unmix will be adapted to use the sliCQT in place of the STFT to investigate the viability of using the sliCQT for music demixing.

\section{Previous Work}

Computational source separation has a long history \parencite{musicsepgood}, originating from the task of separating speech from background noise. Speech algorithms could not be generalized easily to music, and techniques more specific to music source separation were developed as a result \parencite{musicsepintro1}. Accordingly, musical source models arose that exploit the distinct spectral characteristics of the target sources (e.g., harmonic, percussive, or vocals) in the STFT domain, such as Kernel Additive Modeling or Nonnegative Matrix Factorization \parencite{musicsepgood}.

Model-based methods are ``prone to large errors and poor performance'' \parencite[13]{musicsepintro1}, and manipulating time-frequency resolution is one possible strategy to improve their results. \textcite{driedger} used multiple STFTs with different window sizes, \textcite{fitzgerald2} replaced the STFT with the CQT, and \textcite{wavelets} used a custom time-frequency transform based on wavelets. More recently, data-driven models based on deep neural networks (DNN) surpassed previous approaches \parencite{sisec2018, musicsepintro1}.

The earliest approaches for DNN-based music demixing started with fully connected networks (FCN), but these needed many parameters due to the large size of input music spectrograms, which limited the networks to operate on sliding windows of under one second of temporal context \parencite{musicsepgood}. Recurrent neural networks (RNNs) \parencite{umxorig1} and convolutional neural networks (CNNs) \parencite{plumbley1} have both been used to overcome this limitation, as they need fewer parameters for long temporal sequences of input data \parencite{musicsepgood}. A recent example of music demixing in the STFT domain with an RNN architecture is Open-Unmix \parencite{umx}.

\section{Proposed Research / Methodology}

The adaptation of Open-Unmix to use the sliCQT will be done in two steps, using the reference implementation of Open-Unmix\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}} as a starting point. First, the reference Python implementation of the sliCQT\footnote{\url{https://github.com/grrrr/nsgt}} needs to be modified to use PyTorch,\footnote{\url{https://pytorch.org}} because Open-Unmix uses PyTorch as its GPU deep learning framework. Next, the code of Open-Unmix should be modified to replace the STFT with the sliCQT, using an RNN or CNN architecture, while tuning the parameters of the sliCQT to surpass the performance of the STFT.

The MUSDB18-HQ dataset \parencite{musdb18hq} and BSS (Blind Source Separation) eval metrics \parencite{bss} are standard for training and evaluating music demixing systems, and they have been used in previous Signal Source Separation Evaluation Campaigns (SiSEC) \parencite{sisec2018}. The model in this thesis will use the MUSDB18-HQ dataset for both training and evaluation, using BSS eval metrics to allow fair comparisons with Open-Unmix and other published models.

\section{Contributions / Summary}

In music demixing approaches that use the STFT, choosing the appropriate time-frequency resolution plays an important role. In this thesis, the sliCQT with varying time-frequency resolution is explored as a replacement for the STFT in a state-of-the-art deep learning model for music demixing. It is hoped that the resulting model will demonstrate the viability of music demixing with the sliCQT.

\vfill
\clearpage %force a page break

%\nocite{*}

\begingroup
\setstretch{0.9}
\setlength\bibitemsep{0.015em}
\printbibheading[title={References},heading=bibnumbered]
\printbibliography[heading=none]
\endgroup

\vfill
\clearpage %force a page break

\end{document}
