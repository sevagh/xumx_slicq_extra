\documentclass[letter,12pt]{article}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
%\setlength{\parindent}{0pt}
%\usepackage[shortlabels]{enumitem}
%\usepackage{graphicx}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{verbatim}
%\usepackage{listings}
%\usepackage{minted}
%\usepackage{subfig}
\usepackage{todonotes}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate,annotation]{biblatex-chicago}
\addbibresource{citations.bib}
\input{variables.tex}

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
}{\end{enumerate}}

\title{\ThesisTitle}
\author{Sevag Hanssian, sevag.hanssian@mail.mcgill.ca}

\begin{document}

\maketitle

\section{Introduction / Motivation}

The short-time Fourier transform (STFT) is an important tool for the time-frequency analysis of acoustic signals. It is computed by applying the discrete Fourier transform on overlapping, fixed-size windows of the input signal, and it is commonly used to represent music in computational music information retrieval (MIR). Despite its popularity, the STFT limited by a fixed and bounded time-frequency resolution. In the sliCQ transform (\cite{invertiblecqt}), which is the realtime variant of the Nonstationary Gabor Transform (\cite{balazs}), the Fourier transform is applied with varying window sizes to adapt the time-frequency resolution to the characteristics of the studied signal. One important application of the sliCQ transform is to implement an invertible constant-Q transform for music (\cite{jbrown, klapuricqt}).

In the task of music source separation (\cite{musicsepgood}), a mixed song is decomposed into its constituent sources, which are typically either different instruments (drums, bass, vocals) or groups of sources with similar characteristics (percussive, harmonic). Machine learning models for music source separation with the STFT have achieved success in recent years (\cite{sisec2018}), but choosing the appropriate time-frequency resolution for the STFT is an important step which can significantly affect results (\cite{tftradeoff1, tftradeoff2}). In this thesis, the STFT is replaced with the sliCQ transform in Open-Unmix (\cite{umx}), a neural network for music source separation. The proposed model was submitted to the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} where it successfully separated the hidden data set within the required time limit. The sliCQ-based model did not surpass Open-Unmix, but demonstrates a viable and flexible starting point for future improvements.

\section{Previous Work}

Computational source separation has a long history (\cite{musicsepsurvey}), originating from the tasks of computational auditory scene analysis (CASA) and blind source separation (BSS). In CASA and BSS, the mixed audio contains unknown sources that must be separated. In music source separation, the sources are known and fixed. Most commonly, 4 sources (vocals, drums, bass, other) are used as defined by the MUSDB18 dataset (\cite{musdb18}). Independent Component Analysis (ICA) can be used to exploit spatial information of the sources, which can be used when there are as many channels in the mixture (corresponding to differently placed microphones) as the number of sources (\cite{musicsepsurvey, musicsepgood}). In the case of monoaural or stereo music, spatial information cannot be used. Sources are assumed to be sufficiently different from each other such that they can be extracted with time-frequency masks applied in the spectral domain. These methods include techniques like Kernel Additive Modeling (KAM) or Nonnegative Matrix Factorization (NMF) (\cite{musicsepsurvey, musicsepgood}).

More recently, data-driven approaches based on deep learning leapfrogged past approaches (\cite{sisec2018}). Open-Unmix was created by \textcite{umx} as an open-source, reference implementation of a state-of-the-art deep neural network for music source separation. Open-Unmix is based on STFT masking, and is intended to foster reproducible research by only using the open MUSDB18 dataset for training data (\cite{musdb18, musdb18hq}).

Manipulating time-frequency resolution is a common strategy for improving spectrogram-based music source separation. \textcite{fitzgerald1, driedger} use multiple STFTs with different window sizes. \textcite{fitzgerald2, cqtseparation, bettermusicsep} replaced the STFT with the constant-Q transform. \textcite{plumbley2} used varying sizes of time and frequency filters applied to the STFT according to the constant-Q scale, to separate singing voice in music.

\section{Proposed Research / Methodology}

To support the use of the sliCQ transform inside deep learning networks, the reference implementation of the sliCQ transform\footnote{\url{https://github.com/grrrr/nsgt}} needs to be modified to add support for GPU computation using a deep learning framework. This first contribution is necessary to use the sliCQ transform in GPU-based machine or deep learning models. Next, sliCQ parameters must be chosen that can surpass the STFT in music source separation. Spectrograms from the sliCQ transform will be shown alongside STFT spectrograms with different window sizes to demonstrate the improved visual clarity of tonal and transient components, and better detection of musical events.

Finally, Open-Unmix will be adapted to use the sliCQ transform in place of the STFT, potentially incorporating ideas from other neural networks for music source separation including the convolutional denoising autoencoder architecture (\cite{plumbley1, plumbley2}). The network should be trained using only the open MUSDB18-HQ dataset (\cite{musdb18hq}), and music separation results will be compared with the pre-trained Open-Unmix model using the BSS evaluation metrics (\cite{bss, sisec2018}).

\section{Contributions / Summary}

In music source separation approaches that use the STFT, choosing the appropriate time-frequency resolution plays an important role. In this thesis, the sliCQ transform with varying time-frequency resolution is explored as a replacement for the STFT in a deep learning model for music source separation. The proposed model was submitted to the ISMIR 2021 Music Demixing Challenge. It passed the evaluation suite, showing that the sliCQ transform can be used in an effective music source separation system. The final submitted model did not surpass the score of the original Open-Unmix model. However, the sliCQ transform is a flexible transform, and future work could explore alternative parameter search strategies or neural network architectures to further improve the source separation performance.

\vfill
\clearpage %force a page break

%\nocite{*}
\printbibheading[title={References},heading=bibnumbered]
\printbibliography[heading=none]

\vfill
\clearpage %force a page break

\end{document}
