\documentclass[report.tex]{subfiles}
\begin{document}

\section{Experiment}
\label{sec:experiment}

The hardware and software specifications of the computer which produced the results in this section are as follows:
\begin{tight_enumerate}
	\item
		Motherboard: Gigabyte Aorus X570 Elite Wifi
	\item
		CPU: AMD Ryzen 5950X
	\item
		Memory: 64GB DDR4
	\item
		Storage: 1TB ADATA SX8200PNP NVMe
	\item
		GPUs: NVIDIA RTX 3080 Ti (12GB VRAM) and NVIDIA RTX 2070 Super (8GB VRAM)
	\item
		OS: Fedora 34 Workstation Edition, 64-bit
	\item
		Linux kernel version: 5.133.10-200
	\item
		NVIDIA driver version: 470.63.01
	\item
		NVIDIA CUDA toolkit version: 11.4
\end{tight_enumerate}

The Python environments used were designed to be reproducible with pip requirements.txt files or Conda environment files bundled with the source code:

\begin{tight_enumerate}
	\item
		Pip file for oracles, trained model evaluations, boxplot creation, and performance benchmarks:\\
		\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/blob/main/mss_evaluation/mss-oracle-experiments/requirements-cupy.txt}
	\item
		Conda environment file for the NSGT/sliCQT PyTorch implementation:\\
		\url{https://github.com/sevagh/nsgt/blob/main/conda-env.yml}
	\item
		Conda environment file for the xumx-sliCQ neural network:\\
		\href{https://github.com/sevagh/xumx-sliCQ/blob/main/scripts/environment-gpu-linux-cuda11.yml}{https://github.com/sevagh/xumx-sliCQ/blob/main/scripts/environment-gpu-linux-cuda11.yml}
\end{tight_enumerate}

The repositories associated with this thesis are as follows:
\begin{tight_enumerate}
	\item
		xumx-sliCQ neural network:\\
		\url{https://github.com/sevagh/xumx-sliCQ}
	\item
		NSGT/sliCQT PyTorch fork:\\
		\url{https://github.com/sevagh/nsgt}
	\item
		museval (BSS metrics evaluation) CuPy fork:\\
		\url{https://github.com/sevagh/sigsep-mus-eval}
	\item
		LaTeX files and scripts for generating this thesis, including all plots and results:\\
		\url{https://gitlab.com/sevagh/xumx_slicq_extra}
	\item
		Submissions made to the ISMIR 2021 Music Demixing Challenge:\\
		\url{https://gitlab.aicrowd.com/sevagh/music-demixing-challenge-starter-kit}
\end{tight_enumerate}

The author of this thesis takes code availability and reproducibility seriously. Please contact me if you encounter any errors, discrepancies, or difficulties with reproducing any of the results:
\begin{tight_enumerate}
	\item
		Academic e-mail: sevag.hanssian@mail.mcgill.ca
	\item
		Personal e-mail: sevagh+thesis@pm.me
\end{tight_enumerate}

\subsection{Benchmarks for GPU accelerations}

\subsubsection{Faster BSS metrics with CuPy}

The first benchmark script is for the GPU-accelerated BSS metrics computation using the CuPy library from section \ref{sec:fasterbsscupy}. Recall that the forked copy of the library\footnote{\url{https://github.com/sevagh/sigsep-mus-eval}} contains the modifications made for this thesis. The benchmark script can be viewed here.\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/blob/main/mss_evaluation/mss-oracle-experiments/oracle_eval/benchmark_cupy_eval.py}} For the 14 validation tracks of MUSDB18-HQ, it creates a random estimate (using \Verb#numpy.randn# to create an audio signal), and computes the BSS metrics. It measures the time for each BSS metric computation, summed across 14 tracks and repeated for 10 iterations. The final time is the total time for all 14 tracks, divided by 10 to account for the iterations. Three copies of the script were run; one with the argument \Verb#--disable-cupy# to use the CPU code with SciPy and NumPy functions on a Ryzen 5950X with 64GB RAM, one with the argument \Verb#--cuda-device=0# to use the NVIDIA RTX 3080 Ti GPU, and one with the argument \Verb#--cuda-device=1# to use the NVIDIA RTX 2070 Super GPU.

The results were that the regular BSS metrics evaluation on the CPU took 1485.77 seconds, compared to the CuPy GPU-accelerated code which took 585.33 seconds with the 3080 Ti and 738.39 seconds with the weaker 2070 Super. The speedup from CuPy is \textasciitilde 2-2.5x over the Ryzen 5950X CPU. This is a significant reduction that can aid in future large-scale source separation evaluation campaigns like SiSec (\cite{sisec2016}, \cite{sisec2018}). Also, note the passing regression test in listing \ref{lst:regressionout}, showing that the library is working correctly with the speedup.

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
(museval-cupy) sevagh:sigsep-mus-eval \$ py.test tests/test_regression.py -vs
===================================================== test session starts =====================================================
platform linux -- Python 3.9.6, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /home/sevagh/venvs/museval-cupy/bin/python
cachedir: .pytest_cache
rootdir: /home/sevagh/repos/sigsep-mus-eval, configfile: setup.cfg
collected 4 items

tests/test_regression.py::test_aggregate[Music Delta - 80s Rock]     time         target metric     score                   track
[...]
Aggrated Scores (median over frames, median over tracks)
vocals          ==> SDR: -15.622  SIR:   9.165  ISR:  -8.476  SAR:  -7.327
accompaniment   ==> SDR: -13.290  SIR: -18.765  ISR:  -0.322  SAR:  -7.427

PASSED
tests/test_regression.py::test_track_scores[Music Delta - 80s Rock] PASSED
tests/test_regression.py::test_random_estimate[Music Delta - 80s Rock] PASSED
tests/test_regression.py::test_one_estimate[Music Delta - 80s Rock] PASSED
\end{minted}
  \caption{Passing regression test output for the CuPy GPU speedup}
  \label{lst:regressionout}
\end{listing}

\newpagefill

\subsubsection{Faster NSGT/sliCQT with PyTorch}

The second benchmark script is for the PyTorch implementation of the NSGT/sliCQT library from section \ref{sec:torchslicq}. The forked copy of the library contains the PyTorch implementation,\footnote{\url{https://github.com/sevagh/nsgt}} and the benchmark scripts were added in the same repository.\footnote{\url{https://github.com/sevagh/nsgt/blob/main/examples/benchmark.py, https://github.com/sevagh/nsgt/blob/main/examples/run_bench.sh}} The benchmark does the forward and backward sliCQT using the Bark scale with 50 bins between 20--22050 Hz. The sliCQT is taken on a 3:54 minute mixed song from the MUSDB18-HQ dataset, ``Zeno - Signs.'' The sliCQT parameters were chosen so that the transform of the song occupied a maximum of 7.2 GB of memory and could fit on the smallest memory device being benchmarked, the NVIDIA 2070 Super. The computation time was measured and averaged across 100 iterations. The cost of the memory transfer of the song to the GPU was not included in the measurement. Tables \ref{table:nsgttorchresultsmatrix} and \ref{table:nsgttorchresultsragged} show the benchmark results for the matrix and ragged sliCQT respectively.

\begin{table}[ht]
	\centering
	\begin{tabular}{ |l|l|l|l| }
	 \hline
		NSGT library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU, multithreaded & 60.56  \\
	 \hline
		Original & CPU, single-threaded & 9.50  \\
	 \hline
		PyTorch & CPU & 3.10  \\
	 \hline
		PyTorch & GPU (2070 Super) & 2.62 \\
	 \hline
		PyTorch & GPU (3080 Ti) &  2.40 \\
	 \hline
\end{tabular}
	\caption{Execution times for the matrix form of the forward + backward sliCQT}
	\label{table:nsgttorchresultsmatrix}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{ |l|l|l|l| }
	 \hline
		NSGT library & Device & Time (s) \\
	 \hline
	 \hline
		Original & CPU, multithreaded & 51.95  \\
	 \hline
		Original & CPU, single-threaded & 8.72  \\
	 \hline
		PyTorch & GPU (2070 Super) & 2.52 \\
	 \hline
		PyTorch & GPU (3080 Ti) &  2.38 \\
	 \hline
		PyTorch & CPU & 1.85  \\
	 \hline
\end{tabular}
	\caption{Execution times for the ragged form of the forward + backward sliCQT}
	\label{table:nsgttorchresultsragged}
\end{table}

The execution time of the transform improved with PyTorch. The GPUs have an advantage over the CPU in the matrix form of the transform, but not in the ragged form. This can be explained by the fact that the ragged transform is a list of tensors. To perform operations on the ragged transform requires looping over the list which negates some of the benefits of GPU parallelism. Even though the GPU is not faster than the CPU for the tested parameters of ragged sliCQT, having the transform on the GPU allows us to use the sliCQT inside a PyTorch neural network. The original library has a multithreaded option which performs worse than the default single-threaded behavior, but the PyTorch CPU performance leapfrogs both.

\newpagefill

\subsection{Best sliCQT parameters with MPI oracle}

The random hyperparameter search was done to maximize the median SDR score of the MPI oracle across all 4 targets for 60 iterations. The search script is available here,\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/scripts/slicq_explore.py}} and there is also some additional documentation related to the parameter search in the code repository.\footnote{\url{https://github.com/sevagh/xumx-sliCQ/blob/main/docs/slicq_params.md}} The background and parameter range decisions for the search was discussed in \ref{sec:mpiparam}. For demonstration purposes, the inverse SDR score ($-1.0*\text{SDR}$) was maximized in a second search, to find a ``bad'' sliCQT. Repeating the parameter search yields slightly different sliCQT configurations due to the randomness, which is controlled by a script parameter for the random seed.

The sliCQT chosen by one run of the script that produced the best median SDR used 262 bins on the Bark scale between 32.9--22050 Hz, called the ``good sliCQT.'' This is the sliCQT used in the final neural network, xumx-sliCQ. The sliCQT params chosen by the inverse SDR maximization which produced the worst median SDR, or the ``bad sliCQT,'' used 142 bins on the Constant-Q or logarithmic scale between 129.7--22050 Hz. Figure \ref{fig:bipolarslicqs} shows the sliCQT magnitude spectrograms.

\begin{figure}[ht]
	\centering
	\subfloat[Good sliCQT -- Bark, 262 bins, 32.9--22050 Hz]{\includegraphics[width=\textwidth]{./images-gspi/slicqt_good.png}}\\
	\subfloat[Bad sliCQT -- Constant-Q/log, 142 bins, 129.7--22050 Hz]{\includegraphics[width=\textwidth]{./images-gspi/slicqt_bad.png}}
	\caption{sliCQTs from the MPI oracle hyperparameter search}
	\label{fig:bipolarslicqs}
\end{figure}

Finally, the full oracle evaluation on both of these sliCQT parameters are shown in the boxplot in figure \ref{fig:oraclebssboxplot}, alongside different STFT window sizes. The bad sliCQT starts at a high frequency, 129.7 Hz, which is near the maximum of the 10--130 Hz range. From the spectrogram, we can see that the low frequency bins are very diffuse, or blurry. From the boxplot, we can see that the targets which are expected to be more pitched or tonal (vocals, other) perform similar with both the good and bad sliCQT, but the drums and bass are dramatically worse for the bad sliCQT.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./images-bss/oracle_boxplot.pdf}
	\caption{Boxplot for oracle mask evaluations}
	\label{fig:oraclebssboxplot}
\end{figure}

\newpagefill

\subsection{xumx-sliCQ neural network}

\subsubsection{Network architecture and training}

The training parameters of xumx-sliCQ were kept similar to Open-Unmix and CrossNet-Open-Unmix:

The training curves can be seen in figure \ref{fig:networkloss}. These were taken from Tensorboard,\footnote{\url{https://www.tensorflow.org/tensorboard}} which is a visual web component of the Tensorflow deep learning framework (\cite{tensorflow}) that is also compatible with PyTorch. It was used to visualize the loss curves of xumx-sliCQ during training.

\begin{figure}[ht]
	\centering
	\subfloat[Train loss]{\includegraphics[width=\textwidth]{./images-neural/train_loss.png}}
	\hspace{0.5em}
	\subfloat[Validation loss]{\includegraphics[width=\textwidth]{./images-neural/valid_loss.png}}
	\caption{Tensorboard loss curves for xumx-sliCQ, 1000 epochs}
	\label{fig:networkloss}
\end{figure}

\newpagefill

\subsubsection{Music demixing results}

The BSSv4 scores for the demixing results, computed on the test set of the MUSDB18-HQ (\cite{musdb18hq}) dataset, are shown in figure \ref{fig:bssboxplot}. Table \ref{table:bsseval} contains the details of every model evaluated in the boxplot with their label.

\begin{table}[ht]
	\centering
	\begin{tabular}{ |l|l|p{4cm}|p{4cm}| }
	 \hline
	  Project & Legend & Code repository & Pretrained model \\
	 \hline
	 \hline
		\makecell[l]{Open-Unmix \\ \textcite{umx}} & umx & \url{https://github.com/sigsep/open-unmix-pytorch} & \url{https://zenodo.org/record/3370489} (UMX-HQ) \\
	 \hline
		\makecell[l]{CrossNet-Open-Unmix \\ \textcite{xumx}} & xumx & \url{https://github.com/sony/ai-research-code/tree/master/x-umx} & \url{https://nnabla.org/pretrained-models/ai-research-code/x-umx/x-umx.h5} (X-UMX) \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ + sliCQT-Wiener-EM} & slicq-wslicq & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
		\makecell[l]{xumx-sliCQ \\ + STFT-Wiener-EM} & slicq-wstft & \url{https://github.com/sevagh/xumx-sliCQ} & \url{https://github.com/sevagh/xumx-sliCQ/tree/main/pretrained-model} \\
	 \hline
\end{tabular}
	\caption{Evaluated pretrained models in the BSS boxplot}
	\label{table:bsseval}
\end{table}

We discussed in section \ref{sec:replacestft} that after getting the initial estimate with the xumx-sliCQ neural network, there is a post-processing Wiener expectation maximization step that could be done with the sliCQT or the STFT. Both configurations were evaluated and shown in the boxplot to view the difference in music demixing quality.

Note that the default number of iterations for the Wiener-EM step is one, in UMX, X-UMX, and xumx-sliCQ. Anything above one iteration had worse demixing results, and was computationally slower as well. This is shown in listing \ref{lst:wienerem}, where 5 different configurations of xumx-sliCQ's post-processing Wiener-EM were tested on a single track, ``Al James - Schoolboy Facination,'' during my participation in the ISMIR 2021 Music Demixing Challenge.\footnote{\href{https://discourse.aicrowd.com/t/umx-iterative-wiener-expectation-maximization-for-non-stft-time-frequency-transforms/6191/3}\href{https://discourse.aicrowd.com/t/umx-iterative-wiener-expectation-maximization-for-non-stft-time-frequency-transforms/6191/6}} The evaluated configurations were:
\begin{tight_enumerate}
	\item
		1 + 2 iterations of STFT-Wiener-EM
	\item
		1 + 2 iterations of ragged sliCQ-Wiener-EM
	\item
		1 iteration of zero-padded matrix sliCQ-Wiener-EM
\end{tight_enumerate}

\begin{listing}[h]
  \centering
\begin{minted}[numbersep=\mintednumbersep,linenos,mathescape=true,breaklines,frame=single,escapeinside=||,fontsize=\scriptsize]{text}
1 iteration of STFT-Wiener-EM:
    vocals          ==> SDR:   2.727  SIR:   7.992  ISR:   4.126  SAR:   2.082
    drums           ==> SDR:   2.702  SIR:   1.920  ISR:   5.649  SAR:   1.765
    bass            ==> SDR:   4.082  SIR:  10.806  ISR:   1.770  SAR:   2.128
    other           ==> SDR:  -0.939  SIR:  -3.560  ISR:  11.802  SAR:   3.516

    time: 3m41.839s

1 iteration of ragged sliCQ-Wiener-EM:
    vocals          ==> SDR:   2.754  SIR:   7.907  ISR:   4.166  SAR:   2.034
    drums           ==> SDR:   2.885  SIR:   1.511  ISR:   5.872  SAR:   1.649
    bass            ==> SDR:   4.134  SIR:  11.515  ISR:   1.349  SAR:   2.020
    other           ==> SDR:  -0.891  SIR:  -3.382  ISR:  12.241  SAR:   3.458

    time: 7m59.104s

1 iteration of zero-padded matrix sliCQ-Wiener-EM:
    vocals          ==> SDR:   2.753  SIR:   7.953  ISR:   4.158  SAR:   2.024
    drums           ==> SDR:   2.864  SIR:   1.677  ISR:   5.835  SAR:   1.657
    bass            ==> SDR:   4.140  SIR:  11.599  ISR:   1.350  SAR:   2.016
    other           ==> SDR:  -0.893  SIR:  -3.391  ISR:  12.189  SAR:   3.448

    time: 4m28.061s

2 iterations of STFT-Wiener-EM:
    vocals          ==> SDR:   2.368  SIR:   8.594  ISR:   4.081  SAR:   1.094
    drums           ==> SDR:   2.669  SIR:   2.600  ISR:   6.071  SAR:   1.466
    bass            ==> SDR:   3.892  SIR:  11.786  ISR:   1.576  SAR:   1.796
    other           ==> SDR:  -1.620  SIR:  -3.501  ISR:  12.949  SAR:   2.977

    time: 4m5.906s

2 iterations of ragged sliCQ-Wiener-EM:
    vocals          ==> SDR:   2.481  SIR:   8.152  ISR:   4.101  SAR:   1.173
    drums           ==> SDR:   2.832  SIR:   1.770  ISR:   6.270  SAR:   1.372
    bass            ==> SDR:   3.957  SIR:  12.244  ISR:   1.516  SAR:   1.809
    other           ==> SDR:  -1.505  SIR:  -3.357  ISR:  13.263  SAR:   3.029

    time: 12m55.510s
\end{minted}
  \caption{BSS results of various configurations of Wiener-EM post-processing}
  \label{lst:wienerem}
\end{listing}

To briefly discuss these results, the zero-padded matrix form of the sliCQ-Wiener-EM was significantly faster than looping over the ragged list of matrices, with a slight difference in BSS metrics from the ragged form. The zero-padded sliCQ-Wiener-EM is the chosen implementation in the final model, with an option to also use the STFT-Wiener-EM. Both types of Wiener-EM with the sliCQ beat the STFT. Also, in both cases, 2 iterations were considerably worse than 1 iteration. The computation time of the inference step will be discussed in more detail in the next section \ref{sec:inferenceperf} to discuss the tradeoff between the STFT and sliCQ Wiener-EM.

To maximize result reproducibility, even though table \ref{table:bsseval} describes how to download the pretrained models of UMX, X-UMX, and xumx-sliCQ from their respective hosted locations, all the pretrained models and code were mirrored in a separate public repository to generate the results above.\footnote{\url{https://gitlab.com/sevagh/xumx_slicq_extra/-/tree/main/mss_evaluation}}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{./images-bss/boxplot_full.pdf}
	\caption{Boxplot of UMX, X-UMX, and xumx-sliCQ alongside the oracles}
	\label{fig:bssboxplot}
\end{figure}

\newpagefill

\subsubsection{Model size and inference performance comparison}
\label{sec:inferenceperf}

\end{document}
