\documentclass[report.tex]{subfiles}
\begin{document}

\section{Experiment}

\subsection{sliCQ transform performance analysis}

\subsection{sliCQ transform parameter search}

Finally, a full evaluation was done on all 50 tracks of the MUSDB18-HQ test set. The output is a box plot, in the same style as the SiSec 2018 campaign using the same evaluation code.\footnote{\href{https://github.com/sigsep/sigsep-mus-2018-analysis}{https://github.com/sigsep/sigsep-mus-2018-analysis}} The resulting box plots can be seen in figure \ref{fig:nsgtboxplots}.

\begin{figure}[ht]
	\centering
\makebox[\textwidth]{\includegraphics[width=15cm]{./images-bss/oracle_boxplot.pdf}}
\caption{Music separation results for NSGT oracle masks}
\label{fig:nsgtboxplots}
\end{figure}

Considering again the IRM1 (for simplicity), the median SDR score across all tracks for each of the 4 sources is shown in table \ref{table:nsgtbayesresults2} for the same STFT and NSGT configurations from table \ref{table:nsgtbayesresults} are shown along with the SDR score per target, median across all tracks.

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l|l|l|c|c|c|c|c| }
	 \hline
	  Transform & Vocals SDR & Drums SDR & Bass SDR & Other SDR \\
	 \hline
	 \hline
	 STFT, UMX default & 8.62 & 7.07 & 6.68 & 6.87 \\
	 \hline
	 STFT, wider & 6.16 \\
	 \hline
	 STFT, narrow & 5.37 \\
	 \hline
	 NSGT, CQ-Log & 7.20 \\
	 \hline
	 NSGT, VQ-Log & 7.20 \\
	 \hline
	 NSGT, Mel & 7.44 \\
	 \hline
	 NSGT, Bark & 10.42 & 9.29 & 8.06 & 8.21 \\
	 \hline
\end{tabular}
	\caption{SDR per source, full MUSDB18-HQ test set evaluation}
	\label{table:nsgtbayesresults2}
\end{table}

To sum up, using the best selected Bark-scale NSGT discovered by 200 iterations of Bayesian optimization achieved improvements of \textbf{+1.3-2.2} \todo{adjust for fuller testbench} points in median SDR score over the STFT. However, recall that this is simply the maximum possible performance of spectral masking with the NSGT, assuming that the system can produce a perfect estimate of the sources.

The next step is to adapt UMX to use the Bark-scale NSGT instead of the STFT, and check how much of the theoretical SDR improvements occur in the actual separation model, trained only on MUSDB18-HQ. The goal is to try to surpass Conv-Tasnet and Demucs with the UMX-NSGT variant.

\subsubsection{Comparing different sliCQ and STFT configurations}

\ichfeedback{I don't love the title here but what i mean is i want to pick, as we discussed, multiple STFT window sizes e.g. 512 1024 2048 4096 8192, and multiple sliCQ configurations e.g. Bark, Mel, 33Hz, 57Hz, 200 bins, 500 bins, 12 bins and contrast them all side by side - visual spectrograms, describe the time-frequency resolution, etc.}

\ichfeedback{if you pick a bad scoring config, e.g. Constant-Q Log scale with 14 frequency bins and frequency limits 12.3 Hz -- 22050 Hz, you can get an oracle score of -7 dB SDR (completely terrible performance - i've seen it happen), and if you pick a good one, you get +8.9 dB SDR (surpassing the STFT)}

\ichfeedback{it would be good to plot the magnitude and phase of the good and bad slicq. most likely the ``bad'' one is losing phase information when we ignore its phase. something must be controlling why a few hz in either direction is causing such a dramatic SDR difference}

\subsection{Neural network training}

The training parameters of xumx-sliCQ were kept similar to Open-Unmix and CrossNet-Open-Unmix:

The training curves can be seen in figure \ref{fig:networkloss}. These were taken from Tensorboard,\footnote{\url{https://www.tensorflow.org/tensorboard}} which is a visual web component of the Tensorflow deep learning framework (\cite{tensorflow, tensorflowsoft}) that is also compatible with PyTorch. It was used to visualize the loss curves of xumx-sliCQ during training.

\begin{figure}[ht]
	\centering
	\subfloat[Train loss]{\includegraphics[width=\textwidth]{./images-neural/train_loss.png}}
	\hspace{0.5em}
	\subfloat[Validation loss]{\includegraphics[width=\textwidth]{./images-neural/valid_loss.png}}
	\caption{Tensorboard loss curves for xumx-sliCQ, 1000 epochs}
	\label{fig:networkloss}
\end{figure}

\subsection{Demixing results}

The BSSv4 scores for the demixing results, computed on the MUSDB18-HQ (\cite{musdb18hq}) dataset's test split, are shown in figure \ref{fig:bssboxplot}. Open-Unmix is the reference model (\cite{umx}) and is labelled ``umx'' in the boxplot. CrossNet-Open-Unmix 

\begin{figure}[ht]
	\centering
	\includegraphics[height=5cm]{./images-bss/boxplot_full.png}
	\caption{Boxplot of full MUSDB18-HQ test set evaluation of Open-Unmix vs. CrossNet-Open-Unmix vs. xumx-sliCQ}
	\label{fig:bssboxplot}
\end{figure}

\subsection{Inference performance analysis}

\subsection{ISMIR 2021 Music Demixing Challenge}

\ichfeedback{i know this shouldn't be a core part of the thesis, but it's still fun to describe?}

\end{document}
