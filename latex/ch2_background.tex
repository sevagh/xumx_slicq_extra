\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{sec:background}

\subsection{Acoustic signals and the time-domain waveform}

\textcite[Chapter~2]{discretebook} define a signal as a term which

\begin{quote}
	conveys information about the state or behavior of a physical system, and often, signals are synthesized for the purpose of communicating information between humans or between humans and machines... Signals are represented mathematically as functions of one or more independent variables. For example, a speech signal is represented mathematically as a function of time, and a photographic image is represented as a brightness function of two spatial variables.
\end{quote}

As audio or sound waves are the type of signal studied in this thesis, \textcite[Chapter~2]{moore} describes the phenomenon of sound waves as follows:

\begin{quote}
	Sound originates from the vibration of an object. This vibration is impressed upon the surrounding medium (usually air) as a pattern of changes in pressure
\end{quote}

The waveform is defined as the function of pressure variation plotted against time (\cite{moore, melbook}), and the waveform is real-valued and continuous in both time and amplitude (\cite[Chapter~2]{melbook}). A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers (\cite[Chapter~2]{discretebook}). The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$.  A continuous-time signal and its discrete representation are both shown in figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform]{\includegraphics[height=2.25cm]{./images-tftheory/continuoustime.png}}
	\hspace{0.1em}
	\subfloat[Discrete-time waveform]{\includegraphics[height=2.25cm]{./images-tftheory/discretetime.png}}
	\caption{A continous-time signal and its discrete-time representation sampled with $T = 125\mu s$ (\cite[Chapter~2]{discretebook})}
	\label{fig:discretecontinuous}
\end{figure}

Continuous-time signals are continuous in both time and amplitude. They are captured into discrete-time representations by a process called sampling (\cite[Chapter~2]{melbook}) -- more specifically, the discretization of time is called sampling, and the discretization of amplitude is called quantization. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x(t)$ is defined by the sampling period $T$:
\[ x[n] = x_{a}(nT) \]

Two important variables describe the nature of the discrete representation: the sampling rate $F_{s} = \frac{1}{T}$, and the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The sampling rate, $F_{s}$, relates to the Nyquist-Shannon sampling theorem (\cite[Chapter~4]{discretebook}), which was described by both \textcite{nyquist1928} and \textcite{shannon1948}. It states that the maximum frequency that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \frac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency. If the signal contains frequencies above the Nyquist rate ($F \ge F_{\text{nyq}}$), it cannot be accurately represented due to the phenomenon of aliasing due to undersampling (\cite[Chapter~4]{discretebook}). An illustration is shown in figure \ref{fig:aliasing}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{./images-tftheory/aliasing.png}
	\caption{Examples of under, uniform, and oversampling. Image generated with \textcite{aliasmatlab}'s MATLAB package}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \textcite[Chapter~4]{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values.'' The second variable which relates to the quantization process is the number of quantization levels. The quantization operation is represented as
\[ \hat{x}[n] = Q(x[n]) \]

, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation (\cite{discretebook}). An A/D or ADC (analog-to-digital converter) circuit and its operation on a waveform is shown in figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC)]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\subfloat[Waveform results of the ADC process]{\includegraphics[width=0.75\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations (\cite[Chapter~4]{discretebook})}
	\label{fig:adccircuit}
\end{figure}

\vfill
\clearpage

\subsection{Transforms of acoustic signals}

\textcite{ltfat}'s Large Time-Frequency Analysis toolbox (LTFAT) is ``a Matlab/Octave toolbox for working with time-frequency analysis and synthesis.''\footnote{\url{http://ltfat.org/}} It contains a test signal from the glockenspiel instrument, loaded by the \Verb#gspi# function.\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} In fact this signal is well-known and used in audio signal processing papers on the topic of time-frequency (\cite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt}). This is due to the pitched percussive nature of the glockenspiel's timbre, such that it contains both the tonal and transient properties of music, which have conflicting needs in their time and frequency resolution\todo{citeme}.

Figure \ref{fig:glockwaveform} shows the time-domain, discrete-time waveform of the glockenspiel signal plotted using MATLAB and the LTFAT. For the rest of this chapter, demonstrations of each transform will use this same glockenspiel as the input signal.

\begin{figure}[ht]
	\centering
	\includegraphics[height=6.5cm]{./images-gspi/gspi_time_domain.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

\subsubsection{Frequency analysis and the Fourier transform}

The \textit{spectrum} of a signal is a compact representation of the frequency components of a signal that is composed of sinusoids, which are the basic building block of creating complicated signals (\cite[Chapter~3]{dspfirst}).

Its most common implementation choice \todo{citeme}, the Fast Fourier Transform (FFT), is considered one of the most important algorithms in modern computing \todo{citeme}.

\subsubsection{Joint time-frequency analysis -- the Gabor transform and the Short-time Fourier Transform (STFT)}

\textcite{gabor1946}'s seminal signal processing paper, \textit{The Theory of Communication}, contained the first suggestion of joint time-frequency decomposition of a signal by applying the Fourier transform locally to overlapping portions of the signal multiplied by Gaussian windows. In other words, Gabor proposed that any signal of finite energy can be decomposed into a linear combination of time-frequency shifts of the Gaussian function. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described in equation (1):
\begin{flalign}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)]\\
	G_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n},
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform. With his transform, Gabor also introduced the first formulation of the time-frequency uncertainty principle (which is minimized by using the Gaussian function as a window), stating that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit.'' Gabor called named the time-frequency tile the \textit{logon}, or smallest possible unit of time-frequency information. Mathematically, this can be stated as:
\[ \Delta t\Delta f \ge 1 \]

$\Delta t$ and $\Delta f$ are, as defined by Gabor, ``the uncertainties inherent in the definition of the epoch $t$ and frequency $f$ of an oscillation.'' The TF uncertainty principle arises from the fact that time and frequency are, in quantum physics terms, conjugate variables, or Fourier transforms of each other. This is further illustrated in figure \ref{fig:gabortf}, which shows the tiling of the time-frequency plane, and how frequency and time resolution must be sacrificed for one another by the lower bound of the time-frequency tile area.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[height=3cm]{./images-tftheory/gabor3.png}}
	\hspace{0.1em}
	\subfloat{\includegraphics[height=2.56cm]{./images-tftheory/gabor4.png}}
	\caption{A demonstration of the mutually exclusive formulations of time analysis and frequency analysis, and the lower bound of time-frequency resolution defined by Gabor's TF uncertainty principle (\cite{gabordiagrams})}
	\label{fig:gabortf}
\end{figure}

Gabor noted the need for variable-frequency analysis, stating that ``the foregoing solutions [of the Fourier transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren.'' Similarly, psychoacoustics research shows that humans have been able to beat the time-frequency uncertainty principle (\cite{psycho1, psycho2}), indicating the presence of nonlinear operators in the auditory system.

The STFT, or short-time Fourier transform, has been described independently from Gabor's work (\cite{stftindie}), but additional research in the 1980s (\cite{dictionary}) led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described in equation (2):
\begin{flalign}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)]\\
	X_{m}(f) &= \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n},
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window. Note how similar equations (1) and (2) are, which is expected since the original Gabor transform is the STFT with a Gaussian window. Practically, the STFT allows the use of different windows and overlap sizes (\cite{stftinvertible}), as long as overlap-add conditions are respected.\footnote{\url{https://www.mathworks.com/help/signal/ref/iscola.html}}

\subsubsection{Constant-Q Transform (CQT)}

Summary of the most relevant CQT implementations:

\begin{enumerate}
	\item
	    Brown, 1991, first proposed CQT with a naive very slow implementation.
    \item
	    Brown and Puckette, 1992: implemented a faster CQT based on a sparse representation in the frequency domain. This is the current Essentia implementation!
    \item
	    Sch{\"o}rkhuber and Klapuri, 2010: a faster CQT based on the same principles as Brown and Puckette, 1992. For first time, it is introduced an algorithm for an approximated reconstruction of the CQT coefficients. Code: \url{http://www.iem.at/~schoerkhuber/cqt2010/} - this is the librosa implementation
    \item
	    Velasco, Holighaus, D{\"o}rfler and Grill, 2011: they approach the problem differently - by means of a nonstationary Gabor transform. This allows perfect reconstruction for first time while the transform is still computationally efficient (faster than Sch{\"o}rkhuber and Klapuri, 2010). However, it does not allow real-time implementations and phases are not accurate. Code: \url{http://www.univie.ac.at/nonstatgab/toolbox.php}
    \item
	    Holighaus, D{\"o}rfler, Velasco and Grill, 2012: Based on the Velasco, Holighaus, Dörfler and Grill, 2011 - allowing perfect reconstruction. They propose sliCQT (slicing by using an overlapping window) to allow real-time computations. Code: \url{http://www.univie.ac.at/nonstatgab/toolbox.php}
    \item
	    Sch{\"o}rkhuber, Klapuri, Holighaus and Dörfler, 2014: Based on the Velasco, Holighaus, Dörfler and Grill, 2011 - allowing perfect reconstruction. They solve the problem with phases by means of a frequency mapping and they also propose a Variable-Q transform (that allows ie. ERBlets). Code: \url{http://www.cs.tut.fi/sgn/arg/CQT/}
    \item
	    Sch{\"o}rkhuber, Klapuri, Holighaus and D{\"o}rfler, 2014, a nonstationary Gabor transform, allows perfect reconstruction while the phases are still accurate. It might be interesting to implement this in Essentia.
\end{enumerate}

\todo[inline]{the original judith brown CQT is the origin of the research that led to the NSGT}

The Constant-Q transform (CQT) is time-frequency transform for musical signals, originally designed by \textcite{jbrown}, the relationship between the fundamental frequency and its harmonics on a logarithmic frequency scale more clearly than the linear frequency scale of the traditional discrete Fourier transform (DFT).

The original CQT had no inverse transform, but later works led to approximate inverses \cite{klapuricqt, fitzgeraldcqt}. 
, which has an important application in the perfectly-invertible CQT, or CQ-NSGT \cite{invertiblecqt}

The more general NSGT should be studied instead the CQT for the following reasons:
\begin{itemize}
	\item
		It solves the earlier CQT's \cite{jbrown, klapuricqt, fitzgeraldcqt} lack of stable inverse, which was a known weakness \cite{lackinverse}
	\item
		It can use other potentially interesting frequency scales besides the constant-Q logarithmic scale, such as the psychoacoustically-motivated mel, Bark, or ERB scales \todo{cite me}, or variable-Q scales \todo{cite gamma and other}
\end{itemize}

 The CQT has a high temporal resolution at high frequencies \cite{cqtransient} .

 A demonstration of the CQT is shown in figure \ref{fig:earlycqt}.

\begin{figure}[ht]
	\centering
	\subfloat[Linear frequency spectrum]{\includegraphics[height=4.75cm]{./images-tftheory/violindft.png}}
	\subfloat[Constant-Q transform]{\includegraphics[height=4.75cm]{./images-tftheory/violincqt.png}}
	\caption{Violin playing the diatonic scale, $G_{3} \text{(196Hz)} - G_{5} \text{(784Hz)}$}
	\label{fig:earlycqt}
\end{figure}

Additionally, the constant-Q transform \cite{jbrown, klapuricqt, invertiblecqt} even before its formulation as a specialized variant of the nonstationary Gabor transform \cite{balazs}, is an STFT applied with window of different sizes, which are of long duration at low frequencies to create a fine frequency resolution (and sacrificing time resolution as per the time-frequency uncertainty principle), and gradually decrease the windows in duration to improve the time resolution (and sacrifice frequency resolution). At the same time, consider that the iterative harmonic-percussive source separation algorithms in \cite{driedger, fitzgerald2} use two-pass spectral masking with two different configurations of spectrograms -- one with a large window size (4096 samples in \cite{driedger}, 16384 samples in \cite{fitzgerald2}) for representing the harmonic or pitched instruments sharply and estimating the harmonic mask, and one with a short window size (256 samples in \cite{driedger}, 1024 samples in \cite{fitzgerald2}) for representing percussion or transients more sharply and estimating the percussive mask.

The connection to the CQT, or NSGT, is that these contain within a single transform the high frequency resolution of a the large-size spectrogram in the low frequency regions, and the high time resolution of the small-size spectrogram in the high frequency regions. According to \textcite{musicsepgood}'s survey on music source separation, most spectral masking techniques try to exploit the 

\begin{figure}[ht]
	\centering
	\includegraphics[width=9cm]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal}
	\label{fig:dorflertradeoff}
\end{figure}

The time-frequency tradeoff is demonstrated on a musical glockenspiel signal in figure \ref{fig:dorflertradeoff}. Notice how the wide window spectrogram shows frequency components (horizontal lines) with a sharper definition than the blurry lines in the narrow window spectrogram, while the narrow window spectrogram shows temporal events (vertical lines) with a sharper definition than the wide window spectrogram.

\subsubsection{Nonstationary Gabor Transform (NSGT) and the sliCQ transform}
\label{sec:theorynsgt}

\todo[inline]{brief intro to frame theory and math stuff}

\todo[inline]{irregular time and frequency sampling, show grids, varying time-frequency resolution etc.}

\todo[inline]{arbitrary f scales and time scales}

\todo[inline]{slicq is the realtime variant}

\todo[inline]{whats the output, what does it mean, how does it relate to the FFT coefficients, time-frequency matrix}

\vfill
\clearpage

\subsection{Nonlinear frequency scales for music analysis}
\label{sec:freqscales}

\ichfeedback{better title? this is a bit long. ``Frequency scales that may be useful for music or psychoacoustic purposes'' seems like a mouthful - ``Frequency scales for music analysis``?}

\subsubsection{Scales based on Western pitch}

constant-q, log, western pitch scale, octave

variable-q - same with gamma offset

from \cite{variableq1, variableq2}, same as cq-log but with a gamma parameter

\subsubsection{Psychoacoustic scales}

mel, bark

\vfill
\clearpage

\subsection{Machine learning}
\label{sec:ml}

\ichfeedback{if i'm presenting a neural network, it's probably necessary to have this section?}

\subsubsection{Deep learning}
\label{sec:dl}

\vfill
\clearpage

\subsection{Music source separation}
\label{sec:musicsep}

\textcite{musicsepgood} describe the motivation and task of music source separation as follows:

\begin{quote}
	Many people listen to recorded music as part of their everyday lives [...] Sometimes we might want to remix the balance within the music, perhaps to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1- channel surround sound system. We might also want to change the spatial location of a musical instrument within the mix. All of these applications are relatively straightforward, provided we have access to separate sound channels (stems) for each musical audio object. However, if we only have access to the final recording mix, which is usually the case, this is much more challenging. To estimate the original musical sources, which would allow us to remix, suppress or upmix the sources, we need to perform musical source separation (MSS). In the general source separation problem, we are given one or more mixture signals that contain different mixtures of some original source signals. [...] The task is to recover one or more of the source signals given the mixtures.
\end{quote}

\textcite{umx}'s deep learning model, named Open-Unmix, is intended to be a near state-of-the-art, open implementation of music source separation based on the MUSDB18 dataset \cite{musdb18} and intended to foster reproducible research and a baseline of high performance. It uses the STFT spectrogram as the input and output representation of musical signals, and a deep neural network to estimate invidual sources from a mixture. The choice of window size in the task of source separation based on STFT spectrograms can have an impact on the results of speech and music source separation \cite{musicsepwindow}, but this consideration is not explored in Open-Unmix.

The CQT was shown to surpass the performance of the STFT in speech separation \cite{cqtseparation}. A simpler and older algorithm for harmonic/percussive source separation (HPSS) \cite{fitzgerald1}, also based on the STFT spectrogram, was evaluated alongside Open-Unmix in the SiSec 2018 music source separation evaluation campaign \cite{sisec2018} as a primitive (and low performance) baseline. The HPSS algorithm was later modified to use STFT spectrograms with different window sizes \cite{driedger, fitzgerald2}, and to use the CQT in place of the STFT \cite{fitzgerald2}. Using the STFT with a warped, or non-linear frequency scale, was also shown to improve music source separation results \cite{bettermusicsep}.

As the NSGT is a generalization of the CQT, and can be used to implement different frequency scales, it is chosen as the transform to study in this thesis. The objectives of this thesis are to discover which configurations of the NSGT can surpass different window sizes of the STFT in music source separation, and to adapt Open-Unmix to use the NSGT.


\subsubsection{Task motivation and definition}

\todo[inline]{purposes and uses - why do we want to do this}

The task of music source separation is to split a mixed song into its constituent components, or sources. \textcite{musicsepgood} describe that music source separation could operate on the level of instruments, or for broader categories of sources, grouped into harmonic, percussive, and singing voice.

\subsubsection{Public datasets}

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset (\cite{musdb18}), and more recently the HQ (high-quality) version (\cite{musdb18hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets (\cite{otherdataset1, otherdataset2}).

The songs in the MUSDB18-HQ dataset have a fixed train, validation, and test split. Following the rules defined in the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} for a network to be considered trained only on MUSDB18-HQ, the predefined data splits must be used.

\subsubsection{Evaluation measures}

The SigSep\footnote{\url{https://sigsep.github.io/}} community, borrowing from the methodology of Signal Separation Evaluation Campaign (SISEC), uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality. There are 4 distinct metrics that comprise BSS:

\begin{itemize}
\item
	\textbf{ISR:} source Image to Spatial distortion Ratio
\item
	\textbf{SIR:} Signal to Interference Ratio
\item
	\textbf{SAR:} Signal to Artifacts Ratio
\item
	\textbf{SDR:} Signal to Distortion Ratio
\end{itemize}

Out of these 4 scores, SDR is the single global score which is commonly used to summarize the overall performance of a music demixing system (\cite{sdruseful}). The SDR as it was defined in the ISMIR 2021 Music Demixing Challenge (and used to rank the participants) can be computed from the following equation:

\[ \]

In the SigSep community and in the most recent SiSec evaluation (\cite{sisec2018}), the BSS evaluation measure used is BSS v4, a variant of BSS available in their Python libraries museval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} and bsseval.\footnote{\url{https://github.com/sigsep/bsseval}} The differences between BSS as used in SiSec 2016 (\cite{sisec2016}) and BSS v4 are outlined in the bsseval project's GitHub README file:

\begin{quote}
	One particularity of BSSEval is to compute the metrics after optimally matching the estimates to the true sources through linear distortion filters. This allows the criteria to be robust to some linear mismatches... this matching is the reason for most of the computation cost of BSSEval...

	For this package, we enabled the option of having time invariant distortion filters, instead of necessarily taking them as varying over time as done in the previous versions of BSSEval. First, enabling this option significantly reduces the computational cost for evaluation because matching needs to be done only once for the whole signal. Second, it introduces much more dynamics in the evaluation, because time-varying matching filters turn out to over-estimate performance. Third, this makes matching more robust, because true sources are not silent throughout the whole recording, while they often were for short windows
\end{quote}

\subsubsection{Survey of computational approaches}

\ichfeedback{spectral masking, NMF, machine learning, deep learning - i can lean on the machine learning introduction section right before}

\todo[inline]{summary of approaches over the year e.g. nonnegative matrix factorization to machine learning to deep learning}

\subsubsection{Time-frequency masking and oracle estimators}

\ichfeedback{i think the idea of the oracle mask computed from ground truths is important enough to be in the section title}

\ichfeedback{it will come up later in the thesis when choosing hyperparameters for the sliCQ}

\textcite{musicsepgood}'s survey on music source separation describes how many contemporary methods of separation are based on the common technique of spectrogram masking \cite{masking, speechmask, musicmask}, and exploit that different sources have different characteristic representations in the spectrogram -- see figure \ref{fig:sepgood} for an illustration. Optimally sparse representation of music signals in the transform domain is an active area of research \cite{sparsitykowalski, sparsitykowalski2}.

\begin{figure}[ht]
	\centering
	\subfloat[hello world]{\includegraphics[height=4.5cm]{./images-mss/mss1.png}}
	\subfloat[hello world]{\includegraphics[height=4.7cm]{./images-mss/mss2.png}}
	\caption{MSS sparsity}
	\label{fig:sepgood}
\end{figure}

Harmonic/percussive source separation is the quintessential example.

\begin{enumerate}
	\item
		Apply the algorithm in two passes, using a wide window STFT to attain a high frequency resolution and perform a better harmonic estimate, and a narrow window STFT to attain a high time resolution and perform a better percussive estimate \cite{fitzgerald2, driedger}
	\item
		Replace the STFT with the CQT to perform a vocal or singing voice separation \cite{fitzgerald2}
\end{enumerate}

The harmonic-percussive source separation algorithm of \textcite{fitzgerald1} is based on median filtering and spectral masking using magnitude spectrograms. The original algorithm with soft masks, and the binary masking and iterative variants introduced in \cite{driedger}, are both implemented in librosa \cite{librosa}, a popular open-source MIR library -- see figure \ref{fig:hpsslibrosa} for a diagram of how the algorithm works. The algorithm exploits transform sparsity, by noticing that harmonic or pitched sounds as horizontal lines in the spectrogram, percussive sounds as vertical lines, and estimating each using median filtering. \textcite{driedger}'s iterative variant uses two passes, with a large-window STFT to generate a spectrogram with a higher frequency resolution in the first pass for an improved harmonic estimate, and a small-window STFT to generate a spectrogram with a higher time resolution in the second pass for an improved percussive estimate.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/sphx_glr_plot_hprss_001.png}
	\caption{Harmonic/percussive/residual source separation in librosa}
	\label{fig:hpsslibrosa}
\end{figure}


These algorithms sacrifice separation quality for simplicity, scoring rather low in recent objective evaluations\cite{sisec2018}, losing in objective metrics and subjective evaluations to solutions based on deep learning\cite{umx, demucs}.
Surveys on speech \cite{speechmask} and music separation \cite{musicmask} indicate that the majority of separation algorithms use the technique of time-frequency masking (or spectral masking) to separate the sources.

%\begin{wrapfigure}{r}{8cm}
\begin{figure}[ht]
	\vspace{-1.0em}
	\includegraphics[width=8cm]{./images-mss/maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising. The oracle mask is the ideal mask for a given signal -- to compute it, the target and interference signals must be known.}
	\label{fig:masks}
	\vspace{-1.5em}
%\end{wrapfigure}
\end{figure}

\textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins. A soft mask has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only 0 and 1. The soft mask used in \cite{fitzgerald1, fitzgerald2} is a Wiener filter given in the following equation, where $\hat{S}$ represents the complex-valued spectrogram:
\[ M_{\text{target}} = \frac{|\hat{S}_{\text{target}}|^{2}}{|\hat{S}_{\text{interference}}|^{2} + |\hat{S}_{\text{target}}|^{2}} \]

Soft masks generally produce higher quality sound. An illustration of spectral masking is shown in figure \ref{fig:masks}.

Most recently, the SigSep\footnote{\url{https://sigsep.github.io/}} community has been running the Signal Separation Evaluation Campaign (SISEC), which sets the tone for the modern state-of-the-art models. SiSec uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality, or BSSv4 variant.

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset \cite{musdb18} (or the HQ, high-quality, equivalent \cite{musdb18-hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research.

In modern music source separation, the stems of MUSDB18 have determined the four most common sources to separate -- drums, bass, vocals, and other. SigSep's own network, Open-Unmix, is trained only on MUSDB18, and produces near-state-of-the-art results. The absolute state-of-the-art crown is jointly held by Conv-Tasnet and Demucs, both of which surpass Open-Unmix. Both models operate directly on the waveform domain, which indicates that they could surpass the maximum possible poerformance of time-frequency masking approaches, as is done in speech separation. However, in practise they are still below the limits of masking-based approaches.

\subsubsection{Open-Unmix (UMX)}

 \textcite{umx}'s deep learning model for music source separation is intended to be a near state-of-the-art, open implementation based on the open MUSDB18 and MUSDB18-HQ datasets and designed to foster source separation research \cite{musdb18, musdb18hq}. A deep neural network is used to estimate the magnitude spectrograms of the sources given a mixed song as an input. The sources are the same as the four stems per track in MUSDB18: drums, vocals, bass, other. Finally, the estimate is used to compute a soft mask.

\end{document}
