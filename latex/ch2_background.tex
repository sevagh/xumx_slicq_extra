\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{sec:background}

\subsection{Acoustic signals and the time-domain waveform}
\label{sec:timedomain}

\textcite[Chapter~2]{discretebook} define a signal as a function of one or more variables which ``conveys information about the state or behavior of a physical system.'' Some examples of signals are a 2-dimensional image, which is a function of pixel brightness of two spatial variables, and a speech signal, which is a function of time. \textcite[Chapter~2]{moore} describes the more specific case of sound, or audio signals, as being the description of the vibration of an object and how it impresses this vibration upon the ``surrounding medium (usually air) as a pattern of changes in pressure.''

The waveform is defined as the function of pressure variation plotted against time (\cite{moore, melbook}), and is real-valued and continuous in both time and amplitude (\cite[Chapter~2]{melbook}). A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers (\cite[Chapter~2]{discretebook}). The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$.  A continuous-time signal and its discrete representation are shown in figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/continuoustime.png}}\\
	\subfloat[Discrete-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/discretetime.png}}
	\caption{A continous-time signal and its discrete-time representation sampled with $T = 125\mu s$ (\cite[Chapter~2]{discretebook})}
	\label{fig:discretecontinuous}
\end{figure}

Continuous-time signals are converted into discrete-time representations by two processes: sampling and quantization. Periodic points on the continuous time axis of the waveform have their values taken, or are \textit{sampled}. The amplitude values of the waveform at these sampled points is \textit{quantized} to find the closest digital number (\cite[Chapter~2]{melbook}). Time sampling is controlled by the sampling period $T \text{ seconds}$, or the sampling rate $F_{s} = \sfrac{1}{T} \text{ Hz}$, which define the periodicity of the sampling. Amplitude quantization is controlled by the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x_{a}(t)$ is defined by $x[n] = x_{a}(nT)$, where $n$ is an integer and $T = \sfrac{1}{F_{s}}$ is the sampling period. The Nyquist-Shannon sampling theorem (\cite[Chapter~4]{discretebook}), described independently by \textcite{nyquist1928} and \textcite{shannon1948}, states that the maximum frequency of a signal that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \sfrac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency.

Aliasing is one of the pitfalls of choosing an inappropriate sampling rate for the frequencies present in the signal under observation (\cite[Chapter~4]{dspfirst}). The diagram in figure \ref{fig:aliasing} shows the phenomenon of aliasing due to undersampling, which occurs when a time-domain signal is undersampled, i.e. $F_{\text{sig}} > F_{\text{nyq}}$, and the continuous-time signal cannot be reconstructed accurately. We refer the readers to the full chapter in the textbook for a more full description of challenges in signal sampling.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-tftheory/aliasing_undersampling.png}
	\caption{An undersampled cosine wave (red) and the resulting incorrect reconstruction (black) from \textcite{dspfirst}}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \textcite[Chapter~4]{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values.'' The second variable which relates to the quantization process is the number of quantization levels. The quantization operation is represented as $ \hat{x}[n] = Q(x[n])$, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation (\cite{discretebook}). An A/D or ADC (analog-to-digital converter) circuit and its operation on a waveform is shown in figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC)]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\vspace{0.1em}
	\subfloat[Waveform results of the ADC process]{\includegraphics[width=0.85\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations (\cite[Chapter~4]{discretebook})}
	\label{fig:adccircuit}
\end{figure}

\subsection{Transforms of acoustic signals}
\label{sec:freqdomain}

\textcite[Chapter~2B]{moore} states that

\begin{quote}
	[a]lthough all sounds can be specified by their variation in pressure with time, it is often more convenient, and more meaningful, to specify them in a different way when the sounds are complex. This method is based on a theorem by Fourier, who proved that almost any complex waveform can be analyzed, or broken down, into a series of sinusoids with specific frequencies, amplitudes, and phases. This is done using a mathematical procedure called the Fourier Transform.
\end{quote}

Throughout this section, the description of the Fourier Transform of acoustic signals and the evolution of further transforms that build on it will be covered in detail.

\textcite{ltfat}'s Large Time-Frequency Analysis toolbox (LTFAT) is ``a Matlab/Octave toolbox for working with time-frequency analysis and synthesis.''\footnote{\url{http://ltfat.org/}} It contains a test signal from the glockenspiel instrument, loaded by the \Verb#gspi# function.\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} This signal can be seen in audio signal processing papers on the topic of time-frequency (\cite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt, wmdct}). This is because the glockenspiel contains both tonal and transient properties, which have conflicting needs for time and frequency resolution in their analysis. Figure \ref{fig:glockwaveform} shows the discrete-time waveform of the glockenspiel signal. For the rest of this chapter, demonstrations of each transform will use this same glockenspiel signal as the input $x[n]$. The signal has a total duration of 5.94 seconds, and is sampled with a rate of 44100 Hz.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.825\textwidth]{./images-gspi/gspi_time_domain.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

\subsubsection{Frequency analysis and Fourier Transforms (CTFT, DFT, FFT)}
\label{sec:freqanal}

The Fourier Transform originated as an integral transform in mathematics, which are a class of ``useful tools for solving problems involving certain types of partial differential equations (PDEs), mainly when their solutions on the corresponding domains of definition are difficult to deal with'' (\cite{fourierhistory}). The Fourier Transform was originally introduced by Joseph Fourier in his earlier papers (\cite{fourierhist1, fourierhist2}), and fully expanded and collected in his seminal work on heat (\cite{fourierheat}). The connection of the Fourier Transform to music is described by \textcite{fouriermusic}, who state that

\begin{quote}
	[b]eyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contribution represents some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string.
\end{quote}

The continuous-time Fourier Transform (CTFT) of a time-domain acoustic waveform is defined by the pair of equations \ref{equation:ctft} and \ref{equation:ictft} (\cite[Chapter~11]{dspfirst}):
\begin{align}
	X(j\omega) = \int_{-\infty}^{\infty}{x(t)e^{-j\omega t}\mathit{dt}} \tag{1}\label{equation:ctft} \\
	x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{X(j\omega)e^{j\omega t}\mathit{d\omega}} \tag{2}\label{equation:ictft}
\end{align}

\textcite{dspfirst} calls $X(j\omega)$ the frequency-domain representation of the signal $x(t)$. Equation \ref{equation:ictft} defines the signal $x(t)$ in terms of a ``sum of infinitely many complex-exponential signals with $X(j\omega)$ controlling the amplitude and phases of these signals.'' The continuous-time Fourier Transform provides a one-to-one mapping of the time domain to the frequency domain.

Signals needs to be transformed from the continuous to the discrete domain via sampling to be processed digitally or computationally, which was covered previously in section \ref{sec:timedomain}. The discrete-time Fourier Transform (DTFT), also called the discrete Fourier Transform (DFT), is derived from a sampled version of the continuous-time Fourier Transform (shown previously in equations \ref{equation:ctft} and \ref{equation:ictft}), and is defined by the pair of equations \ref{equation:dtft} and \ref{equation:idtft} (\cite[Chapter~12]{melbook}):
\begin{align}
	X(e^{j\omega}) = \sum_{n = -\infty}^{\infty}{x[n]e^{-j\omega n}} \tag{3}\label{equation:dtft} \\
	x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}{X(e^{j\omega})e^{j\omega n}\mathit{d\omega}} \tag{4}\label{equation:idtft}
\end{align}

The Fourier Transform is a complex-valued function of $\omega$, which is the variable that represents the angular frequency in radians. The Fourier Transform can be expressed in the rectangular form in equation \ref{equation:rect} or polar form in equation \ref{equation:polar} (\cite[Chapter~2]{discretebook}):
\begin{align}
	X(e^{j\omega}) = X_{\text{real}}(e^{j\omega}) + j X_{\text{imag}}(e^{j\omega}) \tag{5}\label{equation:rect} \\
	X(e^{j\omega}) = |X(e^{j\omega})|e^{j\measuredangle X(e^{j\omega})} \tag{6}\label{equation:polar}
\end{align}

The quantities $|X(e^{j\omega})|$ and $\measuredangle X(e^{j\omega})$ are referred to as the magnitude and phase respectively. The Fourier Transform is also referred to as the spectrum, while its magnitude and phase are the magnitude and phase spectra respectively (\cite{discretebook}).

According to \textcite{skiena}, algorithms in computer science are often described by their time complexity in a hypothetical computer where simple operations take 1 time step. The ``Big-O'' notation, or $O(N)$, provides the upper bound of algorithm running time in relation to number of input elements. In the case of the DFT, the number of input elements is the number of samples in the discrete signal. In \textcite[Chapter~9]{discretebook} it is described that in its original formulation, the algorithmic complexity of the DFT is $O(N^{2})$, i.e. the running time of the algorithm grows proportionally the size of the input signal squared (\cite{skiena}). 

Starting from legendary mathematician Carl Friedrich Gauss in 1805 (\cite{gausshist}), and reaching its most famous formulation published by \textcite{cooleytukey}, a family of efficient algorithms for the computation of the DFT by computing a series of smaller DFTs, known collectively as the Fast Fourier Transform (FFT), reduced this computation time to $O(N \log{N})$. This resulted in the FFT becoming one of the most important algorithms of the 20th century (\cite{ffttopten, fiftyyears}). Refer to figure \ref{fig:bigo} to see the differences in the expected running times of the FFT over the naive implementation.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{./images-misc/bigo.png}
	\caption{Visual comparison of the ``Big-O'', or worst-case running times of algorithms (taken from \url{https://www.bigocheatsheet.com/})}
	\label{fig:bigo}
\end{figure}

The number of points, or samples, of the DFT, determines the frequency resolution, also called $\mathit{df}$ or $\mathit{\Delta f}$, which is the frequency spacing between each point in the resulting spectrum (\cite{discretebook}). The frequency resolution of an $N$-point DFT is $\mathit{df} = \sfrac{F_{s}}{N}$, where $F_{s}$ is the sampling rate of the input signal. An illustration of the magnitude and phase spectra of the DFT are shown in figure \ref{fig:glockdft}, using two different lengths of DFT to show the difference in the low and high frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[2048-point DFT, $\mathit{df} = 21.533 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/gspi_dft.png}}\\
	\subfloat[262144-point DFT, $\mathit{df} = 0.168 Hz$]{\includegraphics[width=0.95\textwidth]{./images-gspi/gspi_dft_bigger.png}}
	\caption{DFT of the Glockenspiel waveform. Note the richer frequency information in the higher frequency resolution transform in (b)}
	\label{fig:glockdft}
\end{figure}

\newpagefill

\subsubsection{Joint time-frequency analysis with the Gabor Transform and Short-time Fourier Transform (STFT)}
\label{sec:jointtfa}

Continuing from the discussion of the DFT in the previous section \ref{sec:freqanal}, \textcite[Chapter~10]{discretebook} state that ``often, in practical applications of sinusoidal signal models, the signal properties (amplitudes, frequencies, and phases) will change with time. For example, nonstationary signal models of this type are required to describe radar, sonar, speech, and data communication signals. A single DFT estimate is not sufficient to describe such signals...''

\textcite{gabor1946}'s seminal signal processing paper, \textit{The Theory of Communication}, introduced significant and far-reaching concepts in the time and frequency analysis of acoustic signals. Gabor quotes famed American telecommunication engineer John Carson (\cite{carsonfamous}) to describe the limitations of the Fourier Transform:

\begin{quote}
	[t]he foregoing solutions [of the Fourier Transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren
\end{quote}

According to \textcite{korpel}, ``Gabor came to the conclusion that the difficulty lay in our mutually exclusive formulations of time analysis and frequency analysis ... he suggested a new method of analyzing signals in which time and frequency play symmetrical parts.''

Gabor derived the principal of time-frequency uncertainty from Heisenberg's uncertainty principle in quantum physics, which states that ``the more precisely the position [of an electron] is determined, the less precisely the momentum is known, and conversely'' (\cite{hallm, heisenberg1927}). Gabor's time-frequency uncertainty principle states that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit'' (\cite{gabor1946}). This is also referred to as the Gabor limit, and $\Delta t$ and $\Delta f$ are defined as ``the uncertainties inherent in the definition of the epoch t and frequency f of an oscillation.'' Gabor referred to the time-frequency tile $\Delta t \Delta f$ as the \textit{logon}, or smallest possible unit of time-frequency information.

Let us start with a description of the time-domain unit impulse signal or sequence (\cite[Chapter~2]{melbook}) in equation \ref{equation:delta}:
\begin{flalign}\label{equation:delta}
\delta[n] = \begin{cases}
	1 \text{\hspace{1em}} n = 0\\
	0 \text{\hspace{1em}} \text{otherwise}
\end{cases}
\end{flalign}

The impulse is a useful signal, as it is the ``simplest [time-domain] sequence because it has only one nonzero value, which occurs at n = 0. The mathematical notation is that of the Kronecker delta function'' (\cite[Chapter~5]{dspfirst}). Note the Kronecker delta function is the discrete equivalent of the Dirac delta function (\cite[Chapter~2]{melbook}). Contrast this with the DFT spectrum of a signal that is only nonzero at the 0 Hz (or DC) frequency component, which is a ``cosine signal with zero frequency'' (\cite[Chapter~3]{dspfirst}).

Figure \ref{fig:gaborfirst}(a) shows the unit impulse contrasted with the DC-component DFT, and these two in fact demonstrate the mutually exclusive formulations of time and frequency. The unit impulse, which has a single nonzero value in the time domain, has an infinite extent in the frequency domain. Conversely, the DC-component DFT has a single nonzero value in the frequency domain, but has an infinite extent in the time domain. Figure \ref{fig:gaborfirst}(b) shows the intuition of the time-frequency tradeoff as a choice of signal length. The periodicity of the sine wave is more apparent over longer periods of time $\Delta t$, but the detail of the individual sample values become lost.

\begin{figure}[ht]
	\centering
	\subfloat[Mutually exclusive formulations of time and frequency by two extremes, the unit impulse (bottom left) and the 0 Hz cosine DFT spectrum (top right)]{\includegraphics[height=5cm]{./images-tftheory/gabor13.png}}
	\hspace{2em}
	\subfloat[The periodicity of the sine wave is more apparent with a longer $\Delta t$, at the cost of temporally localized sample values]{\includegraphics[height=6.25cm]{./images-tftheory/gabor2.png}}
	\caption{Time-frequency tradeoff intuitions (\cite{gabor2})}
	\label{fig:gaborfirst}
\end{figure}

The result of the time-frequency uncertainty principle is a consequence of how the Fourier Transform is used to swap between the mutually exclusive domains of time and frequency. To further elaborate that this is a property of the Fourier Transform and not a fundamental physical limitation, several psychacoustic studies have shown that humans can exhibit better time-frequency resolution than the Gabor limit. \textcite{psycho2} describes one of these experiments:

\begin{quote}
	It is concluded that models based on a place (spectral) analysis should be subject to a limitation of the type $\Delta f \cdot d \ge \text{constant}$, where $\Delta f$ is the frequency difference limen (DL) for a tone pulse of duration d. [...]  It was found that at short durations the product of $\Delta f$ and d was about one order of magnitude smaller than the minimum predicted [...]
\end{quote}

More recently, according to \textcite{psycho1}:

\begin{quote}
	[w]e have conducted the first direct psychoacoustical test of the Fourier uncertainty principle in human hearing, by measuring simultaneous temporal and frequency discrimination. Our data indicate that human subjects often beat the bound prescribed by the uncertainty theorem, by factors in excess of 10.
\end{quote}

Note that these are not refutations of Gabor's ideas, but in fact a confirmation. \textcite{gabor1946} himself stated that ``most sound analysis and processing tools today continue to use models based on spectral theories... [w]e believe it is time to revisit this issue.'' Nevertheless, if we choose to proceed with time-frequency analysis despite the limitations, it is preferable to minimize time-frequency uncertainty, or to set the \textit{logon} ($\Delta t \Delta f$) to its lowest possible value. \textcite{gabor1946} asks:

\begin{quote}
What is the shape of the signal for which the product $\Delta t \Delta f$ actually assumes the smallest possible value? [... it is] the modulation product of a harmonic oscillation of any frequency with a pulse of the form of the probability function
\end{quote}

Gabor performed joint time-frequency analysis by multiplying overlapping, temporally consecutive portions of the input signal with shifted copies of the Gaussian window function (i.e. the probability function), and by taking the Fourier Transform of the windowed segments of the signal. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described by equation \ref{equation:gabort}:
\begin{flalign}\tag{7}\label{equation:gabort}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)], G_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n}
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform.

The STFT, or Short-Time Fourier Transform, has been described independently from Gabor's work (\cite{stftindie}), but additional research in the 1980s (\cite{dictionary}) led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described by equation \ref{equation:stft}:
\begin{flalign}\tag{8}\label{equation:stft}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)], X_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n}
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window. Note how similar equations \ref{equation:gabort} and \ref{equation:stft} are, which is expected since the original Gabor transform is the STFT with a Gaussian window. Practically, the STFT allows the use of different windows and overlap sizes (\cite{stftinvertible}), as long as overlap-add conditions are respected.\footnote{\url{https://www.mathworks.com/help/signal/ref/iscola.html}} The MATLAB documentation for the \Verb#iscola# function contains the diagram in figure \ref{fig:stftdiagram}, which shows how a windowed Fourier Transform (i.e. Gabor transform or STFT) is performed on the input signal.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/stft_diagram.png}
	\caption{Diagram showing the forward and inverse STFT or Gabor transform}
	\label{fig:stftdiagram}
\end{figure}

Figure \ref{fig:gabortf} shows different sizes of \textit{logon} in the time-frequency plane, and how the Gabor transform and the STFT with higher frequency or higher time resolution appear on the time-frequency plane.

\begin{figure}[ht]
	\centering
	\subfloat[Pure time domain, pure frequency domain, and Gabor's time-frequency tiles of $\Delta t \Delta f = 1$]{\includegraphics[height=3.4cm]{./images-tftheory/gabor3.png}}\\
	\subfloat[High frequency resolution vs. high time resolution]{\includegraphics[height=3.2cm]{./images-tftheory/gabor4.png}}
	\caption{Different tiling of the time-frequency plane (\cite{gabordiagrams})}
	\label{fig:gabortf}
\end{figure}

Figure \ref{fig:stfts} shows the Gabor transform alongside the STFT using a popular choice of window, the Hamming window, for three different window sizes: 128 samples, 2048 samples, and 16384 samples, which at the sample rate of the glockenspiel signal (44100 Hz) represent $\sfrac{128}{44100}\cdot 1000 = $ 2.9 ms, 46.44 ms, and 371.52 ms respectively. The Hamming window was chosen because it is the default window in the MATLAB \Verb#spectrogram# function.\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} The Gaussian and Hamming windows are shown together in figure \ref{fig:gaussvshamm}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/gaussianvshamming.png}
	\caption{2048-sample Hamming and Gaussian windows compared}
	\label{fig:gaussvshamm}
\end{figure}

\newpagefill

\begin{figure}[ht]
	\centering
	\subfloat[STFT, 128-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_128.png}}
	\subfloat[Gabor transform, 128-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_128.png}}\\
	\subfloat[STFT, 2048-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_2048.png}}
	\subfloat[Gabor transform, 2048-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_2048.png}}\\
	\subfloat[STFT, 16384-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_16384.png}}
	\subfloat[Gabor transform, 16384-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_16384.png}}
	\caption{Visual comparison of the Hamming-window STFT and the Gaussian-window STFT (i.e. the Gabor transform), using the Glockenspiel signal}
	\label{fig:stfts}
\end{figure}

We close this section with a quote from \textcite{doerflersouls} on the powerful properties of the STFT:

\begin{quote}
       ... the STFT has at least three souls: it is the Fourier Transform of windowed portions of the signal, it is the convolution of the signal with modulated versions of the window and it is the scalar product of the signal with time-shifted modulated versions of the window. These three souls can be exploited in the applications, for the computation of the STFT and for its sampling ...
\end{quote}

\newpagefill

\subsubsection{Constant-Q Transform (CQT)}
\label{sec:cqt}

\textcite{cqtransient} stated that the problem with the STFT is its ``rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies.'' As described in section \ref{sec:motivation}, music should be analyzed with a high frequency resolution in the low-frequency region, and with a high time resolution in the high-frequency region (\cite{doerflerphd, cqtransient}). 

The Constant-Q Transform was proposed by \textcite{jbrown, msp} to analyze musical signals with a logarithmic frequency scale to show the relationship between the fundamental frequency of a musical instrument and its harmonics better than the Fourier Transform's uniform frequency spacing. A demonstration of a violin analyzed with the DFT and CQT was shown in figure \ref{fig:violin}.

The name ``Constant-Q'' refers to the constant ratio of the frequency being analyzed to the frequency resolution of analysis, or $\sfrac{f}{\delta f} = Q$, such that the frequency resolution increases with the center frequency to maintain the $Q$ factor. The transform uses long-duration windows in the low frequency regions and short-duration windows in the high frequency regions, resulting in good time resolution for transients (\cite{cqtransient}). Figure \ref{fig:jbrowncqt} shows some properties of the linearly-spaced DFT spectrum compared to the Constant-Q Transform, and the different window sizes used for each frequency bin of interest. Equation \ref{equation:jbrowncqt} describes how the different windows $N[k]$ are applied to the signal:
\begin{align}\tag{9}\label{equation:jbrowncqt}
	\nonumber N[k] \text{(window length)} &= \frac{f_{s}}{f_{k}}Q, W[k, n] = \alpha + (1 - \alpha)\cos\big(\frac{2\pi n}{N[k]}\big)
\end{align}

\begin{figure}[ht]
	\centering
	\subfloat[Properties of DFT, CQT]{\includegraphics[height=5.05cm]{./images-tftheory/dftvcqt.png}}
	\hspace{0.5em}
	\subfloat[Window sizes for CQT]{\includegraphics[height=5cm]{./images-tftheory/qwindowchanges.png}}
	\caption{Various aspects of \textcite{jbrown}'s CQT}
	\label{fig:jbrowncqt}
\end{figure}

This first CQT implementation was not designed to be invertible, until \textcite{klapuricqt} introduced an algorithm for an approximate inverse of the CQT back to the time-domain waveform, with an inversion error of $\approx 10^{-3}$. The approximate inverse for the CQT was also approached differently by \textcite{fitzgeraldcqt}.

\textcite{klapuricqt} describe the CQT as ``a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal.'' Frequency bins that are geometrically spaced are also logarithmically spaced with a base of two (\cite{geometriclog}). The bins-per-octave setting of the CQT is related to the Constant-Q ratio by the formula $Q = 2^{\sfrac{1}{\text{bins}}}$. For example, for 12 bins-per-octave, this results in a $Q$ factor of 1.059, which is the well-known ``12th root of two'' based on the Western chromatic scale with equal temperament (\cite{westernpitch1, westernpitch2}).

\textcite{invertiblecqt} implemented the Constant-Q Transform with Nonstationary Gabor frames (\cite{balazs}), which allowed for perfect reconstruction for the first time. This is also called the CQ-NSGT, or Constant-Q Nonstationary Gabor Transform. The next section, \ref{sec:theorynsgt}, will cover the NSGT in detail, but for now we shall continue with how the NSGT led to improved implementations of the CQT.

\textcite{slicq} followed up with a realtime algorithm, the \textit{sliCQ} or ``sliced Constant-Q'' transform (sliCQT), which can process the input signal in fixed-size slices, as opposed to operating on the entire input signal like the NSGT. Finally, \textcite{variableq1} introduced the Variable-Q scale and improved the phase of the transform. From \textcite{invertiblecqt}, the total frequency bins of the CQ-NSGT or sliCQT can be derived from the bins-per-octave, and the minimum and maximum frequencies of the desired frequency scale. The formula given is $K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}}) + 1]$, where $K$ is the total bins of the CQT, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. For example, for $B = 12 \text{ bins-per-octave}$, $\xi_{\text{min}} = 83 \text{ Hz}$ and $\xi_{\text{max}} = 22050 \text{ Hz}$, the result is $K \approx 97 \text{ total frequency bins}$.

In the landscape of music analysis libraries, Essentia,\footnote{\url{https://essentia.upf.edu/}} (\cite{essentia}), LTFAT,\footnote{\url{https://ltfat.org/}} (\cite{ltfat}) and the MATLAB Wavelet Toolbox\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} have converged on the CQ-NSGT (\cite{invertiblecqt, slicq, variableq1}). However, librosa\footnote{\url{https://librosa.org/}} (\cite{librosa}) still uses the older implementation with the approximate reconstruction (\cite{klapuricqt}). Finally, \textcite{invertiblecqt} have released an open-source reference Python implementation\footnote{\url{https://github.com/grrrr/nsgt}} of the CQ-NSGT and the sliCQT.

Various plots of STFT and CQT spectrograms are shown in figure \ref{fig:cqtvstft}, generated from the standard STFT spectrogram\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} and the CQ-NSGT implementation of the CQT from the MATLAB Wavelet Toolbox.

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt12.png}}
	\subfloat[STFT spectrogram, window size 256]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft256.png}}\\
	\subfloat[CQT spectrogram, 24 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt24.png}}
	\subfloat[STFT spectrogram, window size 1024]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft1024.png}}\\
	\subfloat[CQT spectrogram, 48 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt48.png}}
	\subfloat[STFT spectrogram, window size 4096]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft4096.png}}
	\caption{Spectrograms of the glockenspiel signal, showing how the CQT displays more musical information than the STFT for all frequency resolutions}
	\label{fig:cqtvstft}
\end{figure}

\newpagefill

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

\textcite{doerflerphd} analyzed music with multiple Gabor dictionaries (i.e. STFTs). Figure \ref{fig:dorflertradeoff} shows the time-frequency tradeoff of the short and long window STFT analyses of the glockenspiel signal for its transient and tonal characteristics, and how two Gabor dictionaries combined gives us the ability to analyze either the transient or tonal aspects with a more appropriate resolution.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal (\cite{doerflerphd})}
	\label{fig:dorflertradeoff}
\end{figure}

As summarized by \textcite{adaptivecqt}, ``[t]he definition of multiple Gabor frames, which is comprehensively treated in [D{\"o}rfler 2002], provides Gabor frames with analysis techniques with multiple resolutions... The nonstationary Gabor frames [...] are a further development; [...] they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas.'' In other words, a single Nonstationary Gabor transform can replace the need for multiple distinct Gabor transforms (or STFTs).

\textcite{dictionary} describe the shift from the term \textit{transform}, e.g., the Gabor transform or STFT, to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began the ``fundamental shift from transforms to dictionaries for [...] signal representation.'' Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task.''

Using multiple transforms, such as using two Gabor transforms (or STFTs) with a small and large window for the transient and tonal properties of music, is also called an \textit{overcomplete} dictionary (\cite{dictionary}), where there is a lot of redundancy in the transform domain.

The advantage of the CQT over such approaches is that it contains these desirable transform properties in one single transform. The NSGT is a time-frequency transform with varying time-frequency resolution, whose motivating application is the CQT (\cite{jaillet, balazs}). It is constructed from frame theory (\cite{frametheory}), which is a mathematical technique for computing ``redundant, stable way[s] of representing a signal'' (\cite{framesintro}). It can use nonuniform time and frequency spacing in its time-frequency analysis of a signal, and it has a perfect inverse operation, such that there is practically no reconstruction error.

The construction of nonstationary Gabor frames relies on three properties of the windows and time-frequency shift parameters used (\cite{balazs}):
\begin{tight_itemize}
	\item
		The signal $f$ of interest is localized at time- (or frequency-) positions $n$ by means of multiplication with a compactly supported (or limited bandwidth, respectively) window function $g_{n}$
	\item
		The Fourier Transform is applied on the localized pieces $f \cdot g_{n}$. The resulting spectra are sampled densely enough in order to perfectly re-construct $f \cdot g_{n}$ from these samples
	\item
		Adjacent windows overlap to avoid loss of information. At the same time, unnecessary overlap is undesirable. We assume that $0 < A \le \sum_{n \in \mathbb{Z}}|g_{n}(t)|^{2} \le B < \infty$, a.e. (almost everywhere), for some positive A and B
\end{tight_itemize}

These requirements lead to invertibility of the frame operator and therefore to perfect reconstruction. \textcite{balazs} continues on to say that

\begin{quote}
	[m]oreover, the frame operator is diagonal and its inversion is straightforward. Further, the canonical dual frame has the same structure as the original one. Because of these pleasant consequences following from the three above-mentioned requirements, the frames satisfying all of them will be called painless nonstationary Gabor frames and we refer to this situation as the painless case
\end{quote}

To derive the NSGT, let us recall the previously-seen definition of the Gabor transform from section \ref{sec:jointtfa}. In the standard Gabor transform, the same window function (also called the Gabor atom or Gabor function) is shifted in time to cover entire signal (\cite{adaptivecqt}), described by equation \ref{equation:stationarygab}:
\begin{align} \tag{10}\label{equation:stationarygab}
g_{m, n}(t) = g(t - na)e^{2\pi i m b t}
\end{align}

As stated by \textcite{adaptivecqt}, ``[w]e will indicate such a frame as \textit{stationary}, since the window used for time-frequency shifts does not change and the time-frequency shifts form a lattice of $a \times b$.'' Figure \ref{fig:uniformtflattice} shows the resulting uniform $a \times b$ tiling of the time-frequency plane.

\begin{figure}[ht]
	\centering
	\includegraphics[width=6.25cm]{./images-tftheory/stationarygabor.png}
	\caption{Uniform time-frequency resolution with the stationary Gabor transform}
	\label{fig:uniformtflattice}
\end{figure}

We can also describe \textcite{doerflerphd}'s multiple Gabor systems with the following set of equations, continuing from the stationary Gabor transform. Given the stationary Gabor atom, where $a$ and $b$ are the time-frequency shift parameters and $f(t)$ is the reconstructed input signal, the Gabor transform is described by the equations in \ref{equation:onegabor}.
\begin{align}
	\nonumber g_{m,n}(t) &= g(t - na)e^{j2\pi m b t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{m,n \in \mathbb{Z}}c_{m,n}g_{m,n}(t) \tag{12}\label{equation:onegabor}
\end{align}

Similarly, we can describe an overcomplete system that uses $R$ distinct STFTs (or Gabor transforms) with different window sizes, where $a_{r}$ and $b_{r}$ are the time-frequency shift parameters for each window size, with the equations in \ref{equation:multigabor}:
\begin{align}
	\nonumber g_{m,n}^{r}(t) &= g(t - na_{r})e^{j2\pi m b_{r} t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{r=0}^{R-1}\sum_{m,n \in \mathbb{Z}}c^{r}_{m,n}g^{r}_{m,n}(t) \tag{13}\label{equation:multigabor}
\end{align}

For a resolution that changes with time, the Nonstationary Gabor atom is chosen from a set of functions $\{g_{n}\}$ and a fixed frequency sampling step $b_{n}$:
\begin{align}\tag{14}\label{equation:irregulartime}
	g_{m,n}(t) &= g_{n}(t)e^{j2\pi m b_{n}t}, m,n \in \mathbb{Z}
\end{align}

To describe equation \ref{equation:irregulartime}, Balazs states:

\begin{quote}
		[...] the functions $\{g_{n}\}$ are well-localized and centered around time-points $a_{n}$. This is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{n}$ for each position $a_{n}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position.
\end{quote}

For a resolution that changes with frequency, the Nonstationary Gabor atom is chosen from a family of functions $\{h_{m}\}$, which are `well-localized band-pass functions with center frequency $b_{n}$'' and a fixed time sampling step $a_{m}$:
\begin{align}\tag{15}\label{equation:irregularfrequency}
	h_{m,n}(t) = h_{m}(t - na_{m}), m,n \in \mathbb{Z}
\end{align}

Figure \ref{fig:nonuniformtflattices} show both the cases of the varying resolution by time and by frequency in terms of sampled points on the time-frequency grid.

\begin{figure}[ht]
	\centering
	\subfloat[Gabor atoms that change with time]{\includegraphics[width=6.25cm]{./images-tftheory/irregulartime.png}}
	\hspace{1em}
	\subfloat[Gabor atoms that change with frequency]{\includegraphics[width=6.25cm]{./images-tftheory/irregularfrequency.png}}
	\caption{Varying time-frequency sampling of the Nonstationary Gabor transform}
	\label{fig:nonuniformtflattices}
\end{figure}

The NSGT coefficients for a signal of length $L$ are given by an FFT of length $M_{n}$ for each window $g_{n}$. For each window, there are $L$ window operations and $O(M_{n} \cdot \log(M_{n}))$ FFT operations. The overall algorithmic complexity of the NSGT is therefore $O(N \cdot (M \log(M)))$ for $N$ windows.

We believe that the general NSGT is more interesting to study than the CQ-NSGT (Constant-Q NSGT), as it can be constructed with nonuniform frequency scales besides the Constant-Q scale, such as the psychoacoustic mel and Bark scales (\cite[Chapter~4]{melbook}), or the Variable-Q scale based on the psychoacoustic ERBlet transform (\cite{variableq1, variableq2}).

Figures \ref{fig:bunchansgts1} and \ref{fig:bunchansgts2} show several different configurations of the NSGT of the glockenspiel signal, to demonstrate the diversity of the transform. The plots are generated from my fork\footnote{\url{https://github.com/sevagh/nsgt}} of the reference Python NSGT library. My modifications to the library will be described further in section \ref{sec:methodology}. A high and low frequency resolution NSGT, using 100 and 500 total frequency bins respectively, is generated for each of the following 4 frequency scales: mel, Bark, Constant-Q, and Variable-Q, using a frequency range of 20--22050 Hz. 20 Hz is the accepted lowest frequency of human hearing (\cite{moore}), and 22050 Hz is the Nyquist frequency (or maximum signal frequency) of the MUSDB18-HQ sample rate 44100 Hz. In practice, the minimum and maximum frequencies can be also customized for the signal under study. For the Variable-Q scale, the fixed frequency offset applied is 15 Hz.  Details of the frequency scales used to generate the NSGT spectrograms are shown in table \ref{table:nsgtfreqsandqs}.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, Constant-Q scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_cqlog_100.png}}\\
	\subfloat[NSGT, Constant-Q scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_cqlog_500.png}}\\
	\subfloat[NSGT, Variable-Q scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_vqlog_100.png}}\\
	\subfloat[NSGT, Variable-Q scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_vqlog_500.png}}
	\caption{Spectrograms of the glockenspiel signal using the NSGT with a low (100 bins) and high (500 bins) frequency resolution and the constant- and Variable-Q scales. The frequency range for each shown scale is 20--22050 Hz, and there is an offset of 15 Hz per band for the Variable-Q scale}
	\label{fig:bunchansgts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, mel scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_mel_100.png}}\\
	\subfloat[NSGT, mel scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_mel_500.png}}\\
	\subfloat[NSGT, Bark scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_bark_100.png}}\\
	\subfloat[NSGT, Bark scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_bark_500.png}}\\
	\caption{Spectrograms of the glockenspiel signal using the NSGT with a low (100 bins) and high (500 bins) frequency resolution and the mel and Bark psychoacoustic scales. The frequency range for each shown scale is 20--22050 Hz}
	\label{fig:bunchansgts2}
\end{figure}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & First 5 frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 20.00, 21.47, 23.04, 24.73, 26.54 \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 35.00, 36.47, 38.04, 39.73, 41.54 \\
	 \hline
	 mel & 100 & 20.00, 45.56, 72.02, 99.42, 127.80 \\
	 \hline
	 Bark & 100 & 20.00, 45.88, 71.85, 97.96, 124.24 \\
	 \hline
	 Constant-Q & 500 & 20.00, 20.28, 20.57, 20.86, 21.16 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 35.00, 35.28, 35.57, 35.86, 36.16 \\
	 \hline
	 mel & 500 & 20.00, 25.00, 30.03, 35.10, 40.21 \\
	 \hline
	 Bark & 500 & 20.00, 25.13, 30.26, 35.40, 40.54 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & Last 5 frequency bins \\
	 \hline
	 \hline
	 Constant-Q & 100 & 16614.38, 17832.63, 19140.20, 20543.64, 22050.00 \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 16629.38, 17847.63, 19155.20, 20558.64, 22065.00 \\
	 \hline
	 mel & 100 & 19087.44, 19789.79, 20517.07, 21270.17, 22050.00 \\
	 \hline
	 Bark & 100 & 18558.87, 19376.12, 20229.33, 21120.07, 22050.00 \\
	 \hline
	 Constant-Q & 500 & 20845.91, 21140.62, 21439.50, 21742.61, 22050.00 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 20860.91, 21155.62, 21454.50, 21757.61, 22065.00 \\
	 \hline
	 mel & 500 & 21428.92, 21582.58, 21737.31, 21893.11, 22050.00 \\
	 \hline
	 Bark & 500 & 21308.75, 21491.70, 21676.21, 21862.31, 22050.00 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & First 5 Q factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 12.37, 12.00, 11.67, 11.35, 11.06 \\
	 \hline
	 mel & 100 & 0.40, 0.88, 1.34, 1.78, 2.21 \\
	 \hline
	 Bark & 100 & 0.39, 0.89, 1.38, 1.87, 2.35 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 62.33, 61.96, 61.59, 61.23, 60.87 \\
	 \hline
	 mel & 500 & 2.01, 2.49, 2.97, 3.45, 3.92 \\
	 \hline
	 Bark & 500 & 1.95, 2.45, 2.95, 3.45, 3.94 \\
	 \hline
\end{tabular}\\
\vspace{1em}
\begin{tabular}{ |l|l|p{9cm}| }
	 \hline
	 Scale & Bins & Last 5 Q factors \\
	 \hline
	 \hline
	 Constant-Q & 100 & 7.06, 7.06, 7.06, 7.06, 7.06  \\
	 \hline
	 Variable-Q (+15 Hz) & 100 & 7.07, 7.07, 7.07, 7.07, 7.07 \\
	 \hline
	 mel & 100 & 13.83, 13.85, 13.86, 13.88, 13.89 \\
	 \hline
	 Bark & 100 & 11.60, 11.60, 11.60, 11.60, 11.60 \\
	 \hline
	 Constant-Q & 500 & 35.62, 35.62, 35.62, 35.62, 35.62 \\
	 \hline
	 Variable-Q (+15 Hz) & 500 & 35.64, 35.64, 35.64, 35.64, 35.64 \\
	 \hline
	 mel & 500 & 69.97, 69.98, 70.00, 70.02, 70.03 \\
	 \hline
	 Bark & 500 & 58.49, 58.49, 58.49, 58.49, 58.49 \\
	 \hline
\end{tabular}
	\caption{Different frequencies and Q factors for various NSGT scales, 20-22050 Hz}
	\label{table:nsgtfreqsandqs}
\end{table}

\newpagefill

Finally, recall the CQT spectrograms shown in figure \ref{fig:cqtvstft} in the previous section \ref{sec:cqt}. Figure \ref{fig:cqtdetails} shows additional detailed plots that can be generated with the MATLAB CQT (CQ-NSGT) function from the MATLAB Wavelet Toolbox,\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} showing more details of the varying windows of the underlying Nonstationary Gabor Transform process.

\begin{figure}[ht]
	\centering
	\subfloat[Center frequencies, 12 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt12_cf.png}}
	\subfloat[Bandwidth of Gabor atom at the Nyquist rate, 12 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt12_gaborframe.png}}
	\subfloat[Constant-Q ratio, 12 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt12_ratio.png}}\\
	\subfloat[Center frequencies, 24 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt24_cf.png}}
	\subfloat[Bandwidth of Gabor atom at the Nyquist rate, 24 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt24_gaborframe.png}}
	\subfloat[Constant-Q ratio, 24 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt24_ratio.png}}\\
	\subfloat[Center frequencies, 48 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt48_cf.png}}
	\subfloat[Bandwidth of Gabor atom at the Nyquist rate, 48 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt48_gaborframe.png}}
	\subfloat[Constant-Q ratio, 48 bpo]{\includegraphics[width=0.325\textwidth]{./images-gspi/glock_cqt48_ratio.png}}\\
	\caption{Additional NSGT information for the MATLAB CQT}
	\label{fig:cqtdetails}
\end{figure}

\newpagefill

\subsubsection{sliCQ Transform (sliCQT)}
\label{sec:theoryslicqt}

The NSGT processes the entire input signal at once. In cases where the input signal must be processed in fixed-size chunks, such as realtime streaming, the sliCQ Transform (sliCQT, or sliced Constant-Q Transform) was created (\cite{invertiblecqt, slicq}). The slicing operation is shown in figure \ref{fig:slicqtukeys}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.55\textwidth]{./images-misc/slicq_windows.png}
	\caption{Slicing the input signal with 50\% overlapping Tukey windows. N is the slice length and M is the transition area (\cite{slicq})}
	\label{fig:slicqtukeys}
\end{figure}

Slices have a 50\% overlap with adjacent slices. The noninvertibility of the half-overlap-add makes it only useful as a forward operation. Figure \ref{fig:slicqoverlaps} demonstrates this issue. This trait of the sliCQT has also been mentioned by in its implementation in the Essentia project:\footnote{\url{https://mtg.github.io/essentia-labs/news/2019/02/07/invertible-constant-q/}}

\begin{quote}
	[...] we have to overlap-add the spectrograms obtained for each frame. Note that it is not possible to synthesize the audio from this overlapped version as we cannot retrieve the analysis frames from it.
\end{quote}

\begin{figure}[ht]
	\centering
	\subfloat[Adjacent slices placed side-by-side. Note the incorrect doubling of the duration]{\includegraphics[width=\textwidth]{./images-gspi/gspi_overlap_flatten.png}}\\
	\subfloat[Adjacent slices with 50\%-overlap-add]{\includegraphics[width=\textwidth]{./images-gspi/gspi_overlap_proper.png}}
	\caption{sliCQT spectrograms demonstrating the slice half-overlap}
	\label{fig:slicqoverlaps}
\end{figure}

\newpagefill

\subsection{Machine learning and deep learning for music signals}
\label{sec:ml}

Music signals are a form of acoustic signal, and the domain of music signal processing is inseparable from the larger field of signal processing (\cite{musicsp}). Signal processing encompasses the subfields of statistical, approximate, or stochastic signal processing (\cite{stochasticsp, statisticalsp}). The field of optimization has been used for approximate signal processing problems for decades (\cite{optsp}). Some of the classic examples include basis pursuit and matching pursuit (\cite{dictionary1, dictionary2}). To consider the problem of optimization outside of the signal processing domain, we refer to \textcite{introtoml}'s description of machine learning:

\begin{quote}
	In many scientific disciplines, the primary objective is to model the relationship between a set of observable quantities (inputs) and another set of variables that are related to these (outputs). [...] Machine learning provides techniques that can automatically build a computational model of these complex relationships by processing the available data and maximizing a problem dependent performance criterion.
\end{quote}

According to \textcite{introtodl}, deep learning ``is the subfield of machine learning that is devoted to building algorithms that explain and learn a high and low level of abstractions of data that traditional machine learning algorithms often cannot.'' A characteristic of deep networks is that they have many hidden layers, named so ``because we do not necessarily see what the inputs and outputs of these neurons are explicitly beyond knowing they are the output of the preceding layer,'' to model more complex relationships between the input and output. Figure \ref{fig:fcdn} shows a deep network with hidden layers.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-neural/dnn.png}
	\caption{Deep neural network with hidden layers (\cite{introtodl})}
	\label{fig:fcdn}
\end{figure}

\textcite{introtodl} go on to state that ``most machine learning algorithms as they exist now focus on function optimization, and the solutions yielded do not always explain the underlying trends within the data nor give the inferential power that artificial intelligence was trying to get close to.'' However, machine learning methods have achieved success in many fields including natural language processing (\cite{nlpml}), computer vision (\cite{cvml}), and audio (\cite{audiodeeplearning}), indicating that data-driven approaches are useful, despite falling short of general artificial intelligence (\cite{generalai}).

We also note that statistical signal processing techniques and machine and deep learning are compatible with one another and can be used together (\cite{mlsp1, mlsp2}). In fact, according to \textcite{mldspmix}, analytic knowledge and machine learning should both be considered in the study of acoustic signals:

\begin{quote}
	Whereas physical models are reliant on rules, which are updated by physical evidence (data), ML is purely data-driven. By augmenting ML methods with physical models to obtain hybrid models, a synergy of the strengths of physical intuition and data-driven insights can be obtained.
\end{quote}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-neural/dspmlmix.jpg}
	\caption{Mixed analytic and data-driven approaches}
	\label{fig:dspmlmix}
\end{figure}

In machine learning,  the input data is typically split into a training set and a test set (\cite{introtoml}). The model makes predictions with its initial parameters using the training input. It measures how correct its prediction was by using a loss function to compare its predicted output to the training (or ground truth) output. Contemporary machine learning is dependent on the field of optimization (\cite{boyd2004convex, mlopt1, mlopt2}) for the loss function and parameter updates (\cite{sgd}). The performance of the network is measured by its ability to generalize on the test set, which was data not seen during training. There is sometimes a third validation set, which is also unseen during training and used to perform hyperparameter tuning (\cite{splitvaliddata}) which are the parameters of a machine learning model set by the user (\cite{introtodl}). The user tries to maximize the performance of the model on the validation set by modifying the hyperparameters.

\textcite{audiodeeplearning} describe contemporary deep learning architectures that are used for audio applications. They state that models from the field of computer vision, originally designed for 2D images where pixels are related spatially, may not exactly fit audio waveforms where amplitude values are related temporally. However, convolutional neural networks, recurrent neural networks, and sequence-to-sequence models are three popular types of network architecture that have been adapted with varying levels of success to the audio and music domains (\cite{audiodeeplearning}).

\newpagefill

\subsubsection{Convolutional Neural Networks (CNN)}

Convolutional neural networks (CNN) ``are based on convolving their input with learnable kernels'' (\cite{audiodeeplearning}). These are useful for both images and audio according to \textcite{cnns}, because ``the attractive feature of the CNN is its ability to exploit spatial or temporal correlation in data.'' Figure \ref{fig:cnnbasic} shows how a learnable convolution kernel \textit{slides} over the pixels of the input image.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[width=0.48\textwidth]{./images-neural/sliding_conv_1.png}}
	\hspace{0.15em}
	\subfloat{\includegraphics[width=0.48\textwidth]{./images-neural/sliding_conv_2.png}}
	\caption{$3 \times 3$ convolution kernel sliding over patches of input pixels}
	\label{fig:cnnbasic}
\end{figure}

From \textcite{audiocnn2}'s paper, the network architecture and feature map for an audio CNN is shown, where the extracted feature map is visualized in the spectral domain.

\begin{figure}[ht]
	\centering
	\subfloat[CNN architecture for audio waveforms]{\includegraphics[width=0.65\textwidth]{./images-neural/audio_cnn.png}}\\
	\subfloat[Visualization of audio spectral features learned by each layer]{\includegraphics[width=0.65\textwidth]{./images-neural/audio_cnn2.png}}
	\caption{Feature maps in a typical audio CNN}
	\label{fig:audiocnn}
\end{figure}

Finally, we note that for audio applications, both 1D convolutions (also referred to as temporal convolutions) applied directly to the audio waveform and 2D convolutions applied to time-frequency transforms (most commonly the STFT) are both used (\cite{tcn, 2dconv}).

\subsubsection{Recurrent Neural Networks (RNN)}

For time series data, for which the audio waveform is an exemplar given the temporal evolution of its amplitude, recurrent neural networks (RNNs) are useful since the input sequence of data is introduced back into the network with a cyclic or recurrent connection, and outputs are predicted based on past (or future, if the network is bi-directional) values of the sequence (\cite{rnns}). RNNs are also used commonly in sequence-to-sequence, or seq2seq, models (\cite{seq2seqs}).

\textcite{rnnoise} describes different RNN variants, including the LSTM (\cite{lstm1}) and GRU (\cite{gru1}) for their audio noise suppression model, RNNoise:\footnote{\url{https://jmvalin.ca/demo/rnnoise/}}

\begin{quote}
	Recurrent neural networks (RNN) are very important [...] because they make it possible to model time sequences instead of just considering input and output frames independently. [...] RNNs were heavily limited in their ability because they could not hold information for a long period of time [...] [These] problems were solved by the invention of gated units, such as the Long Short-Term Memory (LSTM), the Gated Recurrent Unit (GRU), and their many variants.
\end{quote}

Figure \ref{fig:rnndiags} shows some example RNN architectures, as well as an illustration of the additional complexity in the gated LSTM and GRU units, which allow them to surpass the simple RNN.

\begin{figure}[ht]
	\centering
	\subfloat[Regular RNN with cyclic connections to past data (\cite{birnn})]{\includegraphics[width=0.48\textwidth]{./images-neural/simple_rnn.png}}
	\hspace{0.1em}
	\subfloat[Bi-directional LSTM with cyclic connections to past and future data (\cite{birnn})]{\includegraphics[width=0.48\textwidth]{./images-neural/birnn.png}}\\
	\subfloat[Different recurrent units (\cite{lstmrnngru})]{\includegraphics[width=0.65\textwidth]{./images-neural/gates.png}}
	\caption{RNN diagrams}
	\label{fig:rnndiags}
\end{figure}

\newpagefill

\subsection{Software and code concepts}
\label{sec:softcode}

\subsubsection{Python programming language}

Python\footnote{\url{https://www.python.org/}} is a general-purpose programming language. It is an interpreted language,\footnote{\url{https://www.python.org/doc/essays/blurb/}} which means that there is no compilation step required, and the Python code or script written by a user can be executed right away with the Python interpreter. The Python interpreter can also run statements directly for quick prototyping, without needing to write a script:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
Python 3.9.6 (default, Jul 16 2021, 00:00:00)
[GCC 11.1.1 20210531 (Red Hat 11.1.1-3)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
>>> print('this is python')
this is python
\end{BVerbatim}
\end{listing}

The Python interpreter is widely distributed on most modern operating systems like Linux, Windows, and OS-X. Python has support for various high-level and user-friendly data structures. Python has seen widespread adoption in academia, and especially in numerical contexts (\cite{pythonscience}).

Some important concepts from the Python language will appear in this thesis and will be described further in this section -- lists, generators, decorators, and random number generator seeding.

The first is the list,\footnote{\url{https://docs.python.org/3/tutorial/datastructures.html\#more-on-lists}} which is a core datastructure of the language. The list in Python corresponds to the array data structure in computer science (\cite{skiena}). It allows for the construction of an ordered list of objects. For example, a discrete-time speech signal may be described by a list of its amplitude values, where the position in the list is the time sample $n$:

\hfil \Verb#speech_signal = [0.15, 0.28, 0.57, 0.98, -0.59]#

The generator is a function in Python which generates the next value of a sequence whenever it is called with the \Verb#yield# statement.\footnote{\url{https://docs.python.org/3/reference/simple_stmts.html\#the-yield-statement}} The internal state of the sequence is managed by the generator function. Generators are used for efficiency, where instead of pre-computing the output of some expensive function for the entire set of values, the expensive function is only executed when a single value is demanded at a time. This is also called lazy evaluation.\footnote{\url{https://cvw.cac.cornell.edu/python/lazy}} An example of a generator is shown in listing \ref{lst:siggen}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
>>> def speech_signal():
...     samples = [0.15, 0.28, 0.57, 0.98, -0.59]
...     for s in samples:
...             yield expensive_function(s)
...
>>> s = speech_signal()
>>> print(s)
<generator object speech_signal at 0x7f642933fa50>
# N.B. expensive function has not been executed yet
>>>
>>> for sample in s:
...     print(sample) # expensive function is computed on demand
\end{BVerbatim}
	\caption{A generator in Python, where the expensive function is only called when a value is requested}
	\label{lst:siggen}
\end{listing}

The decorator in Python is a single line with a leading \Verb#@# symbol, added on top of a function, to wrap the execution of the function with another.\footnote{\url{https://docs.python.org/3/glossary.html\#term-decorator}} A simple example of a decorator is shown in listing \ref{lst:decorator}.

\begin{listing}[ht]
\centering
\begin{BVerbatim}
>>> def custom_decorator(func):
...     def wrapper():
...             print('before')
...             func()
...             print('after')
...     return wrapper
...
>>> @custom_decorator
... def print_hello():
...     print('hello')
...
>>>
>>> print_hello()
before
hello
after
\end{BVerbatim}
	\caption{A decorator in Python}
	\label{lst:decorator}
\end{listing}

Decorators are often used to simplify tasks like measuring a function's execution time, or CPU and memory footprint, which is intended to observe or measure but not modify the original function.\footnote{\url{https://pypi.org/project/line-profiler/, https://pypi.org/project/memory-profiler/}}

The \Verb#random# module of Python contains a random number generator (RNG).\footnote{\url{https://docs.python.org/3/library/random.html}} The \Verb#seed# function initializes the random number generator. \footnote{\url{https://docs.python.org/3/library/random.html\#bookkeeping-functions}} This is because the RNG in Python is pseudorandom (\cite{pseudorng}), which means that given the same starting seed, the exact same sequence of numbers are generated deterministically. The random seed is usually a parameter to the script or Python program, such that the user can recreate the same RNG sequences for testing purposes by passing the same seed.

\subsubsection{Numerical and machine learning libraries for Python}

Python has a rich ecosystem of academic libraries. NumPy (\cite{numpy}) and SciPy (\cite{scipy}) are used for numerical computation and parallelized matrix operations that run on the CPU (central processing unit). Pandas (\cite{pandas}) and Seaborn (\cite{seaborn}) are available for data processing. Matplotlib (\cite{matplotlib}) is a plotting library. Tensorflow (\cite{tensorflow}) and PyTorch (\cite{pytorch}) are libraries for machine learning and deep learning, which also have numerical computation and parallelized matrix operations that are similar to NumPy and SciPy, except that they can run on the GPU (graphical processing unit). The use of the GPU allows for more efficient, faster, and larger parallelized matrix operations, which are essential for modern machine learning and deep learning techniques.

In NumPy, the ndarray\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html}} is the core datastructure representing an n-dimensional array of objects, which can be numerical (such as int16 or float32) or general objects (such as strings). The same speech signal represented by the Python list above can be represented by a 1-dimensional ndarray of 64-bit floating-point values:

\begin{listing}[!ht]
\centering
\begin{BVerbatim}
>>> import numpy
>>> speech_signal = numpy.asarray([0.15, 0.28, 0.57, 0.98, -0.59])
>>> print(speech_signal.dtype)
float64
>>> print(speech_signal.shape)
(5,)
\end{BVerbatim}
\end{listing}

The underlying values are stored in formats that allow for efficient numerical computations, which is why using NumPy ndarrays for math is preferred to using regular Python datastructures such as lists that are not designed for speed.

In PyTorch and Tensorflow (and in general machine learning), a similar concept to the ndarray is the tensor. The tensor originates from the field of physics (\cite{whatistensor}). The tensor is essentially the same as an ndarray, or n-dimensional array, and in fact, NumPy ndarrays and PyTorch and Tensorflow tensors are interchangeable.\footnote{\url{https://www.tensorflow.org/guide/tf_numpy}, \url{https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html}}

\subsubsection{Version control and git}

\subsubsection{Open-source software and GitHub}

\newpagefill

\subsection{Music source separation}
\label{sec:musicsep}

\subsubsection{Task motivation and definition}

Typical music recordings are mono or stereo mixtures, with multiple sound objects (drums, vocals, etc.) sharing the same track (\cite{musicsepintro1}). To manipulate the individual sound objects, the stereo audio mixture needs to be separated into a track for each different sound source, in a process called audio source separation.

There are many motivations for performing this separation. \textcite{musicsepgood} describe that once we perform audio source separation and have access to the individual source tracks, we can ``remix the balance within the music [...] to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a 2-channel stereo recording to a 5.1-
channel surround sound system... We might also want to change the spatial location of a musical instrument within the mix.''

In the ISMIR 2021 Music Demixing Challenge (MDX),\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} audio source separation is described as having different subtasks, including ``music source separation systems [that] take a song as input and output one track for each of the instruments,'' and ``speech enhancement systems [that] take noisy speech as input and separate the speech content from the noise.'' The competition description mentions important uses for music demixing systems, ranging from entertainment to hearing aids:

\begin{quote}
	For example, the original master of old movies contains all the material (dialogue, music and sound effects) mixed in mono or stereo: thanks to source separation we can retrieve the individual components and allow for up-mixing to surround systems. [...] Karaoke systems can benefit from the audio source separation technology as users can sing over any original song, where the vocals have been suppressed, instead of picking from a set of ``cover'' songs specifically produced for karaoke.
\end{quote}

A common thread in survey papers is that the domains of speech enhancement and music demixing are both mentioned as important subproblems of audio source separation of (\cite{musicsepintro1, musicsepsurvey}). Music demixing is the focus of this thesis, shown in figure \ref{fig:mixingdiagrams}.

\begin{figure}[ht]
	\centering
	\subfloat{\includegraphics[height=4cm]{./images-mss/mixdemix.png}}
	\caption{Diagram for music mixing and demixing (from ISMIR 2021 MDX)}
	\label{fig:mixingdiagrams}
\end{figure}

\newpagefill

\subsubsection{Computational approaches}

Computational source separation has a history for at least 50 years (\cite{musicsepsurvey, musicsepintro1}), originating from the tasks of computational auditory scene analysis (CASA) and blind source separation (BSS). In CASA and BSS, the mixed audio contains unknown sources that must be separated. In music demixing, the sources are typically known; most commonly, 4 sources (vocals, drums, bass, other) are used as defined by the MUSDB18 dataset (\cite{musdb18}).

Among the earliest computational approaches for blind source separation is Independent Component Analysis (ICA)  (\cite{musicsepsurvey, musicsepgood, musicsepintro1}), which exploits spatial information of the sources, and assumes the sources to be independent. This technique can be used when there are as many channels in the mixture (corresponding to differently placed microphones) as the number of sources. We refer the readers to \textcite{ica1, ica2} for ICA algorithms, and to \textcite{blind1, blind2} for a more in-depth review on the history of blind source separation. Figure \ref{fig:icaposition} shows an example of the positional considerations in a typical ICA system.

\begin{figure}[ht]
	\centering
	\includegraphics[height=8cm]{./images-mss/positional.png}
	\caption{Position-based source separation (\cite{musicsepgood})}
\label{fig:icaposition}
\end{figure}

According to \textcite{musicsepintro1}, ICA techniques arose more typically for speech denoising (\cite{speechsep}), and they make assumptions that cannot be generalized easily to music:

\begin{quote}
	[systems which aim] to recover clean speech from noisy recordings [...] can be seen as a particular instance of source separation. [...] many algorithms assume the audio background can be modeled as stationary. However, the musical sources are characterized by a very rich, non-stationary spectrotemporal structure. This prohibits the use of such methods. Musical sounds often exhibit highly synchronous evolution over both time and frequency, making overlap in both time and frequency very common. Furthermore, a typical commercial music mixture violates all the classical assumptions of ICA. Instruments are correlated (e.g., a chorus of singers), there are more instruments than channels in the mixture, and there are non-linearities in the mixing process (e.g., dynamic range compression).
\end{quote}

Techniques more specific to music were developed as a necessity to deal with these differences (\cite{musicseptechniques1, musicseptechniques2}).

For cases where ICA cannot be applied, musical source models are more popular (\cite{musicsepgood}), which are ``model-based approaches that attempt to capture the spectral characteristics of the target source can be used.'' Sources are assumed to be sufficiently different from each other, or sparse in their spectral representation (\cite{musicsepgood}), such that they can be extracted with time-frequency masks applied in the spectral domain. Figure \ref{fig:sepgood} shows how different sources have unique spectral patterns.

\begin{figure}[ht]
	\centering
	\subfloat[Mixed spectrogram]{\includegraphics[height=4.75cm]{./images-mss/mss1.png}}
	\subfloat[Source spectrograms]{\includegraphics[height=5cm]{./images-mss/mss2.png}}
	\caption{Sparsity of different music sources in the spectral domain}
\label{fig:sepgood}
\end{figure}

Kernel Additive Modeling (KAM) is the simplest form of music source modeling (\cite{musicsepgood}). To estimate a music source at a given time-frequency point, KAM ``selects a set of time-frequency bins, which, given the nature of the target source, e.g., percussive, harmonic or vocals, are likely to be similar in value. This set of time-frequency bins is termed a proximity kernel.'' A well-known example of a KAM-based music separation algorithm is \textcite{fitzgerald1}'s median-filtering Harmonic/Percussive Source Separation (HPSS) algorithm. Figure \ref{fig:fitzhpss} shows the basic operation of the median filtering operator for HPSS.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-misc/hpss.png}
	\caption{Median-filtering HPSS}
	\label{fig:fitzhpss}
\end{figure}

Spectrogram factorization models are more sophisticated than KAM, and the most popular spectrogram factorization model is Nonnegative Matrix Factorization (NMF) (\cite{musicsepsurvey, musicsepgood}). According to \textcite{musicsepgood}, NMF ``attempts to factorize a given non-negative matrix into two non-negative matrices,'' and it can be applied to the non-negative magnitude spectrogram of the mix, $M$, to separate it into frequency weight matrices $W$ and time activation matrices $H$; \textcite{nmfpaper}'s survey on NMF techniques covers the algorithm in more detail. Most recently, data-driven approaches based on machine learning and deep learning have significantly surpassed past approaches (\cite{musicsepgood, sisec2018}). Figure \ref{fig:spectraldemix} shows different techniques for spectral demixing.

\begin{figure}[ht]
	\centering
	\subfloat[KAM vs. NMF for spectral music demixing]{\includegraphics[width=0.7\textwidth]{./images-mss/kamvnmf.png}}\\
	\subfloat[Deep neural networks for spectral music demixing]{\includegraphics[width=0.7\textwidth]{./images-mss/mssdnn.png}}
	\caption{Different techniques for spectral music demixing (\cite{musicsepgood})}
	\label{fig:spectraldemix}
\end{figure}

\newpagefill

\subsubsection{Time-frequency masking and oracle estimators}
\label{sec:masksandoracles}

Surveys on speech (\cite{speechmask}) and music separation (\cite{musicmask}) indicate that the majority of source separation algorithms use the technique of time-frequency masking (or spectral masking) to separate the sources.  \textcite{masking} describe different time-frequency masking strategies in audio source separation. A time-frequency mask (or spectral mask, or masking filter) is a matrix of the same size as the complex STFT, or its real-valued magnitude, by which the STFT is multiplied to mask, filter, or suppress specific time-frequency bins.

A soft mask, or ratio mask, has real values $\in [0.0, 1.0]$, and a binary or hard mask has logical values, i.e., only 0 and 1. To compute a binary mask, there must be an additional real-valued parameter, $\theta \in [0.0, 1.0]$, which is the separation factor -- values below $\theta$ are set to 0, and values above $\theta$ are set to 1. According to \textcite{masking}, soft masks are generally produce a higher quality of sound. An illustration of spectral masking is shown in figure \ref{fig:masks}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{./images-mss/maskdemo.png}
	\caption{Results of a soft and hard oracle mask applied for speech denoising}
	\label{fig:masks}
\end{figure}

The oracle mask, or oracle estimator, is a perfect time-frequency mask which is computed from ground-truth data. The use of the oracle estimator is to give an idea of the upper limit of audio quality from an algorithm or machine learning model for source separation or demixing. Typically, the mask is computed from and applied to the magnitude spectrograms (\cite{fitzgerald1, fitzgerald2, driedger, umx, plumbley1, plumbley2}). Discarding the phase of the complex STFT is a choice made for simplicity, but the phase may be important for source separation applications (\cite{ditchphase}).

To illustrate the calculation of the oracle mask, let us describe a simple case of a mixed song consisting of a vocal and drum track. We assume that we have access to the isolated source recordings of vocals and drums, as well as the mix.

We start by denoting the waveforms $x_{v}[n]$ and $x_{d}[n]$ for the isolated vocal and drum tracks respectively. The mixed song is defined by the waveform $x_{m}[n] = x_{v}[n] + x_{d}[n]$. To apply the music demixing task to this waveform, let us say that we want our algorithm or network to take the mixed waveform $x_{m}[n]$ as an input, and estimate the two source waveforms of vocals $\hat{x}_{v}[n]$ and drums $\hat{x}_{d}[n]$.

Given the following operations to get the STFTs of the mix and two sources, we seek to calculate various interesting oracle masks:
\begin{align}
	\nonumber X_{m} &= \text{STFT}(x_{m}[n]), |X_{m}| = \text{abs}(X_{m})\\
	\nonumber X_{v} &= \text{STFT}(x_{v}[n]), |X_{v}| = \text{abs}(X_{v})\\
	\nonumber X_{d} &= \text{STFT}(x_{d}[n]), |X_{d}| = \text{abs}(X_{d})
\end{align}
 
\textcite{sisec2018} report the performance of 5 oracles, called IRM1, IRM2, IBM1, IBM2, and the MWF (multichannel Wiener filter). The first 4 are more relevant to stereo music, which is the case considered in this thesis. The ``I'' is for Ideal, ``R|B'' denotes a Ratio vs. Binary mask, ``M'' is for Mask, and the trailing number is the $p$th power which the magnitude spectrogram is raised to. For example, the magnitude spectrogram raised to the 1st power remains unchanged, while one raised to the 2nd power is the power spectrogram.

The oracles for the vocal source (and the same equations apply to the drum track) are computed thus, noting that a typical default value for $\theta$ is 0.5:
\begin{align}
	\nonumber \text{IRM1}_{v} &= \frac{|X_{v}|^{1}}{|X_{m}|^{1}}\text{,\qquad}
	\nonumber \text{IRM2}_{v} = \frac{|X_{v}|^{2}}{|X_{m}|^{2}}\\
	\nonumber \text{IBM1}_{v} &= \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{1}}{|X_{m}|^{1}} \ge \theta
	\end{cases},
	\nonumber \text{IBM2}_{v} = \begin{cases}
		0 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} < \theta\\
		1 \text{ where } \frac{|X_{v}|^{2}}{|X_{m}|^{2}} \ge \theta
	\end{cases}
\end{align}

To estimate the time domain waveform from an oracle mask, the complex STFT of the mixed waveform, $X_{m}$, is multiplied by the mask, and the resultant complex STFT is inverted back to the time-domain waveform:
\begin{align}
	\nonumber \hat{X}_{v\text{, IRM1}} &= X_{m} \cdot \text{IRM1}_{v}, \hat{x}_{v\text{, IRM1}} = \text{iSTFT}(\hat{X}_{v\text{, IRM1}})
\end{align}

\newpagefill

\subsubsection{Open-Unmix (UMX) and CrossNet-Open-Unmix (X-UMX)}
\label{sec:umx}

Open-Unmix (UMX) was created by \textcite{umx} as an open-source, reference implementation of a state-of-the-art deep neural network for music source separation. Open-Unmix is based on STFT masking, and is intended to foster reproducible research by only using the open MUSDB18 dataset for training data (\cite{musdb18, musdb18hq}).

In UMX, a deep neural network is used to estimate the magnitude spectrograms of the sources from an input mixed song. The architecture is a variant of a sequence2sequence model. Open-Unmix uses a bidirectional LSTM or Bi-LSTM architecture, which is based on two predecessor networks by \textcite{umxorig1} and \textcite{umxorig2}. Figure \ref{fig:umxes} the architecture of UMX and one of its predecessors.

\begin{figure}[ht]
	\centering
	\subfloat[Open-Unmix Bi-LSTM architecture]{\includegraphics[width=0.8\textwidth]{./images-neural/umx.png}}\\
	\subfloat[Simple DNN architecture for MSS]{\includegraphics[width=0.8\textwidth]{./images-neural/typical_simple_mss_dnn.png}}
	\caption{Architecture of Open-Unmix (\cite{umx}) and one of its simpler predecessors (\cite{umxorig1})}
	\label{fig:umxes}
\end{figure}

\textcite{xumx} combined the 4 separate target networks into a single model to apply the loss functions and optimizations jointly across all 4 targets, rather than optimizing each one separately. The model is named CrossNet-Open-Unmix (X-UMX). It also includes loss computed from the time-domain waveforms, in addition to the loss measured from the magnitude spectral coefficients.

\subsubsection{Convolutional denoising autoencoders}

While the discussed Open-Unmix model is based on sequence2sequence ideas with a bidirectional LSTM architecture, a different class of neural network called convolutional autoencoders, or convolutional denoising autoencoders (CDAE), have been seeing increasing use in music demixing (\cite{plumbley1, plumbley2}). The architectures are shown in figure \ref{fig:cdaes}, where 2D convolutions (in the time and frequency dimensions) are applied on the magnitude STFT.

\begin{figure}[ht]
	\centering
	\subfloat[Simple CDAE]{\includegraphics[width=\textwidth]{./images-neural/cdae_1.png}}\\
	\subfloat[Different sizes of time and frequency filter by frequency band]{\includegraphics[width=0.7\textwidth]{./images-neural/cdae_2.png}}
	\caption{Various convolutional architectures for music source separation}
	\label{fig:cdaes}
\end{figure}

We will note that each of the papers gives us different ideas with how to adapt STFT-based convolutional models to fit the sliCQT. The model introduced in \textcite{plumbley1} shows a simple case of a 2D time and frequency filter applied to the STFT, which can be applied to any time-frequency transform. The model introduced in \textcite{plumbley2} varies the size of time and frequency filter to fit the Constant-Q Transform, which as discussed in section \ref{sec:cqt} analyzes music with higher frequency resolution in the low frequency region, and higher time resolution in the high frequency region.

\subsubsection{Public datasets}

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset (\cite{musdb18}), and more recently the HQ (high-quality) version (\cite{musdb18hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets (\cite{otherdataset1, otherdataset2}).

The songs in the MUSDB18-HQ dataset have a fixed train, validation, and test split. Following the rules defined in the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} for a network to be considered trained only on MUSDB18-HQ, the predefined data splits must be used.

\subsubsection{Evaluation measures}
\label{sec:evalbss}

The SigSep\footnote{\url{https://sigsep.github.io/}} community, borrowing from the methodology of Signal Separation Evaluation Campaign (SISEC), uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality. There are 4 distinct metrics that comprise BSS:

\begin{tight_itemize}
\item
	\textbf{ISR:} source Image to Spatial distortion Ratio
\item
	\textbf{SIR:} Signal to Interference Ratio
\item
	\textbf{SAR:} Signal to Artifacts Ratio
\item
	\textbf{SDR:} Signal to Distortion Ratio
\end{tight_itemize}

Out of these 4 scores, SDR is the single global score which is commonly used to summarize the overall performance of a music demixing system (\cite{sdruseful}). The SDR as it was defined in the ISMIR 2021 Music Demixing Challenge (and used to rank the participants) can be computed from the following equation \ref{equation:sdrinstr}:
\begin{align}
	\nonumber & \text{SDR}_{\text{instr}} = \\
	&10 \log_{10}\frac{\sum_{n}\big(s_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n)\big)^{2}}{\sum_{n}\big(s_{\text{instr, left}}(n) - \hat{s}_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n) - \hat{s}_{\text{instr, right}}(n)\big)^{2}} \tag{10}\label{equation:sdrinstr}
\end{align}

The unit of the SDR score is in decibels or dB, $s_{\text{instr}}(n)$ denotes the ground truth waveform of the instrument, $\hat{s}_{\text{instr}}(n)$ is the estimate, and left and right refer to the two channels in the stereo dataset of MUSDB18-HQ. Given the four stems of MUSDB18-HQ -- vocals, drums, bass, other -- the SDR is computed for each stem from the equation \ref{equation:sdrinstr} above. Then, the four SDR scores are combined for a total song score in equation \ref{equation:sdrsong}:
\begin{align}
	\text{SDR}_{\text{song}} = \frac{1}{4}(\text{SDR}_{\text{bass}} + \text{SDR}_{\text{drums}} + \text{SDR}_{\text{vocals}} + \text{SDR}_{\text{other}}) \tag{11}\label{equation:sdrsong}
\end{align}

In the most recent SiSec evaluation (\cite{sisec2018}), the BSS evaluation measure used is BSS v4, a variant of BSS available in their Python libraries museval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} and bsseval.\footnote{\url{https://github.com/sigsep/bsseval}} The differences between BSS as used in SiSec 2016 (\cite{sisec2016}) and BSS v4 are outlined in the bsseval project's GitHub README file:

\begin{quote}
	One particularity of BSSEval is to compute the metrics after optimally matching the estimates to the true sources through linear distortion filters. This allows the criteria to be robust to some linear mismatches... this matching is the reason for most of the computation cost of BSSEval...

	For this package, we enabled the option of having time invariant distortion filters, instead of necessarily taking them as varying over time as done in the previous versions of BSSEval. First, enabling this option significantly reduces the computational cost for evaluation because matching needs to be done only once for the whole signal. Second, it introduces much more dynamics in the evaluation, because time-varying matching filters turn out to over-estimate performance. Third, this makes matching more robust, because true sources are not silent throughout the whole recording, while they often were for short windows
\end{quote}

\subsubsection{Open-source ecosystem}

There are several groups of researchers who created open-source ecosystems on the code sharing platform GitHub\footnote{\url{https://github.com/}} to provide tools, tutorials, and implementations for source separation. Of note are:

\begin{tight_enumerate}
	\item
		SigSep,\footnote{\url{https://github.com/sigsep}} which contains many tools, including \textcite{umx}'s Open-Unmix model, \textcite{musdb18, musdb18hq}'s MUSDB18 dataset loaders, and \textcite{bss}'s BSS evaluation metrics
	\item
		nussl,\footnote{\url{https://github.com/nussl}} from \textcite{nussl}, which contains a library with many algorithms for music and speech separation
	\item
		Asteroid,\footnote{\url{https://github.com/asteroid-team}} from \textcite{asteroid}, another source separation toolkit
\end{tight_enumerate}

The author of this thesis owes a debt of gratitude to these projects. Without such community-focused efforts, the field of music demixing would be less approachable for beginners.

\end{document}
