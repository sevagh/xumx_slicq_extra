\documentclass[report.tex]{subfiles}
\begin{document}

\section{Background}
\label{sec:background}

\subsection{Acoustic signals and the time-domain waveform}
\label{sec:timedomain}

\textcite[Chapter~2]{discretebook} define a signal as a term which

\begin{quote}
	conveys information about the state or behavior of a physical system... [S]ignals are synthesized for the purpose of communicating information between humans or between humans and machines... Signals are represented mathematically as functions of one or more independent variables. For example, a speech signal is represented mathematically as a function of time, and a photographic image is represented as a brightness function of two spatial variables.
\end{quote}

\textcite[Chapter~2]{moore} describes the more specific case of sound, or audio signals, as follows:

\begin{quote}
	Sound originates from the vibration of an object. This vibration is impressed upon the surrounding medium (usually air) as a pattern of changes in pressure
\end{quote}

The waveform is defined as the function of pressure variation plotted against time (\cite{moore, melbook}), and is real-valued and continuous in both time and amplitude (\cite[Chapter~2]{melbook}). A continuous-time signal $x$, also called an analog signal, is denoted by $x_{a}(t)$. For digital processing, continuous-time signals need to be sampled periodically to form a sequence of numbers (\cite[Chapter~2]{discretebook}). The resultant domain is called discrete time, and a discrete-time signal is denoted by $x[n]$.  A continuous-time signal and its discrete representation are shown in figure \ref{fig:discretecontinuous}.

\begin{figure}[ht]
	\centering
	\subfloat[Continuous-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/continuoustime.png}}\\
	\subfloat[Discrete-time waveform]{\includegraphics[height=2.4cm]{./images-tftheory/discretetime.png}}
	\caption{A continous-time signal and its discrete-time representation sampled with $T = 125\mu s$ (\cite[Chapter~2]{discretebook})}
	\label{fig:discretecontinuous}
\end{figure}

\newpagefill

Continuous-time signals are converted into discrete-time representations by two processes: \textit{sampling}, or considering periodic points on the continuous time axis throughout the waveform, and the \textit{quantization} of the amplitude values of the waveform at those points to find the closest digital number (\cite[Chapter~2]{melbook}). The sampling process is controlled by the sampling period $T \text{ seconds}$, or the sampling (or sample) rate $F_{s} = \sfrac{1}{T} \text{ Hz}$, which define the periodicity with which time values are considered. The amplitude quantization process is controlled by the number of quantization levels $2^{B}$, where $B$ is the number of bits per sample of the representation.

First, continuous time needs to be sampled into the discrete time domain. The relationship of a discrete-time signal $x[n]$ of a continuous-time signal $x_{a}(t)$ is defined by $x[n] = x_{a}(nT)$, where $n$ is an integer and $T = \sfrac{1}{F_{s}}$ is the sampling period. The Nyquist-Shannon sampling theorem (\cite[Chapter~4]{discretebook}), described independently by \textcite{nyquist1928} and \textcite{shannon1948}, states that the maximum frequency of a signal that can be represented by a sampling rate $F_{s}$ is $F_{\text{nyq}} = \sfrac{F_{s}}{2}$, which is also called the Nyquist rate or Nyquist frequency. The diagram in figure \ref{fig:aliasing} shows the phenomenon of \textit{undersampling}, which occurs when a time-domain signal is undersampled, i.e. $F_{\text{sig}} > F_{\text{nyq}}$, and the continuous-time signal cannot be reconstructed accurately (\cite[Chapter~4]{dspfirst}). We refer the readers to the chapter in the textbook for a more full description of challenges in signal sampling.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{./images-tftheory/aliasing_undersampling.png}
	\caption{An undersampled cosine wave (red) and the resulting incorrect reconstruction (black) from \textcite{dspfirst}}
	\label{fig:aliasing}
\end{figure}

Second, the continuous amplitude needs to be quantized. Stated by \textcite[Chapter~4]{discretebook}, ``the quantizer is a nonlinear system whose purpose is to transform the input sample $x[n]$ into one of a finite set of prescribed values.'' The second variable which relates to the quantization process is the number of quantization levels. The quantization operation is represented as $ \hat{x}[n] = Q(x[n])$, where $\hat{x}[n]$ is the quantized sample. Quantization levels can be defined to be uniform (evenly spaced) or nonuniform, and in essence the sample values are rounded to the nearest quantization level $2^{B}$, recalling that $B$ is the number of bits per sample in the representation (\cite{discretebook}). An A/D or ADC (analog-to-digital converter) circuit and its operation on a waveform is shown in figure \ref{fig:adccircuit}.

\begin{figure}[ht]
	\centering
	\subfloat[Analog-to-digital converter (ADC)]{\includegraphics[height=2.2cm]{./images-tftheory/adc1.png}}
	\hspace{0.1em}
	\subfloat[ADC details: sampler and quantizer]{\includegraphics[height=2.2cm]{./images-tftheory/adc2.png}}\\
	\vspace{0.1em}
	\subfloat[Waveform results of the ADC process]{\includegraphics[width=0.85\textwidth]{./images-tftheory/adc3.png}}\\
	\caption{An ADC converter circuit showing the time sampling and amplitude quantizing operations (\cite[Chapter~4]{discretebook})}
	\label{fig:adccircuit}
\end{figure}

\newpagefill

\subsection{Transforms of acoustic signals}
\label{sec:freqdomain}

\textcite[Chapter~2B]{moore} states that

\begin{quote}
	[a]lthough all sounds can be specified by their variation in pressure with time, it is often more convenient, and more meaningful, to specify them in a different way when the sounds are complex. This method is based on a theorem by Fourier, who proved that almost any complex waveform can be analyzed, or broken down, into a series of sinusoids with specific frequencies, amplitudes, and phases. This is done using a mathematical procedure called the Fourier transform.
\end{quote}

Throughout this section, the description of the Fourier transform of acoustic signals and the evolution of further transforms that build on it will be covered in detail.

\textcite{ltfat}'s Large Time-Frequency Analysis toolbox (LTFAT) is ``a Matlab/Octave toolbox for working with time-frequency analysis and synthesis.''\footnote{\url{http://ltfat.org/}} It contains a test signal from the glockenspiel instrument, loaded by the \Verb#gspi# function.\footnote{\url{https://ltfat.github.io/doc/signals/gspi.html}} This signal can be seen in audio signal processing papers on the topic of time-frequency (\cite{doerflerphd, balazs, jaillet, tfjigsaw, invertiblecqt, wmdct}). This is because the glockenspiel contains both tonal and transient properties, which have conflicting needs for time and frequency resolution in their analysis. Figure \ref{fig:glockwaveform} shows the discrete-time waveform of the glockenspiel signal. For the rest of this chapter, demonstrations of each transform will use this same glockenspiel signal as the input $x[n]$. The signal contains 262144 samples, and is sampled with a rate of 44100 Hz, meaning the total duration is $\sfrac{262144}{44100} \approx$ 5.94 seconds.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.825\textwidth]{./images-gspi/gspi_time_domain.png}
	\caption{Glockenspiel waveform}
	\label{fig:glockwaveform}
\end{figure}

\newpagefill

\subsubsection{Frequency analysis and the Fourier transform}
\label{sec:freqanal}

The Fourier transform originated as an integral transform in mathematics, which are a class of ``useful tools for solving problems involving certain types of partial differential equations (PDEs), mainly when their solutions on the corresponding domains of definition are difficult to deal with'' (\cite{fourierhistory}). The Fourier transform was originally introduced by Joseph Fourier in his earlier papers (\cite{fourierhist1, fourierhist2}), and fully expanded and collected in his seminal work on heat (\cite{fourierheat}). The connection of the Fourier transform to music is described by \textcite{fouriermusic}, who state that

\begin{quote}
	[b]eyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contribution represents some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string.
\end{quote}

The continuous-time Fourier transform (CTFT) of a time-domain acoustic waveform is defined by the pair of equations \ref{equation:ctft} and \ref{equation:ictft} (\cite[Chapter~11]{dspfirst}):
\begin{align}
	X(j\omega) = \int_{-\infty}^{\infty}{x(t)e^{-j\omega t}\mathit{dt}} \tag{1}\label{equation:ctft} \\
	x(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{X(j\omega)e^{j\omega t}\mathit{d\omega}} \tag{2}\label{equation:ictft}
\end{align}

$X(j\omega)$ is referred to by \textcite{dspfirst} as the frequency-domain representation of the signal $x(t)$, as the equation \ref{equation:ictft} defines the signal $x(t)$ as a ``sum of infinitely many complex-exponential signals with $X(j\omega)$ controlling the amplitude and phases of these signals.'' The continuous-time Fourier transform provides a one-to-one mapping of the time domain to the frequency domain.

As discussed in section \ref{sec:timedomain}, signals needs to be transformed from the continuous to the discrete domain via sampling to be processed digitally or computationally. The discrete-time Fourier transform (DTFT), also called the discrete Fourier transform (DFT), is derived from a sampled version of the continuous-time Fourier transform (shown previously in equations \ref{equation:ctft} and \ref{equation:ictft}), and is defined by the pair of equations \ref{equation:dtft} and \ref{equation:idtft} (\cite[Chapter~12]{melbook}):
\begin{align}
	X(e^{j\omega}) = \sum_{n = -\infty}^{\infty}{x[n]e^{-j\omega n}} \tag{3}\label{equation:dtft} \\
	x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}{X(e^{j\omega})e^{j\omega n}\mathit{d\omega}} \tag{4}\label{equation:idtft}
\end{align}

The Fourier transform is a complex-valued function of $\omega$, which is the variable that represents the angular frequency in radians. The Fourier transform can be expressed in the rectangular form in equation \ref{equation:rect} or polar form in equation \ref{equation:polar} (\cite[Chapter~2]{discretebook}):
\begin{align}
	X(e^{j\omega}) = X_{\text{real}}(e^{j\omega}) + j X_{\text{imag}}(e^{j\omega}) \tag{5}\label{equation:rect} \\
	X(e^{j\omega}) = |X(e^{j\omega})|e^{j\measuredangle X(e^{j\omega})} \tag{6}\label{equation:polar}
\end{align}

The quantities $|X(e^{j\omega})|$ and $\measuredangle X(e^{j\omega})$ are referred to as the magnitude and phase respectively. The Fourier transform is also referred to as the spectrum, while its magnitude and phase are the magnitude and phase spectra respectively (\cite{discretebook}).

In \textcite[Chapter~9]{discretebook} it is described that in its original formulation, the algorithmic complexity of the DFT is $O(N^{2})$, i.e. the running time of the algorithm grows proportionally the size of the input signal squared (\cite{skiena}). However, starting from legendary mathematician Carl Friedrich Gauss in 1805 (\cite{gausshist}), and reaching its most famous formulation published by \textcite{cooleytukey}, a family of efficient algorithms for the computation of the DFT by computing a series of smaller DFTs, known collectively as the Fast Fourier Transform (FFT), reduced this computation time to $O(N \log{N})$. This resulted in the FFT becoming one of the most important algorithms of the 20th century (\cite{ffttopten}). Refer to figure \ref{fig:bigo} to see the differences in the expected running times of the FFT over the naive implementation.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{./images-misc/bigo.png}
	\caption{Visual comparison of the ``Big-O'', or worst-case running times of algorithms (taken from \url{https://www.bigocheatsheet.com/})}
	\label{fig:bigo}
\end{figure}

The number of points, or samples, of the DFT, determines the frequency resolution, also called $\mathit{df}$ or $\mathit{\Delta f}$, which is the frequency spacing between each point in the resulting spectrum (\cite{discretebook}). The frequency resolution of an $N$-point DFT is $\mathit{df} = \sfrac{F_{s}}{N}$, where $F_{s}$ is the sampling rate of the input signal. An illustration of the magnitude and phase spectra of the DFT are shown in figure \ref{fig:glockdft}, using two different lengths of DFT to show the difference in the low and high frequency resolutions.

\begin{figure}[ht]
	\centering
	\subfloat[2048-point DFT, $\mathit{df} = 21.533 Hz$]{\includegraphics[width=\textwidth]{./images-gspi/gspi_dft.png}}\\
	\subfloat[262144-point DFT, $\mathit{df} = 0.168 Hz$]{\includegraphics[width=\textwidth]{./images-gspi/gspi_dft_bigger.png}}
	\caption{DFT of the Glockenspiel waveform. Note the richer frequency information in the higher frequency resolution transform in (b)}
	\label{fig:glockdft}
\end{figure}

\newpagefill

\subsubsection{Joint time-frequency analysis -- the Gabor transform and the Short-time Fourier Transform (STFT)}
\label{sec:jointtfa}

Continuing from the discussion of the DFT in the previous section \ref{sec:freqanal}, \textcite[Chapter~10]{discretebook} state that ``often, in practical applications of sinusoidal signal models, the signal properties (amplitudes, frequencies, and phases) will change with time. For example, nonstationary signal models of this type are required to describe radar, sonar, speech, and data communication signals. A single DFT estimate is not sufficient to describe such signals...''

\textcite{gabor1946}'s seminal signal processing paper, \textit{The Theory of Communication}, introduced significant and far-reaching concepts in the time and frequency analysis of acoustic signals. Gabor quotes famed American telecommunication engineer John Carson (\cite{carsonfamous}) to describe the limitations of the Fourier transform:

\begin{quote}
	[t]he foregoing solutions [of the Fourier transform], though unquestionably mathematically correct, are somewhat difficult to reconcile with our physical intuitions and our physical concepts of such variable frequency mechanisms as, for instance, the siren
\end{quote}

According to \textcite{korpel}, ``Gabor came to the conclusion that the difficulty lay in our mutually exclusive formulations of time analysis and frequency analysis ... he suggested a new method of analyzing signals in which time and frequency play symmetrical parts.''

Gabor derived the principal of time-frequency uncertainty from Heisenberg's uncertainty principle in quantum physics, which states that ``the more precisely the position [of an electron] is determined, the less precisely the momentum is known, and conversely'' (\cite{hallm, heisenberg1927}). Gabor's time-frequency uncertainty principle states that ``although we can carry out the analysis [of the acoustic signal] with any degree of accuracy in the time direction or frequency direction, we cannot carry it out simultaneously in both beyond a certain limit'' (\cite{gabor1946}). This is also referred to as the Gabor limit, and $\Delta t$ and $\Delta f$ are defined as ``the uncertainties inherent in the definition of the epoch t and frequency f of an oscillation.'' Gabor referred to the time-frequency tile $\Delta t \Delta f$ as the \textit{logon}, or smallest possible unit of time-frequency information.

Let's start with a description of the time-domain unit impulse signal or sequence (\cite[Chapter~2]{melbook}) in equation \ref{equation:delta}:
\begin{flalign}\label{equation:delta}
\delta[n] = \begin{cases}
	1 \text{\hspace{1em}} n = 0\\
	0 \text{\hspace{1em}} \text{otherwise}
\end{cases}
\end{flalign}

The impulse is a useful signal, as it is the ``simplest [time-domain] sequence because it has only one nonzero value, which occurs at n = 0. The mathematical notation is that of the Kronecker delta function'' (\cite[Chapter~5]{dspfirst}). Note the Kronecker delta function is the discrete equivalent of the Dirac delta function (\cite[Chapter~2]{melbook}). Contrast this with the DFT spectrum of a signal that is only non-zero at the 0 Hz (or DC) frequency component, which is a ``cosine signal with zero frequency'' (\cite[Chapter~3]{dspfirst}).

Figure \ref{fig:gaborfirst}(a) shows the unit impulse contrasted with the DC-component DFT, and these two in fact demonstrate the mutually exclusive formulations of time and frequency. The unit impulse, which has a single non-zero value in the time domain, has an infinite extent in the frequency domain. Conversely, the DC-component DFT has a single non-zero value in the frequency domain, but has an infinite extent in the time domain. Figure \ref{fig:gaborfirst}(b) shows the intuition of the time-frequency tradeoff as a choice of signal length. The sine wave's periodicity is more apparent over longer periods of time $\Delta t$, but the detail of the individual sample values become lost.

\begin{figure}[ht]
	\centering
	\subfloat[Mutually exclusive formulations of time and frequency by two extremes, the unit impulse (bottom left) and the 0 Hz cosine DFT spectrum (top right)]{\includegraphics[height=5cm]{./images-tftheory/gabor13.png}}
	\hspace{2em}
	\subfloat[The periodicity of the sine wave is more apparent with a longer $\Delta t$, at the cost of temporally localized sample values]{\includegraphics[height=6.25cm]{./images-tftheory/gabor2.png}}
	\caption{Time-frequency tradeoff intuitions (\cite{gabor2})}
	\label{fig:gaborfirst}
\end{figure}

The result of the time-frequency uncertainty principle is a property of the use of the Fourier transform to swap between the mutually exclusive domains of time and frequency. To further elaborate that this is a consequence of the Fourier transform, several psychacoustic studies have shown that humans can exhibit better time-frequency resolution than the Gabor limit. \textcite{psycho2} describes one of these experiments:

\begin{quote}
	It is concluded that models based on a place (spectral) analysis should be subject to a limitation of the type $\Delta f \cdot d \ge \text{constant}$, where $\Delta f$ is the frequency difference limen (DL) for a tone pulse of duration d. [...]  It was found that at short durations the product of $\Delta f$ and d was about one order of magnitude smaller than the minimum predicted [...]
\end{quote}

More recently, according to \textcite{psycho1}:

\begin{quote}
	[w]e have conducted the first direct psychoacoustical test of the Fourier uncertainty principle in human hearing, by measuring simultaneous temporal and frequency discrimination. Our data indicate that human subjects often beat the bound prescribed by the uncertainty theorem, by factors in excess of 10.
\end{quote}

Note that these are not refutations of Gabor's ideas, but in fact a confirmation. \textcite{gabor1946} himself stated that ``most sound analysis and processing tools today continue to use models based on spectral theories... [w]e believe it is time to revisit this issue.'' Nevertheless, if we choose to proceed with time-frequency analysis despite the limitations, it is preferable to minimize time-frequency uncertainty, or to set the \textit{logon} ($\Delta t \Delta f$) to its lowest possible value. \textcite{gabor1946} asks:

\begin{quote}
What is the shape of the signal for which the product $\Delta t \Delta f$ actually assumes the smallest possible value? [... it is] the modulation product of a harmonic oscillation of any frequency with a pulse of the form of the probability function
\end{quote}

Gabor performed joint time-frequency analysis by multiplying overlapping, temporally consecutive portions of the input signal with shifted copies of the Gaussian window function (i.e. the probability function), and by taking the Fourier transform of the windowed segments of the signal. The Gabor transform $G(f)$ of a discrete-time signal $x(n)$ is described by equation \ref{equation:gabort}:
\begin{flalign}\tag{7}\label{equation:gabort}
	\nonumber \mathbf{G(f)} &= [G_{1}(f), G_{2}(f), ..., G_{k}(f)], G_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-\beta m)e^{-j2\pi \alpha n}
\end{flalign}

where $g(\cdot)$ is a Gaussian low-pass window function localized at 0, $G_{m}(f)$ is the DFT of the signal centered around time $\beta m$, and $\alpha$ and $\beta$ control the time and frequency resolution of the transform.

The STFT, or short-time Fourier transform, has been described independently from Gabor's work (\cite{stftindie}), but additional research in the 1980s (\cite{dictionary}) led to the STFT being formalized and described as a special case of the Gabor transform, in recognition of Gabor's pioneering work. The STFT $X(f)$ of a discrete-time signal $x(n)$ is described by equation \ref{equation:stft}:
\begin{flalign}\tag{8}\label{equation:stft}
	\nonumber \mathbf{X(f)} &= [X_{1}(f), X_{2}(f), ..., X_{k}(f)], X_{m}(f) = \sum_{n = -\infty}^{\infty}x(n)g(n-mR)e^{-j2\pi f n}
\end{flalign}

where $g(\cdot)$ are the time-shifted, localized windows, $X_{m}(f)$ is the DFT of the audio signal centered about time $mR$, and $R$ is the hop size between successive time-shifts of the window. Note how similar equations \ref{equation:gabort} and \ref{equation:stft} are, which is expected since the original Gabor transform is the STFT with a Gaussian window. Practically, the STFT allows the use of different windows and overlap sizes (\cite{stftinvertible}), as long as overlap-add conditions are respected.\footnote{\url{https://www.mathworks.com/help/signal/ref/iscola.html}} The MATLAB documentation for the \Verb#iscola# function contains the diagram in figure \ref{fig:stftdiagram}, which shows how a windowed Fourier transform (i.e. Gabor transform or STFT) is performed on the input signal.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/stft_diagram.png}
	\caption{Diagram showing the forward and inverse STFT or Gabor transform}
	\label{fig:stftdiagram}
\end{figure}

Figure \ref{fig:gabortf} shows different sizes of \textit{logon} in the time-frequency plane, and how the Gabor transform and the STFT with higher frequency or higher time resolution appear on the time-frequency plane.

\begin{figure}[ht]
	\centering
	\subfloat[Pure time domain, pure frequency domain, and Gabor's time-frequency tiles of $\Delta t \Delta f = 1$]{\includegraphics[height=3.4cm]{./images-tftheory/gabor3.png}}\\
	\subfloat[High frequency resolution vs. high time resolution]{\includegraphics[height=3.2cm]{./images-tftheory/gabor4.png}}
	\caption{Different tiling of the time-frequency plane (\cite{gabordiagrams})}
	\label{fig:gabortf}
\end{figure}

Figure \ref{fig:stfts} shows the Gabor transform alongside the STFT using a popular choice of window, the Hamming window, for three different window sizes: 128 samples, 2048 samples, and 16384 samples, which at the sample rate of the glockenspiel signal (44100 Hz) represent $\sfrac{128}{44100}\cdot 1000 = $ 2.9 ms, 46.44 ms, and 371.52 ms respectively. The Hamming window was chosen because it is the default window in the MATLAB \Verb#spectrogram# function.\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}} The Gaussian and Hamming windows are shown together in figure \ref{fig:gaussvshamm}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/gaussianvshamming.png}
	\caption{2048-sample Hamming and Gaussian windows compared}
	\label{fig:gaussvshamm}
\end{figure}

\newpagefill

\begin{figure}[ht]
	\centering
	\subfloat[STFT, 128-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_128.png}}
	\subfloat[Gabor transform, 128-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_128.png}}\\
	\subfloat[STFT, 2048-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_2048.png}}
	\subfloat[Gabor transform, 2048-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_2048.png}}\\
	\subfloat[STFT, 16384-sample Hamming]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_hamm_16384.png}}
	\subfloat[Gabor transform, 16384-sample Gaussian]{\includegraphics[width=0.5\textwidth]{./images-gspi/gspi_gauss_16384.png}}
	\caption{Visual comparison of the Hamming-window STFT and the Gaussian-window STFT (i.e. the Gabor transform), using the Glockenspiel signal}
	\label{fig:stfts}
\end{figure}

We close this section with a quote from \textcite{doerflersouls} on the powerful properties of the STFT:

\begin{quote}
       ... the STFT has at least three souls: it is the Fourier transform of windowed portions of the signal, it is the convolution of the signal with modulated versions of the window and it is the scalar product of the signal with time-shifted modulated versions of the window. These three souls can be exploited in the applications, for the computation of the STFT and for its sampling ...
\end{quote}

\newpagefill

\subsubsection{Constant-Q Transform (CQT)}

\textcite{cqtransient} stated that the problem with the STFT is its ``rigid time-frequency resolution trade-off providing a constant absolute frequency resolution throughout the entire range of audible frequencies.'' As described in section \ref{sec:motivation}, music should be analyzed with a high frequency resolution in the low frequency region, and with a high time resolution in the high frequency region (\cite{doerflerphd, cqtransient}). 

The constant-Q transform was proposed by \textcite{jbrown, msp} to analyze musical signals with a logarithmic frequency scale to show the relationship between the fundamental frequency of a musical instrument and its harmonics better than the Fourier transform's uniform frequency spacing. A demonstration of a violin analyzed with the DFT and CQT was shown in figure \ref{fig:violin}.

The name ``constant-Q'' refers to the constant ratio of the frequency being analyzed to the frequency resolution of analysis, or $\sfrac{f}{\delta f} = Q$, such that the frequency resolution increases with the center frequency to maintain the $Q$ factor. The transform uses long-duration windows in the low frequency regions and short-duration windows in the high frequency regions, resulting in good time resolution for transients (\cite{cqtransient}). Figure \ref{fig:jbrowncqt} shows some properties of the linearly-spaced DFT spectrum compared to the constant-Q transform, and the different window sizes used for each frequency bin of interest. Equation \ref{equation:jbrowncqt} describes how the different windows $N[k]$ are applied to the signal:
\begin{align}\tag{9}\label{equation:jbrowncqt}
	\nonumber N[k] \text{(window length)} &= \frac{f_{s}}{f_{k}}Q, W[k, n] = \alpha + (1 - \alpha)\cos\big(\frac{2\pi n}{N[k]}\big)
\end{align}

\begin{figure}[ht]
	\centering
	\subfloat[Properties of DFT, CQT]{\includegraphics[height=5.05cm]{./images-tftheory/dftvcqt.png}}
	\hspace{0.5em}
	\subfloat[Window sizes for CQT]{\includegraphics[height=5cm]{./images-tftheory/qwindowchanges.png}}
	\caption{Various aspects of \textcite{jbrown}'s CQT}
	\label{fig:jbrowncqt}
\end{figure}

This first CQT implementation was not designed to be invertible, until \textcite{klapuricqt} introduced an algorithm for an approximate inverse of the CQT back to the time-domain waveform, with an inversion error of $\approx 10^{-3}$. The approximate inverse for the CQT was also approached differently by \textcite{fitzgeraldcqt}.

\textcite{klapuricqt} describe the CQT as ``a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal.'' Frequency bins that are geometrically spaced are also logarithmically spaced with a base of two (\cite{geometriclog}). The bins-per-octave setting of the CQT is related to the constant-Q ratio by the formula $Q = 2^{\sfrac{1}{\text{bins}}}$. For example, for 12 bins-per-octave, this results in a $Q$ factor of 1.059, which is the well-known ``12th root of two'' based on the Western chromatic scale (\cite{westernpitch1, westernpitch2}).

\textcite{invertiblecqt} implemented the constant-Q transform with Nonstationary Gabor frames (\cite{balazs}), which allowed for perfect reconstruction for the first time. This is also called the CQ-NSGT, or constant-Q Nonstationary Gabor Transform. The next section, \ref{sec:theorynsgt}, will cover the NSGT in detail, but for now we shall continue with how the NSGT led to improved implementations of the CQT.

\textcite{slicq} followed up with a realtime algorithm, the \textit{sliCQ} or ``sliced constant-Q'' transform (sliCQT), which can process the input signal in fixed-size slices, as opposed to operating on the entire input signal like the NSGT. Finally, \textcite{variableq1} introduced the variable-Q scale and improved the phase of the transform. From \textcite{invertiblecqt}, the total frequency bins of the CQ-NSGT or sliCQT can be derived from the bins-per-octave, and the minimum and maximum frequencies of the desired frequency scale. The formula given is $K = [B \log_{2}(\sfrac{\xi_{\text{max}}}{\xi_{\text{min}}}) + 1]$, where $K$ is the total bins of the CQT, $B$ is the bins-per-octave, and $\xi_{\text{min,max}}$ are the minimum and maximum frequencies. For example, for $B = 12 \text{ bins-per-octave}$, $\xi_{\text{min}} = 83 \text{ Hz}$ and $\xi_{\text{max}} = 22050 \text{ Hz}$, the result is $K \approx 97 \text{ total frequency bins}$.

In the landscape of music analysis libraries, Essentia,\footnote{\url{https://essentia.upf.edu/}} (\cite{essentia}), LTFAT,\footnote{\url{https://ltfat.org/}} (\cite{ltfat}) and the MATLAB Wavelet Toolbox\footnote{\url{https://www.mathworks.com/help/wavelet/ref/cqt.html}} have converged on the CQ-NSGT (\cite{invertiblecqt, slicq, variableq1}). However, librosa\footnote{\url{https://librosa.org/}} (\cite{librosa}) still uses the older implementation with the approximate reconstruction (\cite{klapuricqt}). Finally, \textcite{invertiblecqt} have released an open-source reference Python implementation\footnote{\url{https://github.com/grrrr/nsgt}} of the CQ-NSGT and the sliCQT.

Let's consider spectrogram plots generated from the CQ-NSGT from the MATLAB Wavelet Toolbox using the \Verb#cqt# function compared with the standard STFT spectrogram using the \Verb#spectrogram# function in MATLAB,\footnote{\url{https://www.mathworks.com/help/signal/ref/spectrogram.html}}. Various plots of the STFT spectrogram vs. CQT spectrogram are shown in figure \ref{fig:cqtvstft}. The bins-per-octave parameter controls the frequency resolution -- more bins-per-octave gives us a finer frequency discrimination.

\begin{figure}[ht]
	\centering
	\subfloat[CQT spectrogram, 12 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt12.png}}
	\subfloat[STFT spectrogram, window size 256]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft256.png}}\\
	\subfloat[CQT spectrogram, 24 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt24.png}}
	\subfloat[STFT spectrogram, window size 1024]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft1024.png}}\\
	\subfloat[CQT spectrogram, 48 bins-per-octave]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_cqt48.png}}
	\subfloat[STFT spectrogram, window size 4096]{\includegraphics[width=0.475\textwidth]{./images-gspi/glock_stft4096.png}}
	\caption{Spectrograms of the glockenspiel signal. Note how the CQT displays more frequencies, or musical information, than the STFT for all frequency resolutions.}
	\label{fig:cqtvstft}
\end{figure}

\newpagefill

\subsubsection{Nonstationary Gabor Transform (NSGT)}
\label{sec:theorynsgt}

The section on the Nonstationary Gabor Transform begins with \textcite{doerflerphd}, who analyzed music with multiple Gabor dictionaries (i.e. STFTs). Figure \ref{fig:dorflertradeoff} shows the time-frequency tradeoff of the short and long window STFT analyses of the glockenspiel signal for its transient and tonal characteristics, and how two Gabor dictionaries combined gives us the ability to analyze either the transient or tonal aspects with a more appropriate resolution.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./images-tftheory/tf_tradeoff_dorfler.png}
	\caption{Time-frequency tradeoff for a glockenspiel signal (\cite{doerflerphd})}
	\label{fig:dorflertradeoff}
\end{figure}

As summarized by \textcite{adaptivecqt}, ``[t]he definition of multiple Gabor frames, which is comprehensively treated in [D{\"o}rfler 2002], provides Gabor frames with analysis techniques with multiple resolutions... The nonstationary Gabor frames [...] are a further development; [...] they provide for a class of FFT-based algorithms [...] together with perfect reconstruction formulas.'' In other words, a single Nonstationary Gabor transform can replace the need for multiple distinct Gabor transforms (or STFTs).

\textcite{dictionary} describe the shift from the term \textit{transform}, e.g., the Gabor transform or STFT, to \textit{dictionary}, stating that works by \cite{dictionary1} and \cite{dictionary2} began the ``fundamental shift from transforms to dictionaries for [...] signal representation.'' Accordingly, an important outcome of this terminology change was the ``idea that a signal was allowed to have more than one description in the representation domain, and that selecting the best one depended on the task.''

Using multiple transforms, such as using two Gabor transforms (or STFTs) with a small and large window for the transient and tonal properties of music, is also called an \textit{overcomplete} dictionary (\cite{dictionary}), where there is a lot of redundancy in the transform domain.

The advantage of the CQT over such approaches is that it contains these desirable transform properties in one single transform. The NSGT is a time-frequency transform with varying time-frequency resolution, whose motivating application is the CQT (\cite{jaillet, balazs}). It is constructed from frame theory (\cite{frametheory}), which is a mathematical technique for computing ``redundant, stable way[s] of representing a signal'' (\cite{framesintro}). It can use nonuniform time and frequency spacing in its time-frequency analysis of a signal, and it has a perfect inverse operation, such that there is practically no reconstruction error.

The construction of nonstationary Gabor frames relies on three properties of the windows and time-frequency shift parameters used (\cite{balazs}):
\begin{tight_itemize}
	\item
		The signal $f$ of interest is localized at time- (or frequency-) positions $n$ by means of multiplication with a compactly supported (or limited bandwidth, respectively) window function $g_{n}$
	\item
		The Fourier transform is applied on the localized pieces $f \cdot g_{n}$. The resulting spectra are sampled densely enough in order to perfectly re-construct $f \cdot g_{n}$ from these samples
	\item
		Adjacent windows overlap to avoid loss of information. At the same time, unnecessary overlap is undesirable. We assume that $0 < A \le \sum_{n \in \mathbb{Z}}|g_{n}(t)|^{2} \le B < \infty$, a.e. (almost everywhere), for some positive A and B
\end{tight_itemize}

These requirements lead to invertibility of the frame operator and therefore to perfect reconstruction. \textcite{balazs} continues on to say that

\begin{quote}
	[m]oreover, the frame operator is diagonal and its inversion is straightforward. Further, the canonical dual frame has the same structure as the original one. Because of these pleasant consequences following from the three above-mentioned requirements, the frames satisfying all of them will be called painless nonstationary Gabor frames and we refer to this situation as the painless case
\end{quote}

To derive the NSGT, let's recall the previously-seen definition of the Gabor transform from section \ref{sec:jointtfa}. In the standard Gabor transform, the same window function (also called the Gabor atom or Gabor function) is shifted in time to cover entire signal (\cite{adaptivecqt}), described by equation \ref{equation:stationarygab}:
\begin{align} \tag{10}\label{equation:stationarygab}
g_{m, n}(t) = g(t - na)e^{2\pi i m b t}
\end{align}

As stated by \textcite{adaptivecqt}, ``[w]e will indicate such a frame as \textit{stationary}, since the window used for time-frequency shifts does not change and the time-frequency shifts form a lattice of $a \times b$.'' Figure \ref{fig:uniformtflattice} shows the resulting uniform $a \times b$ tiling of the time-frequency plane.

\begin{figure}[ht]
	\centering
	\includegraphics[width=6.25cm]{./images-tftheory/stationarygabor.png}
	\caption{Uniform time-frequency resolution with the stationary Gabor transform}
	\label{fig:uniformtflattice}
\end{figure}

We can also describe \textcite{doerflerphd}'s multiple Gabor systems with the following set of equations, continuing from the stationary Gabor transform. Given the stationary Gabor atom, where $a$ and $b$ are the time-frequency shift parameters and $f(t)$ is the reconstructed input signal, the Gabor transform is described by the equations in \ref{equation:onegabor}.
\begin{align}
	\nonumber g_{m,n}(t) &= g(t - na)e^{j2\pi m b t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{m,n \in \mathbb{Z}}c_{m,n}g_{m,n}(t) \tag{12}\label{equation:onegabor}
\end{align}

Similarly, we can describe an overcomplete system that uses $R$ distinct STFTs (or Gabor transforms) with different window sizes, where $a_{r}$ and $b_{r}$ are the time-frequency shift parameters for each distinct window size, with the equations in \ref{equation:multigabor}:
\begin{align}
	\nonumber g_{m,n}^{r}(t) &= g(t - na_{r})e^{j2\pi m b_{r} t}, m,n \in \mathbb{Z}\\
	\nonumber f(t) &= \sum_{r=0}^{R-1}\sum_{m,n \in \mathbb{Z}}c^{r}_{m,n}g^{r}_{m,n}(t) \tag{13}\label{equation:multigabor}
\end{align}

For a resolution that changes with time, the Nonstationary Gabor atom is chosen from a set of functions $\{g_{n}\}$ and a fixed frequency sampling step $b_{n}$:
\begin{align}\tag{14}\label{equation:irregulartime}
	g_{m,n}(t) &= g_{n}(t)e^{j2\pi m b_{n}t}, m,n \in \mathbb{Z}
\end{align}

To describe equation \ref{equation:irregulartime}, Balazs states:

\begin{quote}
		[...] the functions $\{g_{n}\}$ are well-localized and centered around time-points $a_{n}$. This is similar to the standard Gabor scheme [...] with the possibility to vary the window $g_{n}$ for each position $a_{n}$. Thus, sampling of the time-frequency plane is done on a grid which is irregular over time, but regular over frequency at each temporal position.
\end{quote}

For a resolution that changes with frequency, the Nonstationary Gabor atom is chosen from a family of functions $\{h_{m}\}$, which are `well-localized band-pass function with center frequency $b_{n}$'' and a fixed time sampling step $a_{m}$:
\begin{align}\tag{15}\label{equation:irregularfrequency}
	h_{m,n}(t) = h_{m}(t - na_{m}), m,n \in \mathbb{Z}
\end{align}

Figure \ref{fig:nonuniformtflattices} show both the cases of the varying resolution by time and by frequency in terms of sampled points on the time-frequency grid.

\begin{figure}[ht]
	\centering
	\subfloat[Gabor atoms that change with time]{\includegraphics[width=6.25cm]{./images-tftheory/irregulartime.png}}
	\hspace{1em}
	\subfloat[Gabor atoms that change with frequency]{\includegraphics[width=6.25cm]{./images-tftheory/irregularfrequency.png}}
	\caption{Varying time-frequency sampling of the Nonstationary Gabor transform}
	\label{fig:nonuniformtflattices}
\end{figure}

As a final note, the general NSGT is more interesting to study than the CQ-NSGT (constant-Q NSGT), as it can be constructed with nonuniform frequency scales besides the constant-Q scale, such as the psychoacoustic mel and Bark scales (\cite[Chapter~4]{melbook}), or the variable-Q scale based on the psychoacoustic ERBlet transform (\cite{variableq1, variableq2}).

Figures \ref{fig:bunchansgts1} and \ref{fig:bunchansgts2} show several different configurations of the NSGT of the glockenspiel signal, to demonstrate the diversity of the transform. The plots are generated from my fork\footnote{\url{https://github.com/sevagh/nsgt}} of the reference Python NSGT library which has a spectrogram script. My modifications to the library will be described further in sections \ref{sec:methodology} and \ref{sec:experiment}. A high and low frequency resolution NSGT, using 100 and 500 total frequency bins respectively, is generated for each of the following 4 frequency scales: mel, Bark, constant-Q, and variable-Q, using a frequency range of 20--22050 Hz. For the variable-Q scale, the fixed frequency offset applied is 15 Hz. 20 Hz is the accepted lowest frequency of human hearing (\cite{moore}), and 22050 Hz is the Nyquist frequency (or maximum supported signal frequency) of the common sample rate, 44100 Hz. In practice, the minimum and maximum frequencies are also useful parameters that can be customized to fit the type of signal or application.

Details of the frequency scales used to generate the NSGT spectrograms are shown in tables \ref{table:nsgtfreqs} and \ref{table:nsgtqs}.

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, constant-Q scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_cqlog_100.png}}\\
	\subfloat[NSGT, constant-Q scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_cqlog_500.png}}\\
	\subfloat[NSGT, variable-Q scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_vqlog_100.png}}\\
	\subfloat[NSGT, variable-Q scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_vqlog_500.png}}
	\caption{Spectrograms of the glockenspiel signal using the NSGT with a low (100 bins) and high (500 bins) frequency resolution and the constant- and variable-Q scales. The frequency range for each shown scale is 20--22050 Hz, and there is an offset of 15 Hz per band for the variable-Q scale}
	\label{fig:bunchansgts1}
\end{figure}

\begin{figure}[ht]
	\centering
	\subfloat[NSGT, mel scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_mel_100.png}}\\
	\subfloat[NSGT, mel scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_mel_500.png}}\\
	\subfloat[NSGT, Bark scale, 100 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_bark_100.png}}\\
	\subfloat[NSGT, Bark scale, 500 bins]{\includegraphics[width=\textwidth]{./images-gspi/gspi_nsgt_bark_500.png}}\\
	\caption{Spectrograms of the glockenspiel signal using the NSGT with a low (100 bins) and high (500 bins) frequency resolution and the mel and Bark psychoacoustic scales. The frequency range for each shown scale is 20--22050 Hz}
	\label{fig:bunchansgts2}
\end{figure}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l| }
	 \hline
	 Scale & First 10 frequencies & Last 10 frequencies  \\
	 \hline
	 \hline
	 Constant-Q & b & c \\
	 \hline
	 Variable-Q & b & c \\
	 \hline
	 mel & b & c \\
	 \hline
	 Bark & b & c \\
	 \hline
\end{tabular}
	\caption{NSGT scale frequencies for 100 bins and 20-22050 Hz}
	\label{table:nsgtfreqs100}
\end{table}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l| }
	 \hline
	 Scale & First 10 Q factors & Last 10 Q factors  \\
	 \hline
	 \hline
	 Constant-Q & b & c \\
	 \hline
	 Variable-Q & b & c \\
	 \hline
	 mel & b & c \\
	 \hline
	 Bark & b & c \\
	 \hline
\end{tabular}
	\caption{NSGT scale Q factors for 100 bins and 20-22050 Hz}
	\label{table:nsgtqs100}
\end{table}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l| }
	 \hline
	 Scale & First 10 frequencies & Last 10 frequencies  \\
	 \hline
	 \hline
	 Constant-Q & b & c \\
	 \hline
	 Variable-Q & e & f \\
	 \hline
	 mel & e & f \\
	 \hline
	 Bark & e & f \\
	 \hline
\end{tabular}
	\caption{NSGT scale frequencies for 500 bins and 20-22050 Hz}
	\label{table:nsgtfreqs500}
\end{table}

\begin{table}[ht]
	\centering
\begin{tabular}{ |l|l|l| }
	 \hline
	 Scale & First 10 Q factors & Last 10 Q factors  \\
	 \hline
	 \hline
	 Constant-Q & b & c \\
	 \hline
	 Variable-Q & b & c \\
	 \hline
	 mel & b & c \\
	 \hline
	 Bark & b & c \\
	 \hline
\end{tabular}
	\caption{NSGT scale Q factors for 500 bins and 20-22050 Hz}
	\label{table:nsgtqs500}
\end{table}

\newpagefill

\subsubsection{sliCQ Transform (sliCQT)}
\label{sec:theoryslicqt}

\todo[inline]{
	slicq is the realtime variant\\
	noninvertible overlap thing here
}

\newpagefill

\subsection{Machine learning and deep learning}
\label{sec:ml}

\ichfeedback{if i'm presenting a neural network, it's probably necessary to have this section?}

\chaptertodo{
	use MIR presentation introduction to machine and deep learning\\
	mention signal processing, basis pursuit, etc., i.e. classic approximate/probabilistic signal processing
}

\newpagefill

\subsection{Music source separation}
\label{sec:musicsep}

\subsubsection{Task motivation and definition}

\todo[inline]{purposes and uses - why do we want to do this}

\subsubsection{Public datasets}

The most popular music stem dataset used by SISEC and SigSep is the MUSDB18 dataset (\cite{musdb18}), and more recently the HQ (high-quality) version (\cite{musdb18hq}). MUSDB18-HQ contains stereo wav files sampled at 44100 Hz representing stems (drum, vocal, bass, and other) from a collection of permissively licensed music, specifically intended for recording, mastering, mixing (and in this case, ``de-mixing'', or source separation) research. It combines earlier mixing/demixing datasets (\cite{otherdataset1, otherdataset2}).

The songs in the MUSDB18-HQ dataset have a fixed train, validation, and test split. Following the rules defined in the ISMIR 2021 Music Demixing Challenge,\footnote{\url{https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021}} for a network to be considered trained only on MUSDB18-HQ, the predefined data splits must be used.

\subsubsection{Evaluation measures}

The SigSep\footnote{\url{https://sigsep.github.io/}} community, borrowing from the methodology of Signal Separation Evaluation Campaign (SISEC), uses the BSS (Blind Source Separation) Eval \cite{bss} objective measure for separation quality. There are 4 distinct metrics that comprise BSS:

\begin{tight_itemize}
\item
	\textbf{ISR:} source Image to Spatial distortion Ratio
\item
	\textbf{SIR:} Signal to Interference Ratio
\item
	\textbf{SAR:} Signal to Artifacts Ratio
\item
	\textbf{SDR:} Signal to Distortion Ratio
\end{tight_itemize}

Out of these 4 scores, SDR is the single global score which is commonly used to summarize the overall performance of a music demixing system (\cite{sdruseful}). The SDR as it was defined in the ISMIR 2021 Music Demixing Challenge (and used to rank the participants) can be computed from the following equation \ref{equation:sdrinstr}:
\begin{align}
	\nonumber & \text{SDR}_{\text{instr}} = \\
	&10 \log_{10}\frac{\sum_{n}\big(s_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n)\big)^{2}}{\sum_{n}\big(s_{\text{instr, left}}(n) - \hat{s}_{\text{instr, left}}(n)\big)^{2} + \sum_{n}\big(s_{\text{instr, right}}(n) - \hat{s}_{\text{instr, right}}(n)\big)^{2}} \tag{10}\label{equation:sdrinstr}
\end{align}

The unit of the SDR score is in decibels or dB, $s_{\text{instr}}(n)$ denotes the ground truth waveform of the instrument, $\hat{s}_{\text{instr}}(n)$ is the estimate, and left and right refer to the two channels in the stereo dataset of MUSDB18-HQ. Given the four stems of MUSDB18-HQ -- vocals, drums, bass, other -- the SDR is computed for each stem from the equation \ref{equation:sdrinstr} above. Then, the four SDR scores are combined for a total song score in equation \ref{equation:sdrsong}:
\begin{align}
	\text{SDR}_{\text{song}} = \frac{1}{4}(\text{SDR}_{\text{bass}} + \text{SDR}_{\text{drums}} + \text{SDR}_{\text{vocals}} + \text{SDR}_{\text{other}}) \tag{11}\label{equation:sdrsong}
\end{align}

In the most recent SiSec evaluation (\cite{sisec2018}), the BSS evaluation measure used is BSS v4, a variant of BSS available in their Python libraries museval\footnote{\url{https://github.com/sigsep/sigsep-mus-eval}} and bsseval.\footnote{\url{https://github.com/sigsep/bsseval}} The differences between BSS as used in SiSec 2016 (\cite{sisec2016}) and BSS v4 are outlined in the bsseval project's GitHub README file:

\begin{quote}
	One particularity of BSSEval is to compute the metrics after optimally matching the estimates to the true sources through linear distortion filters. This allows the criteria to be robust to some linear mismatches... this matching is the reason for most of the computation cost of BSSEval...

	For this package, we enabled the option of having time invariant distortion filters, instead of necessarily taking them as varying over time as done in the previous versions of BSSEval. First, enabling this option significantly reduces the computational cost for evaluation because matching needs to be done only once for the whole signal. Second, it introduces much more dynamics in the evaluation, because time-varying matching filters turn out to over-estimate performance. Third, this makes matching more robust, because true sources are not silent throughout the whole recording, while they often were for short windows
\end{quote}

\subsubsection{Survey of computational approaches}

\ichfeedback{spectral masking, NMF, machine learning, deep learning - i can lean on the machine learning introduction section right before}

\todo[inline]{summary of approaches over the year e.g. nonnegative matrix factorization to machine learning to deep learning}

\subsubsection{Time-frequency masking and oracle estimators}

\ichfeedback{i think the idea of the oracle mask computed from ground truths is important enough to be in the section title}

\ichfeedback{it will come up later in the thesis when choosing hyperparameters for the sliCQ}

\subsubsection{Open-Unmix (UMX) and CrossNet-Open-Unmix (X-UMX)}

\chaptertodo{
	\url{https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf}\\
	use my RNN slides from MIR-presentations\\
	UMX bi-LSTM comes from \url{https://www.researchgate.net/publication/335688695_Open-Unmix_-_A_Reference_Implementation_for_Music_Source_Separation}\\
	more umx \url{https://hal.inria.fr/hal-02293689/document}\\
	bi-LSTM here which comes from \url{https://www.researchgate.net/publication/315100151_Improving_music_source_separation_based_on_deep_neural_networks_through_data_augmentation_and_network_blending}\\
	more here \url{https://ieeexplore.ieee.org/document/7952158}\\
	more here \url{https://www.researchgate.net/profile/Yuki-Mitsufuji/publication/315100151_Improving_music_source_separation_based_on_deep_neural_networks_through_data_augmentation_and_network_blending/links/59ed4f844585151983ccdcba/Improving-music-source-separation-based-on-deep-neural-networks-through-data-augmentation-and-network-blending.pdf}\\
	finally comes from here \url{https://www.semanticscholar.org/paper/Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber/2f83f6e1afadf0963153974968af6b8342775d82}

}

 \textcite{umx}'s deep learning model for music source separation is intended to be a near state-of-the-art, open implementation based on the open MUSDB18 and MUSDB18-HQ datasets and designed to foster source separation research \cite{musdb18, musdb18hq}. A deep neural network is used to estimate the magnitude spectrograms of the sources given a mixed song as an input. The sources are the same as the four stems per track in MUSDB18: drums, vocals, bass, other. Finally, the estimate is used to compute a soft mask.

 \subsubsection{Convolutional denoising autoencoders}

 While the discussed Open-Unmix model is based on a sequence-to-sequence LSTM architecture, a different class of neural network called convolutional autoencoders, or convolutional denoising autoencoders (CDAE), have been seeing increasing use in music demixing (\cite{plumbley1, plumbley2}).

 In particular, the CDAE networks of \textcite{plumbley1} and \textcite{plumbley2} give some simple and adaptable ideas for performing music demixing with 2D convolutional layers applied on time-frequency transforms.

 \todo[inline]{plumbley's papers here and a bit of CNN theory}

\end{document}
